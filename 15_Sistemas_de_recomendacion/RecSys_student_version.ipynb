{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender systems\n",
    "\n",
    "## One of the most common uses of big data is to predict and suggest what users may want.  This allows Google to show you relevant ads or to suggest news in Google Now; Amazon to recommend relevant products; Netflix to recommend movies that you might like; or most recently, the famous **Weekly Dicovery** of Spotify.\n",
    "\n",
    "## All these products are based on systems of recommendation: a information retrieval method to provide users with relevant, yet novel and diverse, information. \n",
    "\n",
    "## In this class we will use a pretty famous dataset based on movies ratings, 'MovieLens', to learn the basics of recommender systems. \n",
    "\n",
    "## Table of Contents (times are approximated)\n",
    "\n",
    "1. [Getting and analysing some data (~1:30 h)](#data)\n",
    "2. [Most popular movies (~30 min)](#popular)\n",
    "3. [Metrics for recommender systems (~1.30h)](#metrics)\n",
    "4. [Collaborative Filtering (~15 min)](#cf)  \n",
    "   4.1 [Co-occurrence Matrix (~1.30h)](#copurchase)\n",
    "   <br></br>\n",
    "   4.2 [Memory-based CF (~1 h)](#memory-base)\n",
    "   <br></br>\n",
    "   4.3 [Model-based CF (~2 h)](#model-base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='data'></a>\n",
    "## 1.1 Load data\n",
    "\n",
    "We will use MovieLens dataset, which is one of the most common datasets used when implementing and testing recommender engines. This data set consists of:\n",
    "* 100,000 ratings (1-5) from 943 users on 1682 movies. \n",
    "* Each user has rated at least 20 movies. \n",
    "* Simple demographic info for the users (age, gender, occupation, zip)\n",
    "\n",
    "The data was collected through the MovieLens [website](https://movielens.org) during the seven-month period from September 19th, \n",
    "1997 through April 22nd, 1998. This data has been cleaned up - users\n",
    "who had less than 20 ratings or did not have complete demographic\n",
    "information were removed from this data set.\n",
    "\n",
    "You can download the dataset [here](http://files.grouplens.org/datasets/movielens/ml-100k.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the readme file!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY & USAGE LICENSE\r\n",
      "=============================================\r\n",
      "\r\n",
      "MovieLens data sets were collected by the GroupLens Research Project\r\n",
      "at the University of Minnesota.\r\n",
      " \r\n",
      "This data set consists of:\r\n",
      "\t* 100,000 ratings (1-5) from 943 users on 1682 movies. \r\n",
      "\t* Each user has rated at least 20 movies. \r\n",
      "        * Simple demographic info for the users (age, gender, occupation, zip)\r\n",
      "\r\n",
      "The data was collected through the MovieLens web site\r\n",
      "(movielens.umn.edu) during the seven-month period from September 19th, \r\n",
      "1997 through April 22nd, 1998. This data has been cleaned up - users\r\n",
      "who had less than 20 ratings or did not have complete demographic\r\n",
      "information were removed from this data set. Detailed descriptions of\r\n",
      "the data file can be found at the end of this file.\r\n",
      "\r\n",
      "Neither the University of Minnesota nor any of the researchers\r\n",
      "involved can guarantee the correctness of the data, its suitability\r\n",
      "for any particular purpose, or the validity of results based on the\r\n",
      "use of the data set.  The data set may be used for any research\r\n",
      "purposes under the following conditions:\r\n",
      "\r\n",
      "     * The user may not state or imply any endorsement from the\r\n",
      "       University of Minnesota or the GroupLens Research Group.\r\n",
      "\r\n",
      "     * The user must acknowledge the use of the data set in\r\n",
      "       publications resulting from the use of the data set\r\n",
      "       (see below for citation information).\r\n",
      "\r\n",
      "     * The user may not redistribute the data without separate\r\n",
      "       permission.\r\n",
      "\r\n",
      "     * The user may not use this information for any commercial or\r\n",
      "       revenue-bearing purposes without first obtaining permission\r\n",
      "       from a faculty member of the GroupLens Research Project at the\r\n",
      "       University of Minnesota.\r\n",
      "\r\n",
      "If you have any further questions or comments, please contact GroupLens\r\n",
      "<grouplens-info@cs.umn.edu>. \r\n",
      "\r\n",
      "CITATION\r\n",
      "==============================================\r\n",
      "\r\n",
      "To acknowledge use of the dataset in publications, please cite the \r\n",
      "following paper:\r\n",
      "\r\n",
      "F. Maxwell Harper and Joseph A. Konstan. 2015. The MovieLens Datasets:\r\n",
      "History and Context. ACM Transactions on Interactive Intelligent\r\n",
      "Systems (TiiS) 5, 4, Article 19 (December 2015), 19 pages.\r\n",
      "DOI=http://dx.doi.org/10.1145/2827872\r\n",
      "\r\n",
      "\r\n",
      "ACKNOWLEDGEMENTS\r\n",
      "==============================================\r\n",
      "\r\n",
      "Thanks to Al Borchers for cleaning up this data and writing the\r\n",
      "accompanying scripts.\r\n",
      "\r\n",
      "PUBLISHED WORK THAT HAS USED THIS DATASET\r\n",
      "==============================================\r\n",
      "\r\n",
      "Herlocker, J., Konstan, J., Borchers, A., Riedl, J.. An Algorithmic\r\n",
      "Framework for Performing Collaborative Filtering. Proceedings of the\r\n",
      "1999 Conference on Research and Development in Information\r\n",
      "Retrieval. Aug. 1999.\r\n",
      "\r\n",
      "FURTHER INFORMATION ABOUT THE GROUPLENS RESEARCH PROJECT\r\n",
      "==============================================\r\n",
      "\r\n",
      "The GroupLens Research Project is a research group in the Department\r\n",
      "of Computer Science and Engineering at the University of Minnesota.\r\n",
      "Members of the GroupLens Research Project are involved in many\r\n",
      "research projects related to the fields of information filtering,\r\n",
      "collaborative filtering, and recommender systems. The project is lead\r\n",
      "by professors John Riedl and Joseph Konstan. The project began to\r\n",
      "explore automated collaborative filtering in 1992, but is most well\r\n",
      "known for its world wide trial of an automated collaborative filtering\r\n",
      "system for Usenet news in 1996.  The technology developed in the\r\n",
      "Usenet trial formed the base for the formation of Net Perceptions,\r\n",
      "Inc., which was founded by members of GroupLens Research. Since then\r\n",
      "the project has expanded its scope to research overall information\r\n",
      "filtering solutions, integrating in content-based methods as well as\r\n",
      "improving current collaborative filtering technology.\r\n",
      "\r\n",
      "Further information on the GroupLens Research project, including\r\n",
      "research publications, can be found at the following web site:\r\n",
      "        \r\n",
      "        http://www.grouplens.org/\r\n",
      "\r\n",
      "GroupLens Research currently operates a movie recommender based on\r\n",
      "collaborative filtering:\r\n",
      "\r\n",
      "        http://www.movielens.org/\r\n",
      "\r\n",
      "DETAILED DESCRIPTIONS OF DATA FILES\r\n",
      "==============================================\r\n",
      "\r\n",
      "Here are brief descriptions of the data.\r\n",
      "\r\n",
      "ml-data.tar.gz   -- Compressed tar file.  To rebuild the u data files do this:\r\n",
      "                gunzip ml-data.tar.gz\r\n",
      "                tar xvf ml-data.tar\r\n",
      "                mku.sh\r\n",
      "\r\n",
      "u.data     -- The full u data set, 100000 ratings by 943 users on 1682 items.\r\n",
      "              Each user has rated at least 20 movies.  Users and items are\r\n",
      "              numbered consecutively from 1.  The data is randomly\r\n",
      "              ordered. This is a tab separated list of \r\n",
      "\t         user id | item id | rating | timestamp. \r\n",
      "              The time stamps are unix seconds since 1/1/1970 UTC   \r\n",
      "\r\n",
      "u.info     -- The number of users, items, and ratings in the u data set.\r\n",
      "\r\n",
      "u.item     -- Information about the items (movies); this is a tab separated\r\n",
      "              list of\r\n",
      "              movie id | movie title | release date | video release date |\r\n",
      "              IMDb URL | unknown | Action | Adventure | Animation |\r\n",
      "              Children's | Comedy | Crime | Documentary | Drama | Fantasy |\r\n",
      "              Film-Noir | Horror | Musical | Mystery | Romance | Sci-Fi |\r\n",
      "              Thriller | War | Western |\r\n",
      "              The last 19 fields are the genres, a 1 indicates the movie\r\n",
      "              is of that genre, a 0 indicates it is not; movies can be in\r\n",
      "              several genres at once.\r\n",
      "              The movie ids are the ones used in the u.data data set.\r\n",
      "\r\n",
      "u.genre    -- A list of the genres.\r\n",
      "\r\n",
      "u.user     -- Demographic information about the users; this is a tab\r\n",
      "              separated list of\r\n",
      "              user id | age | gender | occupation | zip code\r\n",
      "              The user ids are the ones used in the u.data data set.\r\n",
      "\r\n",
      "u.occupation -- A list of the occupations.\r\n",
      "\r\n",
      "u1.base    -- The data sets u1.base and u1.test through u5.base and u5.test\r\n",
      "u1.test       are 80%/20% splits of the u data into training and test data.\r\n",
      "u2.base       Each of u1, ..., u5 have disjoint test sets; this if for\r\n",
      "u2.test       5 fold cross validation (where you repeat your experiment\r\n",
      "u3.base       with each training and test set and average the results).\r\n",
      "u3.test       These data sets can be generated from u.data by mku.sh.\r\n",
      "u4.base\r\n",
      "u4.test\r\n",
      "u5.base\r\n",
      "u5.test\r\n",
      "\r\n",
      "ua.base    -- The data sets ua.base, ua.test, ub.base, and ub.test\r\n",
      "ua.test       split the u data into a training set and a test set with\r\n",
      "ub.base       exactly 10 ratings per user in the test set.  The sets\r\n",
      "ub.test       ua.test and ub.test are disjoint.  These data sets can\r\n",
      "              be generated from u.data by mku.sh.\r\n",
      "\r\n",
      "allbut.pl  -- The script that generates training and test sets where\r\n",
      "              all but n of a users ratings are in the training data.\r\n",
      "\r\n",
      "mku.sh     -- A shell script to generate all the u data sets from u.data.\r\n"
     ]
    }
   ],
   "source": [
    "# take a look at the readme file\n",
    "data_root = \"/home/dsc/repos/master-kschool/15_Sistemas_de_recomendacion/ml-100k/\"\n",
    "readme = os.path.join(data_root, 'README')\n",
    "!cat $readme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>196</td>\n",
       "      <td>242</td>\n",
       "      <td>3</td>\n",
       "      <td>881250949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>186</td>\n",
       "      <td>302</td>\n",
       "      <td>3</td>\n",
       "      <td>891717742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22</td>\n",
       "      <td>377</td>\n",
       "      <td>1</td>\n",
       "      <td>878887116</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>244</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>880606923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>166</td>\n",
       "      <td>346</td>\n",
       "      <td>1</td>\n",
       "      <td>886397596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0      196      242       3  881250949\n",
       "1      186      302       3  891717742\n",
       "2       22      377       1  878887116\n",
       "3      244       51       2  880606923\n",
       "4      166      346       1  886397596"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# display some data\n",
    "columns = ['user_id' , 'item_id' , 'rating' , 'timestamp']\n",
    "datafile = os.path.join(data_root, 'u.data')\n",
    "data = pd.read_csv(datafile, sep='\\t', names = columns)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Pandas library is nothing alse than numpy under the hood (numpy with steroids, if you like). You can access the data (in matrix from) with he \"values\" attribute, e.g. data.values*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 196,  242,    3],\n",
       "       [ 186,  302,    3],\n",
       "       [  22,  377,    1],\n",
       "       ...,\n",
       "       [ 276, 1090,    1],\n",
       "       [  13,  225,    2],\n",
       "       [  12,  203,    3]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access all rows, and first 3 columns \n",
    "#[filas, columnas]\n",
    "data.values[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      196,       242,         3, 881250949],\n",
       "       [      186,       302,         3, 891717742],\n",
       "       [       22,       377,         1, 878887116]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access all collumns, and first 3 rows \n",
    "data.values[:3,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[       62,       257,         2, 879372434],\n",
       "       [      286,      1014,         5, 879781125],\n",
       "       [      200,       222,         5, 876042340],\n",
       "       ...,\n",
       "       [      276,      1090,         1, 874795795],\n",
       "       [       13,       225,         2, 882399156],\n",
       "       [       12,       203,         3, 879959583]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access first 10 rows\n",
    "data.values[10:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([196, 186,  22, ..., 276,  13,  12])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# access first column\n",
    "data.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The attribute shape provides the shape of the matrix\n",
    "data.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000,)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Note that if we return the first column, we get a vector (of 100000 components)\n",
    "data.values[:,0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same with the first row (this time, we get a vector of 4 components)\n",
    "data.values[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 943 users and 1682 item\n"
     ]
    }
   ],
   "source": [
    "# Number of users and items\n",
    "n_users = data.user_id.unique().shape[0]\n",
    "n_items = data.item_id.unique().shape[0]\n",
    "print('There are %s users and %s item' % (n_users, n_items))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 A dictionary for movies and a search tool\n",
    "\n",
    "In order to analyze the predicted recommendations, let's create a python dictonary that will allow us to translate any item id to the corresponding movie title. Also, let's write a small function that returns the ids of the movies containing some text.\n",
    "\n",
    "The correspondance between titles and ids is stored in the u.item file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1|Toy Story (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)|0|0|0|1|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0\r\n",
      "2|GoldenEye (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?GoldenEye%20(1995)|0|1|1|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\r\n",
      "3|Four Rooms (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Four%20Rooms%20(1995)|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|0|1|0|0\r\n",
      "4|Get Shorty (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Get%20Shorty%20(1995)|0|1|0|0|0|1|0|0|1|0|0|0|0|0|0|0|0|0|0\r\n",
      "5|Copycat (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Copycat%20(1995)|0|0|0|0|0|0|1|0|1|0|0|0|0|0|0|0|1|0|0\r\n",
      "6|Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)|01-Jan-1995||http://us.imdb.com/Title?Yao+a+yao+yao+dao+waipo+qiao+(1995)|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0|0|0\r\n",
      "7|Twelve Monkeys (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Twelve%20Monkeys%20(1995)|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|1|0|0|0\r\n",
      "8|Babe (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Babe%20(1995)|0|0|0|0|1|1|0|0|1|0|0|0|0|0|0|0|0|0|0\r\n",
      "9|Dead Man Walking (1995)|01-Jan-1995||http://us.imdb.com/M/title-exact?Dead%20Man%20Walking%20(1995)|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0|0|0\r\n",
      "10|Richard III (1995)|22-Jan-1996||http://us.imdb.com/M/title-exact?Richard%20III%20(1995)|0|0|0|0|0|0|0|0|1|0|0|0|0|0|0|0|0|1|0\r\n"
     ]
    }
   ],
   "source": [
    "# u.item file\n",
    "data_root = '/home/dsc/repos/master-kschool/15_Sistemas_de_recomendacion/ml-100k/'\n",
    "items_id_file = os.path.join(data_root, 'u.item')\n",
    "!head $items_id_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Simple reminder of dictionaries*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux dictionary\n",
    "aux = {'hola': 'que haces', 1:'234'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'que haces'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Access value of key='hola'\n",
    "aux['hola']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hola': 'que haces', 1: '234', 'nuevo': 'soy nuevo'}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new key\n",
    "aux['nuevo'] = 'soy nuevo'\n",
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hola': 'que haces', 1: '234', 'nuevo': 'ya no lo soy'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update value of existing key\n",
    "aux['nuevo'] = 'ya no lo soy'\n",
    "aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User 196 viewed 'b'Kolya (1996)'' and gave a 3 rating\n",
      "User 186 viewed 'b'L.A. Confidential (1997)'' and gave a 3 rating\n",
      "User 22 viewed 'b'Heavyweights (1994)'' and gave a 1 rating\n",
      "User 244 viewed 'b'Legends of the Fall (1994)'' and gave a 2 rating\n",
      "User 166 viewed 'b'Jackie Brown (1997)'' and gave a 1 rating\n",
      "User 298 viewed 'b'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)'' and gave a 4 rating\n",
      "User 115 viewed 'b'Hunt for Red October, The (1990)'' and gave a 2 rating\n",
      "User 253 viewed 'b'Jungle Book, The (1994)'' and gave a 5 rating\n",
      "User 305 viewed 'b'Grease (1978)'' and gave a 3 rating\n",
      "User 6 viewed 'b'Remains of the Day, The (1993)'' and gave a 3 rating\n",
      "User 62 viewed 'b'Men in Black (1997)'' and gave a 2 rating\n",
      "User 286 viewed 'b\"Romy and Michele's High School Reunion (1997)\"' and gave a 5 rating\n",
      "User 200 viewed 'b'Star Trek: First Contact (1996)'' and gave a 5 rating\n",
      "User 210 viewed 'b'To Wong Foo, Thanks for Everything! Julie Newmar (1995)'' and gave a 3 rating\n",
      "User 224 viewed 'b'Batman Forever (1995)'' and gave a 3 rating\n",
      "User 303 viewed 'b'Only You (1994)'' and gave a 3 rating\n",
      "User 122 viewed 'b'Age of Innocence, The (1993)'' and gave a 5 rating\n",
      "User 194 viewed 'b'Sabrina (1995)'' and gave a 2 rating\n",
      "User 291 viewed 'b'Just Cause (1995)'' and gave a 4 rating\n",
      "User 234 viewed 'b'Endless Summer 2, The (1994)'' and gave a 2 rating\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary for movie titles and ids\n",
    "item_dict = {}\n",
    "with io.open(items_id_file, 'rb') as f:\n",
    "    for line in f.readlines():\n",
    "        record = line.split(b'|')\n",
    "        item_dict[int(record[0])] = str(record[1])\n",
    "\n",
    "    \n",
    "# We can use this dict to see the films a user has seen, for instance. \n",
    "for record in data.values[:20]:\n",
    "    print(\"User {u} viewed '{m}' and gave a {r} rating\".format(\n",
    "        u=record[0], m=item_dict[record[1]], r=record[2]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: \"b'Toy Story (1995)'\",\n",
       " 2: \"b'GoldenEye (1995)'\",\n",
       " 3: \"b'Four Rooms (1995)'\",\n",
       " 4: \"b'Get Shorty (1995)'\",\n",
       " 5: \"b'Copycat (1995)'\",\n",
       " 6: \"b'Shanghai Triad (Yao a yao yao dao waipo qiao) (1995)'\",\n",
       " 7: \"b'Twelve Monkeys (1995)'\",\n",
       " 8: \"b'Babe (1995)'\",\n",
       " 9: \"b'Dead Man Walking (1995)'\",\n",
       " 10: \"b'Richard III (1995)'\",\n",
       " 11: \"b'Seven (Se7en) (1995)'\",\n",
       " 12: \"b'Usual Suspects, The (1995)'\",\n",
       " 13: \"b'Mighty Aphrodite (1995)'\",\n",
       " 14: \"b'Postino, Il (1994)'\",\n",
       " 15: 'b\"Mr. Holland\\'s Opus (1995)\"',\n",
       " 16: \"b'French Twist (Gazon maudit) (1995)'\",\n",
       " 17: \"b'From Dusk Till Dawn (1996)'\",\n",
       " 18: \"b'White Balloon, The (1995)'\",\n",
       " 19: 'b\"Antonia\\'s Line (1995)\"',\n",
       " 20: \"b'Angels and Insects (1995)'\",\n",
       " 21: \"b'Muppet Treasure Island (1996)'\",\n",
       " 22: \"b'Braveheart (1995)'\",\n",
       " 23: \"b'Taxi Driver (1976)'\",\n",
       " 24: \"b'Rumble in the Bronx (1995)'\",\n",
       " 25: \"b'Birdcage, The (1996)'\",\n",
       " 26: \"b'Brothers McMullen, The (1995)'\",\n",
       " 27: \"b'Bad Boys (1995)'\",\n",
       " 28: \"b'Apollo 13 (1995)'\",\n",
       " 29: \"b'Batman Forever (1995)'\",\n",
       " 30: \"b'Belle de jour (1967)'\",\n",
       " 31: \"b'Crimson Tide (1995)'\",\n",
       " 32: \"b'Crumb (1994)'\",\n",
       " 33: \"b'Desperado (1995)'\",\n",
       " 34: \"b'Doom Generation, The (1995)'\",\n",
       " 35: \"b'Free Willy 2: The Adventure Home (1995)'\",\n",
       " 36: \"b'Mad Love (1995)'\",\n",
       " 37: \"b'Nadja (1994)'\",\n",
       " 38: \"b'Net, The (1995)'\",\n",
       " 39: \"b'Strange Days (1995)'\",\n",
       " 40: \"b'To Wong Foo, Thanks for Everything! Julie Newmar (1995)'\",\n",
       " 41: \"b'Billy Madison (1995)'\",\n",
       " 42: \"b'Clerks (1994)'\",\n",
       " 43: \"b'Disclosure (1994)'\",\n",
       " 44: \"b'Dolores Claiborne (1994)'\",\n",
       " 45: \"b'Eat Drink Man Woman (1994)'\",\n",
       " 46: \"b'Exotica (1994)'\",\n",
       " 47: \"b'Ed Wood (1994)'\",\n",
       " 48: \"b'Hoop Dreams (1994)'\",\n",
       " 49: \"b'I.Q. (1994)'\",\n",
       " 50: \"b'Star Wars (1977)'\",\n",
       " 51: \"b'Legends of the Fall (1994)'\",\n",
       " 52: \"b'Madness of King George, The (1994)'\",\n",
       " 53: \"b'Natural Born Killers (1994)'\",\n",
       " 54: \"b'Outbreak (1995)'\",\n",
       " 55: \"b'Professional, The (1994)'\",\n",
       " 56: \"b'Pulp Fiction (1994)'\",\n",
       " 57: \"b'Priest (1994)'\",\n",
       " 58: \"b'Quiz Show (1994)'\",\n",
       " 59: \"b'Three Colors: Red (1994)'\",\n",
       " 60: \"b'Three Colors: Blue (1993)'\",\n",
       " 61: \"b'Three Colors: White (1994)'\",\n",
       " 62: \"b'Stargate (1994)'\",\n",
       " 63: \"b'Santa Clause, The (1994)'\",\n",
       " 64: \"b'Shawshank Redemption, The (1994)'\",\n",
       " 65: 'b\"What\\'s Eating Gilbert Grape (1993)\"',\n",
       " 66: \"b'While You Were Sleeping (1995)'\",\n",
       " 67: \"b'Ace Ventura: Pet Detective (1994)'\",\n",
       " 68: \"b'Crow, The (1994)'\",\n",
       " 69: \"b'Forrest Gump (1994)'\",\n",
       " 70: \"b'Four Weddings and a Funeral (1994)'\",\n",
       " 71: \"b'Lion King, The (1994)'\",\n",
       " 72: \"b'Mask, The (1994)'\",\n",
       " 73: \"b'Maverick (1994)'\",\n",
       " 74: \"b'Faster Pussycat! Kill! Kill! (1965)'\",\n",
       " 75: \"b'Brother Minister: The Assassination of Malcolm X (1994)'\",\n",
       " 76: 'b\"Carlito\\'s Way (1993)\"',\n",
       " 77: \"b'Firm, The (1993)'\",\n",
       " 78: \"b'Free Willy (1993)'\",\n",
       " 79: \"b'Fugitive, The (1993)'\",\n",
       " 80: \"b'Hot Shots! Part Deux (1993)'\",\n",
       " 81: \"b'Hudsucker Proxy, The (1994)'\",\n",
       " 82: \"b'Jurassic Park (1993)'\",\n",
       " 83: \"b'Much Ado About Nothing (1993)'\",\n",
       " 84: 'b\"Robert A. Heinlein\\'s The Puppet Masters (1994)\"',\n",
       " 85: \"b'Ref, The (1994)'\",\n",
       " 86: \"b'Remains of the Day, The (1993)'\",\n",
       " 87: \"b'Searching for Bobby Fischer (1993)'\",\n",
       " 88: \"b'Sleepless in Seattle (1993)'\",\n",
       " 89: \"b'Blade Runner (1982)'\",\n",
       " 90: \"b'So I Married an Axe Murderer (1993)'\",\n",
       " 91: \"b'Nightmare Before Christmas, The (1993)'\",\n",
       " 92: \"b'True Romance (1993)'\",\n",
       " 93: \"b'Welcome to the Dollhouse (1995)'\",\n",
       " 94: \"b'Home Alone (1990)'\",\n",
       " 95: \"b'Aladdin (1992)'\",\n",
       " 96: \"b'Terminator 2: Judgment Day (1991)'\",\n",
       " 97: \"b'Dances with Wolves (1990)'\",\n",
       " 98: \"b'Silence of the Lambs, The (1991)'\",\n",
       " 99: \"b'Snow White and the Seven Dwarfs (1937)'\",\n",
       " 100: \"b'Fargo (1996)'\",\n",
       " 101: \"b'Heavy Metal (1981)'\",\n",
       " 102: \"b'Aristocats, The (1970)'\",\n",
       " 103: \"b'All Dogs Go to Heaven 2 (1996)'\",\n",
       " 104: \"b'Theodore Rex (1995)'\",\n",
       " 105: \"b'Sgt. Bilko (1996)'\",\n",
       " 106: \"b'Diabolique (1996)'\",\n",
       " 107: \"b'Moll Flanders (1996)'\",\n",
       " 108: \"b'Kids in the Hall: Brain Candy (1996)'\",\n",
       " 109: \"b'Mystery Science Theater 3000: The Movie (1996)'\",\n",
       " 110: \"b'Operation Dumbo Drop (1995)'\",\n",
       " 111: \"b'Truth About Cats & Dogs, The (1996)'\",\n",
       " 112: \"b'Flipper (1996)'\",\n",
       " 113: \"b'Horseman on the Roof, The (Hussard sur le toit, Le) (1995)'\",\n",
       " 114: \"b'Wallace & Gromit: The Best of Aardman Animation (1996)'\",\n",
       " 115: \"b'Haunted World of Edward D. Wood Jr., The (1995)'\",\n",
       " 116: \"b'Cold Comfort Farm (1995)'\",\n",
       " 117: \"b'Rock, The (1996)'\",\n",
       " 118: \"b'Twister (1996)'\",\n",
       " 119: \"b'Maya Lin: A Strong Clear Vision (1994)'\",\n",
       " 120: \"b'Striptease (1996)'\",\n",
       " 121: \"b'Independence Day (ID4) (1996)'\",\n",
       " 122: \"b'Cable Guy, The (1996)'\",\n",
       " 123: \"b'Frighteners, The (1996)'\",\n",
       " 124: \"b'Lone Star (1996)'\",\n",
       " 125: \"b'Phenomenon (1996)'\",\n",
       " 126: \"b'Spitfire Grill, The (1996)'\",\n",
       " 127: \"b'Godfather, The (1972)'\",\n",
       " 128: \"b'Supercop (1992)'\",\n",
       " 129: \"b'Bound (1996)'\",\n",
       " 130: \"b'Kansas City (1996)'\",\n",
       " 131: 'b\"Breakfast at Tiffany\\'s (1961)\"',\n",
       " 132: \"b'Wizard of Oz, The (1939)'\",\n",
       " 133: \"b'Gone with the Wind (1939)'\",\n",
       " 134: \"b'Citizen Kane (1941)'\",\n",
       " 135: \"b'2001: A Space Odyssey (1968)'\",\n",
       " 136: \"b'Mr. Smith Goes to Washington (1939)'\",\n",
       " 137: \"b'Big Night (1996)'\",\n",
       " 138: \"b'D3: The Mighty Ducks (1996)'\",\n",
       " 139: \"b'Love Bug, The (1969)'\",\n",
       " 140: \"b'Homeward Bound: The Incredible Journey (1993)'\",\n",
       " 141: \"b'20,000 Leagues Under the Sea (1954)'\",\n",
       " 142: \"b'Bedknobs and Broomsticks (1971)'\",\n",
       " 143: \"b'Sound of Music, The (1965)'\",\n",
       " 144: \"b'Die Hard (1988)'\",\n",
       " 145: \"b'Lawnmower Man, The (1992)'\",\n",
       " 146: \"b'Unhook the Stars (1996)'\",\n",
       " 147: \"b'Long Kiss Goodnight, The (1996)'\",\n",
       " 148: \"b'Ghost and the Darkness, The (1996)'\",\n",
       " 149: \"b'Jude (1996)'\",\n",
       " 150: \"b'Swingers (1996)'\",\n",
       " 151: \"b'Willy Wonka and the Chocolate Factory (1971)'\",\n",
       " 152: \"b'Sleeper (1973)'\",\n",
       " 153: \"b'Fish Called Wanda, A (1988)'\",\n",
       " 154: 'b\"Monty Python\\'s Life of Brian (1979)\"',\n",
       " 155: \"b'Dirty Dancing (1987)'\",\n",
       " 156: \"b'Reservoir Dogs (1992)'\",\n",
       " 157: \"b'Platoon (1986)'\",\n",
       " 158: 'b\"Weekend at Bernie\\'s (1989)\"',\n",
       " 159: \"b'Basic Instinct (1992)'\",\n",
       " 160: \"b'Glengarry Glen Ross (1992)'\",\n",
       " 161: \"b'Top Gun (1986)'\",\n",
       " 162: \"b'On Golden Pond (1981)'\",\n",
       " 163: \"b'Return of the Pink Panther, The (1974)'\",\n",
       " 164: \"b'Abyss, The (1989)'\",\n",
       " 165: \"b'Jean de Florette (1986)'\",\n",
       " 166: \"b'Manon of the Spring (Manon des sources) (1986)'\",\n",
       " 167: \"b'Private Benjamin (1980)'\",\n",
       " 168: \"b'Monty Python and the Holy Grail (1974)'\",\n",
       " 169: \"b'Wrong Trousers, The (1993)'\",\n",
       " 170: \"b'Cinema Paradiso (1988)'\",\n",
       " 171: \"b'Delicatessen (1991)'\",\n",
       " 172: \"b'Empire Strikes Back, The (1980)'\",\n",
       " 173: \"b'Princess Bride, The (1987)'\",\n",
       " 174: \"b'Raiders of the Lost Ark (1981)'\",\n",
       " 175: \"b'Brazil (1985)'\",\n",
       " 176: \"b'Aliens (1986)'\",\n",
       " 177: \"b'Good, The Bad and The Ugly, The (1966)'\",\n",
       " 178: \"b'12 Angry Men (1957)'\",\n",
       " 179: \"b'Clockwork Orange, A (1971)'\",\n",
       " 180: \"b'Apocalypse Now (1979)'\",\n",
       " 181: \"b'Return of the Jedi (1983)'\",\n",
       " 182: \"b'GoodFellas (1990)'\",\n",
       " 183: \"b'Alien (1979)'\",\n",
       " 184: \"b'Army of Darkness (1993)'\",\n",
       " 185: \"b'Psycho (1960)'\",\n",
       " 186: \"b'Blues Brothers, The (1980)'\",\n",
       " 187: \"b'Godfather: Part II, The (1974)'\",\n",
       " 188: \"b'Full Metal Jacket (1987)'\",\n",
       " 189: \"b'Grand Day Out, A (1992)'\",\n",
       " 190: \"b'Henry V (1989)'\",\n",
       " 191: \"b'Amadeus (1984)'\",\n",
       " 192: \"b'Raging Bull (1980)'\",\n",
       " 193: \"b'Right Stuff, The (1983)'\",\n",
       " 194: \"b'Sting, The (1973)'\",\n",
       " 195: \"b'Terminator, The (1984)'\",\n",
       " 196: \"b'Dead Poets Society (1989)'\",\n",
       " 197: \"b'Graduate, The (1967)'\",\n",
       " 198: \"b'Nikita (La Femme Nikita) (1990)'\",\n",
       " 199: \"b'Bridge on the River Kwai, The (1957)'\",\n",
       " 200: \"b'Shining, The (1980)'\",\n",
       " 201: \"b'Evil Dead II (1987)'\",\n",
       " 202: \"b'Groundhog Day (1993)'\",\n",
       " 203: \"b'Unforgiven (1992)'\",\n",
       " 204: \"b'Back to the Future (1985)'\",\n",
       " 205: \"b'Patton (1970)'\",\n",
       " 206: \"b'Akira (1988)'\",\n",
       " 207: \"b'Cyrano de Bergerac (1990)'\",\n",
       " 208: \"b'Young Frankenstein (1974)'\",\n",
       " 209: \"b'This Is Spinal Tap (1984)'\",\n",
       " 210: \"b'Indiana Jones and the Last Crusade (1989)'\",\n",
       " 211: \"b'M*A*S*H (1970)'\",\n",
       " 212: \"b'Unbearable Lightness of Being, The (1988)'\",\n",
       " 213: \"b'Room with a View, A (1986)'\",\n",
       " 214: \"b'Pink Floyd - The Wall (1982)'\",\n",
       " 215: \"b'Field of Dreams (1989)'\",\n",
       " 216: \"b'When Harry Met Sally... (1989)'\",\n",
       " 217: 'b\"Bram Stoker\\'s Dracula (1992)\"',\n",
       " 218: \"b'Cape Fear (1991)'\",\n",
       " 219: \"b'Nightmare on Elm Street, A (1984)'\",\n",
       " 220: \"b'Mirror Has Two Faces, The (1996)'\",\n",
       " 221: \"b'Breaking the Waves (1996)'\",\n",
       " 222: \"b'Star Trek: First Contact (1996)'\",\n",
       " 223: \"b'Sling Blade (1996)'\",\n",
       " 224: \"b'Ridicule (1996)'\",\n",
       " 225: \"b'101 Dalmatians (1996)'\",\n",
       " 226: \"b'Die Hard 2 (1990)'\",\n",
       " 227: \"b'Star Trek VI: The Undiscovered Country (1991)'\",\n",
       " 228: \"b'Star Trek: The Wrath of Khan (1982)'\",\n",
       " 229: \"b'Star Trek III: The Search for Spock (1984)'\",\n",
       " 230: \"b'Star Trek IV: The Voyage Home (1986)'\",\n",
       " 231: \"b'Batman Returns (1992)'\",\n",
       " 232: \"b'Young Guns (1988)'\",\n",
       " 233: \"b'Under Siege (1992)'\",\n",
       " 234: \"b'Jaws (1975)'\",\n",
       " 235: \"b'Mars Attacks! (1996)'\",\n",
       " 236: \"b'Citizen Ruth (1996)'\",\n",
       " 237: \"b'Jerry Maguire (1996)'\",\n",
       " 238: \"b'Raising Arizona (1987)'\",\n",
       " 239: \"b'Sneakers (1992)'\",\n",
       " 240: \"b'Beavis and Butt-head Do America (1996)'\",\n",
       " 241: \"b'Last of the Mohicans, The (1992)'\",\n",
       " 242: \"b'Kolya (1996)'\",\n",
       " 243: \"b'Jungle2Jungle (1997)'\",\n",
       " 244: 'b\"Smilla\\'s Sense of Snow (1997)\"',\n",
       " 245: 'b\"Devil\\'s Own, The (1997)\"',\n",
       " 246: \"b'Chasing Amy (1997)'\",\n",
       " 247: \"b'Turbo: A Power Rangers Movie (1997)'\",\n",
       " 248: \"b'Grosse Pointe Blank (1997)'\",\n",
       " 249: \"b'Austin Powers: International Man of Mystery (1997)'\",\n",
       " 250: \"b'Fifth Element, The (1997)'\",\n",
       " 251: \"b'Shall We Dance? (1996)'\",\n",
       " 252: \"b'Lost World: Jurassic Park, The (1997)'\",\n",
       " 253: \"b'Pillow Book, The (1995)'\",\n",
       " 254: \"b'Batman & Robin (1997)'\",\n",
       " 255: 'b\"My Best Friend\\'s Wedding (1997)\"',\n",
       " 256: \"b'When the Cats Away (Chacun cherche son chat) (1996)'\",\n",
       " 257: \"b'Men in Black (1997)'\",\n",
       " 258: \"b'Contact (1997)'\",\n",
       " 259: \"b'George of the Jungle (1997)'\",\n",
       " 260: \"b'Event Horizon (1997)'\",\n",
       " 261: \"b'Air Bud (1997)'\",\n",
       " 262: \"b'In the Company of Men (1997)'\",\n",
       " 263: \"b'Steel (1997)'\",\n",
       " 264: \"b'Mimic (1997)'\",\n",
       " 265: \"b'Hunt for Red October, The (1990)'\",\n",
       " 266: \"b'Kull the Conqueror (1997)'\",\n",
       " 267: \"b'unknown'\",\n",
       " 268: \"b'Chasing Amy (1997)'\",\n",
       " 269: \"b'Full Monty, The (1997)'\",\n",
       " 270: \"b'Gattaca (1997)'\",\n",
       " 271: \"b'Starship Troopers (1997)'\",\n",
       " 272: \"b'Good Will Hunting (1997)'\",\n",
       " 273: \"b'Heat (1995)'\",\n",
       " 274: \"b'Sabrina (1995)'\",\n",
       " 275: \"b'Sense and Sensibility (1995)'\",\n",
       " 276: \"b'Leaving Las Vegas (1995)'\",\n",
       " 277: \"b'Restoration (1995)'\",\n",
       " 278: \"b'Bed of Roses (1996)'\",\n",
       " 279: \"b'Once Upon a Time... When We Were Colored (1995)'\",\n",
       " 280: \"b'Up Close and Personal (1996)'\",\n",
       " 281: \"b'River Wild, The (1994)'\",\n",
       " 282: \"b'Time to Kill, A (1996)'\",\n",
       " 283: \"b'Emma (1996)'\",\n",
       " 284: \"b'Tin Cup (1996)'\",\n",
       " 285: \"b'Secrets & Lies (1996)'\",\n",
       " 286: \"b'English Patient, The (1996)'\",\n",
       " 287: 'b\"Marvin\\'s Room (1996)\"',\n",
       " 288: \"b'Scream (1996)'\",\n",
       " 289: \"b'Evita (1996)'\",\n",
       " 290: \"b'Fierce Creatures (1997)'\",\n",
       " 291: \"b'Absolute Power (1997)'\",\n",
       " 292: \"b'Rosewood (1997)'\",\n",
       " 293: \"b'Donnie Brasco (1997)'\",\n",
       " 294: \"b'Liar Liar (1997)'\",\n",
       " 295: \"b'Breakdown (1997)'\",\n",
       " 296: \"b'Promesse, La (1996)'\",\n",
       " 297: 'b\"Ulee\\'s Gold (1997)\"',\n",
       " 298: \"b'Face/Off (1997)'\",\n",
       " 299: \"b'Hoodlum (1997)'\",\n",
       " 300: \"b'Air Force One (1997)'\",\n",
       " 301: \"b'In & Out (1997)'\",\n",
       " 302: \"b'L.A. Confidential (1997)'\",\n",
       " 303: 'b\"Ulee\\'s Gold (1997)\"',\n",
       " 304: \"b'Fly Away Home (1996)'\",\n",
       " 305: \"b'Ice Storm, The (1997)'\",\n",
       " 306: \"b'Mrs. Brown (Her Majesty, Mrs. Brown) (1997)'\",\n",
       " 307: 'b\"Devil\\'s Advocate, The (1997)\"',\n",
       " 308: \"b'FairyTale: A True Story (1997)'\",\n",
       " 309: \"b'Deceiver (1997)'\",\n",
       " 310: \"b'Rainmaker, The (1997)'\",\n",
       " 311: \"b'Wings of the Dove, The (1997)'\",\n",
       " 312: \"b'Midnight in the Garden of Good and Evil (1997)'\",\n",
       " 313: \"b'Titanic (1997)'\",\n",
       " 314: \"b'3 Ninjas: High Noon At Mega Mountain (1998)'\",\n",
       " 315: \"b'Apt Pupil (1998)'\",\n",
       " 316: \"b'As Good As It Gets (1997)'\",\n",
       " 317: \"b'In the Name of the Father (1993)'\",\n",
       " 318: 'b\"Schindler\\'s List (1993)\"',\n",
       " 319: \"b'Everyone Says I Love You (1996)'\",\n",
       " 320: \"b'Paradise Lost: The Child Murders at Robin Hood Hills (1996)'\",\n",
       " 321: \"b'Mother (1996)'\",\n",
       " 322: \"b'Murder at 1600 (1997)'\",\n",
       " 323: 'b\"Dante\\'s Peak (1997)\"',\n",
       " 324: \"b'Lost Highway (1997)'\",\n",
       " 325: \"b'Crash (1996)'\",\n",
       " 326: \"b'G.I. Jane (1997)'\",\n",
       " 327: \"b'Cop Land (1997)'\",\n",
       " 328: \"b'Conspiracy Theory (1997)'\",\n",
       " 329: \"b'Desperate Measures (1998)'\",\n",
       " 330: \"b'187 (1997)'\",\n",
       " 331: \"b'Edge, The (1997)'\",\n",
       " 332: \"b'Kiss the Girls (1997)'\",\n",
       " 333: \"b'Game, The (1997)'\",\n",
       " 334: \"b'U Turn (1997)'\",\n",
       " 335: \"b'How to Be a Player (1997)'\",\n",
       " 336: \"b'Playing God (1997)'\",\n",
       " 337: \"b'House of Yes, The (1997)'\",\n",
       " 338: \"b'Bean (1997)'\",\n",
       " 339: \"b'Mad City (1997)'\",\n",
       " 340: \"b'Boogie Nights (1997)'\",\n",
       " 341: \"b'Critical Care (1997)'\",\n",
       " 342: \"b'Man Who Knew Too Little, The (1997)'\",\n",
       " 343: \"b'Alien: Resurrection (1997)'\",\n",
       " 344: \"b'Apostle, The (1997)'\",\n",
       " 345: \"b'Deconstructing Harry (1997)'\",\n",
       " 346: \"b'Jackie Brown (1997)'\",\n",
       " 347: \"b'Wag the Dog (1997)'\",\n",
       " 348: \"b'Desperate Measures (1998)'\",\n",
       " 349: \"b'Hard Rain (1998)'\",\n",
       " 350: \"b'Fallen (1998)'\",\n",
       " 351: \"b'Prophecy II, The (1998)'\",\n",
       " 352: \"b'Spice World (1997)'\",\n",
       " 353: \"b'Deep Rising (1998)'\",\n",
       " 354: \"b'Wedding Singer, The (1998)'\",\n",
       " 355: \"b'Sphere (1998)'\",\n",
       " 356: \"b'Client, The (1994)'\",\n",
       " 357: 'b\"One Flew Over the Cuckoo\\'s Nest (1975)\"',\n",
       " 358: \"b'Spawn (1997)'\",\n",
       " 359: \"b'Assignment, The (1997)'\",\n",
       " 360: \"b'Wonderland (1997)'\",\n",
       " 361: \"b'Incognito (1997)'\",\n",
       " 362: \"b'Blues Brothers 2000 (1998)'\",\n",
       " 363: \"b'Sudden Death (1995)'\",\n",
       " 364: \"b'Ace Ventura: When Nature Calls (1995)'\",\n",
       " 365: \"b'Powder (1995)'\",\n",
       " 366: \"b'Dangerous Minds (1995)'\",\n",
       " 367: \"b'Clueless (1995)'\",\n",
       " 368: \"b'Bio-Dome (1996)'\",\n",
       " 369: \"b'Black Sheep (1996)'\",\n",
       " 370: \"b'Mary Reilly (1996)'\",\n",
       " 371: \"b'Bridges of Madison County, The (1995)'\",\n",
       " 372: \"b'Jeffrey (1995)'\",\n",
       " 373: \"b'Judge Dredd (1995)'\",\n",
       " 374: \"b'Mighty Morphin Power Rangers: The Movie (1995)'\",\n",
       " 375: \"b'Showgirls (1995)'\",\n",
       " 376: \"b'Houseguest (1994)'\",\n",
       " 377: \"b'Heavyweights (1994)'\",\n",
       " 378: \"b'Miracle on 34th Street (1994)'\",\n",
       " 379: \"b'Tales From the Crypt Presents: Demon Knight (1995)'\",\n",
       " 380: \"b'Star Trek: Generations (1994)'\",\n",
       " 381: 'b\"Muriel\\'s Wedding (1994)\"',\n",
       " 382: \"b'Adventures of Priscilla, Queen of the Desert, The (1994)'\",\n",
       " 383: \"b'Flintstones, The (1994)'\",\n",
       " 384: \"b'Naked Gun 33 1/3: The Final Insult (1994)'\",\n",
       " 385: \"b'True Lies (1994)'\",\n",
       " 386: \"b'Addams Family Values (1993)'\",\n",
       " 387: \"b'Age of Innocence, The (1993)'\",\n",
       " 388: \"b'Beverly Hills Cop III (1994)'\",\n",
       " 389: \"b'Black Beauty (1994)'\",\n",
       " 390: \"b'Fear of a Black Hat (1993)'\",\n",
       " 391: \"b'Last Action Hero (1993)'\",\n",
       " 392: \"b'Man Without a Face, The (1993)'\",\n",
       " 393: \"b'Mrs. Doubtfire (1993)'\",\n",
       " 394: \"b'Radioland Murders (1994)'\",\n",
       " 395: \"b'Robin Hood: Men in Tights (1993)'\",\n",
       " 396: \"b'Serial Mom (1994)'\",\n",
       " 397: \"b'Striking Distance (1993)'\",\n",
       " 398: \"b'Super Mario Bros. (1993)'\",\n",
       " 399: \"b'Three Musketeers, The (1993)'\",\n",
       " 400: \"b'Little Rascals, The (1994)'\",\n",
       " 401: \"b'Brady Bunch Movie, The (1995)'\",\n",
       " 402: \"b'Ghost (1990)'\",\n",
       " 403: \"b'Batman (1989)'\",\n",
       " 404: \"b'Pinocchio (1940)'\",\n",
       " 405: \"b'Mission: Impossible (1996)'\",\n",
       " 406: \"b'Thinner (1996)'\",\n",
       " 407: \"b'Spy Hard (1996)'\",\n",
       " 408: \"b'Close Shave, A (1995)'\",\n",
       " 409: \"b'Jack (1996)'\",\n",
       " 410: \"b'Kingpin (1996)'\",\n",
       " 411: \"b'Nutty Professor, The (1996)'\",\n",
       " 412: \"b'Very Brady Sequel, A (1996)'\",\n",
       " 413: \"b'Tales from the Crypt Presents: Bordello of Blood (1996)'\",\n",
       " 414: \"b'My Favorite Year (1982)'\",\n",
       " 415: \"b'Apple Dumpling Gang, The (1975)'\",\n",
       " 416: \"b'Old Yeller (1957)'\",\n",
       " 417: \"b'Parent Trap, The (1961)'\",\n",
       " 418: \"b'Cinderella (1950)'\",\n",
       " 419: \"b'Mary Poppins (1964)'\",\n",
       " 420: \"b'Alice in Wonderland (1951)'\",\n",
       " 421: 'b\"William Shakespeare\\'s Romeo and Juliet (1996)\"',\n",
       " 422: \"b'Aladdin and the King of Thieves (1996)'\",\n",
       " 423: \"b'E.T. the Extra-Terrestrial (1982)'\",\n",
       " 424: \"b'Children of the Corn: The Gathering (1996)'\",\n",
       " 425: \"b'Bob Roberts (1992)'\",\n",
       " 426: \"b'Transformers: The Movie, The (1986)'\",\n",
       " 427: \"b'To Kill a Mockingbird (1962)'\",\n",
       " 428: \"b'Harold and Maude (1971)'\",\n",
       " 429: \"b'Day the Earth Stood Still, The (1951)'\",\n",
       " 430: \"b'Duck Soup (1933)'\",\n",
       " 431: \"b'Highlander (1986)'\",\n",
       " 432: \"b'Fantasia (1940)'\",\n",
       " 433: \"b'Heathers (1989)'\",\n",
       " 434: \"b'Forbidden Planet (1956)'\",\n",
       " 435: \"b'Butch Cassidy and the Sundance Kid (1969)'\",\n",
       " 436: \"b'American Werewolf in London, An (1981)'\",\n",
       " 437: 'b\"Amityville 1992: It\\'s About Time (1992)\"',\n",
       " 438: \"b'Amityville 3-D (1983)'\",\n",
       " 439: \"b'Amityville: A New Generation (1993)'\",\n",
       " 440: \"b'Amityville II: The Possession (1982)'\",\n",
       " 441: \"b'Amityville Horror, The (1979)'\",\n",
       " 442: \"b'Amityville Curse, The (1990)'\",\n",
       " 443: \"b'Birds, The (1963)'\",\n",
       " 444: \"b'Blob, The (1958)'\",\n",
       " 445: \"b'Body Snatcher, The (1945)'\",\n",
       " 446: \"b'Burnt Offerings (1976)'\",\n",
       " 447: \"b'Carrie (1976)'\",\n",
       " 448: \"b'Omen, The (1976)'\",\n",
       " 449: \"b'Star Trek: The Motion Picture (1979)'\",\n",
       " 450: \"b'Star Trek V: The Final Frontier (1989)'\",\n",
       " 451: \"b'Grease (1978)'\",\n",
       " 452: \"b'Jaws 2 (1978)'\",\n",
       " 453: \"b'Jaws 3-D (1983)'\",\n",
       " 454: \"b'Bastard Out of Carolina (1996)'\",\n",
       " 455: 'b\"Jackie Chan\\'s First Strike (1996)\"',\n",
       " 456: \"b'Beverly Hills Ninja (1997)'\",\n",
       " 457: \"b'Free Willy 3: The Rescue (1997)'\",\n",
       " 458: \"b'Nixon (1995)'\",\n",
       " 459: \"b'Cry, the Beloved Country (1995)'\",\n",
       " 460: \"b'Crossing Guard, The (1995)'\",\n",
       " 461: \"b'Smoke (1995)'\",\n",
       " 462: \"b'Like Water For Chocolate (Como agua para chocolate) (1992)'\",\n",
       " 463: \"b'Secret of Roan Inish, The (1994)'\",\n",
       " 464: \"b'Vanya on 42nd Street (1994)'\",\n",
       " 465: \"b'Jungle Book, The (1994)'\",\n",
       " 466: \"b'Red Rock West (1992)'\",\n",
       " 467: \"b'Bronx Tale, A (1993)'\",\n",
       " 468: \"b'Rudy (1993)'\",\n",
       " 469: \"b'Short Cuts (1993)'\",\n",
       " 470: \"b'Tombstone (1993)'\",\n",
       " 471: \"b'Courage Under Fire (1996)'\",\n",
       " 472: \"b'Dragonheart (1996)'\",\n",
       " 473: \"b'James and the Giant Peach (1996)'\",\n",
       " 474: \"b'Dr. Strangelove or: How I Learned to Stop Worrying and Love the Bomb (1963)'\",\n",
       " 475: \"b'Trainspotting (1996)'\",\n",
       " 476: \"b'First Wives Club, The (1996)'\",\n",
       " 477: \"b'Matilda (1996)'\",\n",
       " 478: \"b'Philadelphia Story, The (1940)'\",\n",
       " 479: \"b'Vertigo (1958)'\",\n",
       " 480: \"b'North by Northwest (1959)'\",\n",
       " 481: \"b'Apartment, The (1960)'\",\n",
       " 482: \"b'Some Like It Hot (1959)'\",\n",
       " 483: \"b'Casablanca (1942)'\",\n",
       " 484: \"b'Maltese Falcon, The (1941)'\",\n",
       " 485: \"b'My Fair Lady (1964)'\",\n",
       " 486: \"b'Sabrina (1954)'\",\n",
       " 487: \"b'Roman Holiday (1953)'\",\n",
       " 488: \"b'Sunset Blvd. (1950)'\",\n",
       " 489: \"b'Notorious (1946)'\",\n",
       " 490: \"b'To Catch a Thief (1955)'\",\n",
       " 491: \"b'Adventures of Robin Hood, The (1938)'\",\n",
       " 492: \"b'East of Eden (1955)'\",\n",
       " 493: \"b'Thin Man, The (1934)'\",\n",
       " 494: \"b'His Girl Friday (1940)'\",\n",
       " 495: \"b'Around the World in 80 Days (1956)'\",\n",
       " 496: 'b\"It\\'s a Wonderful Life (1946)\"',\n",
       " 497: \"b'Bringing Up Baby (1938)'\",\n",
       " 498: \"b'African Queen, The (1951)'\",\n",
       " 499: \"b'Cat on a Hot Tin Roof (1958)'\",\n",
       " 500: \"b'Fly Away Home (1996)'\",\n",
       " 501: \"b'Dumbo (1941)'\",\n",
       " 502: \"b'Bananas (1971)'\",\n",
       " 503: \"b'Candidate, The (1972)'\",\n",
       " 504: \"b'Bonnie and Clyde (1967)'\",\n",
       " 505: \"b'Dial M for Murder (1954)'\",\n",
       " 506: \"b'Rebel Without a Cause (1955)'\",\n",
       " 507: \"b'Streetcar Named Desire, A (1951)'\",\n",
       " 508: \"b'People vs. Larry Flynt, The (1996)'\",\n",
       " 509: \"b'My Left Foot (1989)'\",\n",
       " 510: \"b'Magnificent Seven, The (1954)'\",\n",
       " 511: \"b'Lawrence of Arabia (1962)'\",\n",
       " 512: \"b'Wings of Desire (1987)'\",\n",
       " 513: \"b'Third Man, The (1949)'\",\n",
       " 514: \"b'Annie Hall (1977)'\",\n",
       " 515: \"b'Boot, Das (1981)'\",\n",
       " 516: \"b'Local Hero (1983)'\",\n",
       " 517: \"b'Manhattan (1979)'\",\n",
       " 518: 'b\"Miller\\'s Crossing (1990)\"',\n",
       " 519: \"b'Treasure of the Sierra Madre, The (1948)'\",\n",
       " 520: \"b'Great Escape, The (1963)'\",\n",
       " 521: \"b'Deer Hunter, The (1978)'\",\n",
       " 522: \"b'Down by Law (1986)'\",\n",
       " 523: \"b'Cool Hand Luke (1967)'\",\n",
       " 524: \"b'Great Dictator, The (1940)'\",\n",
       " 525: \"b'Big Sleep, The (1946)'\",\n",
       " 526: \"b'Ben-Hur (1959)'\",\n",
       " 527: \"b'Gandhi (1982)'\",\n",
       " 528: \"b'Killing Fields, The (1984)'\",\n",
       " 529: \"b'My Life as a Dog (Mitt liv som hund) (1985)'\",\n",
       " 530: \"b'Man Who Would Be King, The (1975)'\",\n",
       " 531: \"b'Shine (1996)'\",\n",
       " 532: \"b'Kama Sutra: A Tale of Love (1996)'\",\n",
       " 533: \"b'Daytrippers, The (1996)'\",\n",
       " 534: \"b'Traveller (1997)'\",\n",
       " 535: \"b'Addicted to Love (1997)'\",\n",
       " 536: \"b'Ponette (1996)'\",\n",
       " 537: \"b'My Own Private Idaho (1991)'\",\n",
       " 538: \"b'Anastasia (1997)'\",\n",
       " 539: \"b'Mouse Hunt (1997)'\",\n",
       " 540: \"b'Money Train (1995)'\",\n",
       " 541: \"b'Mortal Kombat (1995)'\",\n",
       " 542: \"b'Pocahontas (1995)'\",\n",
       " 543: \"b'Mis\\\\xe9rables, Les (1995)'\",\n",
       " 544: 'b\"Things to Do in Denver when You\\'re Dead (1995)\"',\n",
       " 545: \"b'Vampire in Brooklyn (1995)'\",\n",
       " 546: \"b'Broken Arrow (1996)'\",\n",
       " 547: 'b\"Young Poisoner\\'s Handbook, The (1995)\"',\n",
       " 548: \"b'NeverEnding Story III, The (1994)'\",\n",
       " 549: \"b'Rob Roy (1995)'\",\n",
       " 550: \"b'Die Hard: With a Vengeance (1995)'\",\n",
       " 551: \"b'Lord of Illusions (1995)'\",\n",
       " 552: \"b'Species (1995)'\",\n",
       " 553: \"b'Walk in the Clouds, A (1995)'\",\n",
       " 554: \"b'Waterworld (1995)'\",\n",
       " 555: 'b\"White Man\\'s Burden (1995)\"',\n",
       " 556: \"b'Wild Bill (1995)'\",\n",
       " 557: \"b'Farinelli: il castrato (1994)'\",\n",
       " 558: \"b'Heavenly Creatures (1994)'\",\n",
       " 559: \"b'Interview with the Vampire (1994)'\",\n",
       " 560: 'b\"Kid in King Arthur\\'s Court, A (1995)\"',\n",
       " 561: 'b\"Mary Shelley\\'s Frankenstein (1994)\"',\n",
       " 562: \"b'Quick and the Dead, The (1995)'\",\n",
       " 563: 'b\"Stephen King\\'s The Langoliers (1995)\"',\n",
       " 564: \"b'Tales from the Hood (1995)'\",\n",
       " 565: \"b'Village of the Damned (1995)'\",\n",
       " 566: \"b'Clear and Present Danger (1994)'\",\n",
       " 567: 'b\"Wes Craven\\'s New Nightmare (1994)\"',\n",
       " 568: \"b'Speed (1994)'\",\n",
       " 569: \"b'Wolf (1994)'\",\n",
       " 570: \"b'Wyatt Earp (1994)'\",\n",
       " 571: \"b'Another Stakeout (1993)'\",\n",
       " 572: \"b'Blown Away (1994)'\",\n",
       " 573: \"b'Body Snatchers (1993)'\",\n",
       " 574: \"b'Boxing Helena (1993)'\",\n",
       " 575: 'b\"City Slickers II: The Legend of Curly\\'s Gold (1994)\"',\n",
       " 576: \"b'Cliffhanger (1993)'\",\n",
       " 577: \"b'Coneheads (1993)'\",\n",
       " 578: \"b'Demolition Man (1993)'\",\n",
       " 579: \"b'Fatal Instinct (1993)'\",\n",
       " 580: \"b'Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)'\",\n",
       " 581: \"b'Kalifornia (1993)'\",\n",
       " 582: \"b'Piano, The (1993)'\",\n",
       " 583: \"b'Romeo Is Bleeding (1993)'\",\n",
       " 584: \"b'Secret Garden, The (1993)'\",\n",
       " 585: \"b'Son in Law (1993)'\",\n",
       " 586: \"b'Terminal Velocity (1994)'\",\n",
       " 587: \"b'Hour of the Pig, The (1993)'\",\n",
       " 588: \"b'Beauty and the Beast (1991)'\",\n",
       " 589: \"b'Wild Bunch, The (1969)'\",\n",
       " 590: \"b'Hellraiser: Bloodline (1996)'\",\n",
       " 591: \"b'Primal Fear (1996)'\",\n",
       " 592: \"b'True Crime (1995)'\",\n",
       " 593: \"b'Stalingrad (1993)'\",\n",
       " 594: \"b'Heavy (1995)'\",\n",
       " 595: \"b'Fan, The (1996)'\",\n",
       " 596: \"b'Hunchback of Notre Dame, The (1996)'\",\n",
       " 597: \"b'Eraser (1996)'\",\n",
       " 598: \"b'Big Squeeze, The (1996)'\",\n",
       " 599: \"b'Police Story 4: Project S (Chao ji ji hua) (1993)'\",\n",
       " 600: 'b\"Daniel Defoe\\'s Robinson Crusoe (1996)\"',\n",
       " 601: \"b'For Whom the Bell Tolls (1943)'\",\n",
       " 602: \"b'American in Paris, An (1951)'\",\n",
       " 603: \"b'Rear Window (1954)'\",\n",
       " 604: \"b'It Happened One Night (1934)'\",\n",
       " 605: \"b'Meet Me in St. Louis (1944)'\",\n",
       " 606: \"b'All About Eve (1950)'\",\n",
       " 607: \"b'Rebecca (1940)'\",\n",
       " 608: \"b'Spellbound (1945)'\",\n",
       " 609: \"b'Father of the Bride (1950)'\",\n",
       " 610: \"b'Gigi (1958)'\",\n",
       " 611: \"b'Laura (1944)'\",\n",
       " 612: \"b'Lost Horizon (1937)'\",\n",
       " 613: \"b'My Man Godfrey (1936)'\",\n",
       " 614: \"b'Giant (1956)'\",\n",
       " 615: \"b'39 Steps, The (1935)'\",\n",
       " 616: \"b'Night of the Living Dead (1968)'\",\n",
       " 617: \"b'Blue Angel, The (Blaue Engel, Der) (1930)'\",\n",
       " 618: \"b'Picnic (1955)'\",\n",
       " 619: \"b'Extreme Measures (1996)'\",\n",
       " 620: \"b'Chamber, The (1996)'\",\n",
       " 621: \"b'Davy Crockett, King of the Wild Frontier (1955)'\",\n",
       " 622: \"b'Swiss Family Robinson (1960)'\",\n",
       " 623: \"b'Angels in the Outfield (1994)'\",\n",
       " 624: \"b'Three Caballeros, The (1945)'\",\n",
       " 625: \"b'Sword in the Stone, The (1963)'\",\n",
       " 626: \"b'So Dear to My Heart (1949)'\",\n",
       " 627: \"b'Robin Hood: Prince of Thieves (1991)'\",\n",
       " 628: \"b'Sleepers (1996)'\",\n",
       " 629: \"b'Victor/Victoria (1982)'\",\n",
       " 630: \"b'Great Race, The (1965)'\",\n",
       " 631: \"b'Crying Game, The (1992)'\",\n",
       " 632: 'b\"Sophie\\'s Choice (1982)\"',\n",
       " 633: \"b'Christmas Carol, A (1938)'\",\n",
       " 634: 'b\"Microcosmos: Le peuple de l\\'herbe (1996)\"',\n",
       " 635: \"b'Fog, The (1980)'\",\n",
       " 636: \"b'Escape from New York (1981)'\",\n",
       " 637: \"b'Howling, The (1981)'\",\n",
       " 638: \"b'Return of Martin Guerre, The (Retour de Martin Guerre, Le) (1982)'\",\n",
       " 639: \"b'Tin Drum, The (Blechtrommel, Die) (1979)'\",\n",
       " 640: \"b'Cook the Thief His Wife & Her Lover, The (1989)'\",\n",
       " 641: \"b'Paths of Glory (1957)'\",\n",
       " 642: \"b'Grifters, The (1990)'\",\n",
       " 643: \"b'The Innocent (1994)'\",\n",
       " 644: \"b'Thin Blue Line, The (1988)'\",\n",
       " 645: \"b'Paris Is Burning (1990)'\",\n",
       " 646: \"b'Once Upon a Time in the West (1969)'\",\n",
       " 647: \"b'Ran (1985)'\",\n",
       " 648: \"b'Quiet Man, The (1952)'\",\n",
       " 649: \"b'Once Upon a Time in America (1984)'\",\n",
       " 650: \"b'Seventh Seal, The (Sjunde inseglet, Det) (1957)'\",\n",
       " 651: \"b'Glory (1989)'\",\n",
       " 652: \"b'Rosencrantz and Guildenstern Are Dead (1990)'\",\n",
       " 653: \"b'Touch of Evil (1958)'\",\n",
       " 654: \"b'Chinatown (1974)'\",\n",
       " 655: \"b'Stand by Me (1986)'\",\n",
       " 656: \"b'M (1931)'\",\n",
       " 657: \"b'Manchurian Candidate, The (1962)'\",\n",
       " 658: \"b'Pump Up the Volume (1990)'\",\n",
       " 659: \"b'Arsenic and Old Lace (1944)'\",\n",
       " 660: \"b'Fried Green Tomatoes (1991)'\",\n",
       " 661: \"b'High Noon (1952)'\",\n",
       " 662: \"b'Somewhere in Time (1980)'\",\n",
       " 663: \"b'Being There (1979)'\",\n",
       " 664: \"b'Paris, Texas (1984)'\",\n",
       " 665: \"b'Alien 3 (1992)'\",\n",
       " 666: 'b\"Blood For Dracula (Andy Warhol\\'s Dracula) (1974)\"',\n",
       " 667: \"b'Audrey Rose (1977)'\",\n",
       " 668: \"b'Blood Beach (1981)'\",\n",
       " 669: \"b'Body Parts (1991)'\",\n",
       " 670: \"b'Body Snatchers (1993)'\",\n",
       " 671: \"b'Bride of Frankenstein (1935)'\",\n",
       " 672: \"b'Candyman (1992)'\",\n",
       " 673: \"b'Cape Fear (1962)'\",\n",
       " 674: \"b'Cat People (1982)'\",\n",
       " 675: \"b'Nosferatu (Nosferatu, eine Symphonie des Grauens) (1922)'\",\n",
       " 676: \"b'Crucible, The (1996)'\",\n",
       " 677: \"b'Fire on the Mountain (1996)'\",\n",
       " 678: \"b'Volcano (1997)'\",\n",
       " 679: \"b'Conan the Barbarian (1981)'\",\n",
       " 680: \"b'Kull the Conqueror (1997)'\",\n",
       " 681: \"b'Wishmaster (1997)'\",\n",
       " 682: \"b'I Know What You Did Last Summer (1997)'\",\n",
       " 683: \"b'Rocket Man (1997)'\",\n",
       " 684: \"b'In the Line of Fire (1993)'\",\n",
       " 685: \"b'Executive Decision (1996)'\",\n",
       " 686: \"b'Perfect World, A (1993)'\",\n",
       " 687: 'b\"McHale\\'s Navy (1997)\"',\n",
       " 688: \"b'Leave It to Beaver (1997)'\",\n",
       " 689: \"b'Jackal, The (1997)'\",\n",
       " 690: \"b'Seven Years in Tibet (1997)'\",\n",
       " 691: \"b'Dark City (1998)'\",\n",
       " 692: \"b'American President, The (1995)'\",\n",
       " 693: \"b'Casino (1995)'\",\n",
       " 694: \"b'Persuasion (1995)'\",\n",
       " 695: \"b'Kicking and Screaming (1995)'\",\n",
       " 696: \"b'City Hall (1996)'\",\n",
       " 697: \"b'Basketball Diaries, The (1995)'\",\n",
       " 698: \"b'Browning Version, The (1994)'\",\n",
       " 699: \"b'Little Women (1994)'\",\n",
       " 700: \"b'Miami Rhapsody (1995)'\",\n",
       " 701: \"b'Wonderful, Horrible Life of Leni Riefenstahl, The (1993)'\",\n",
       " 702: \"b'Barcelona (1994)'\",\n",
       " 703: 'b\"Widows\\' Peak (1994)\"',\n",
       " 704: \"b'House of the Spirits, The (1993)'\",\n",
       " 705: 'b\"Singin\\' in the Rain (1952)\"',\n",
       " 706: \"b'Bad Moon (1996)'\",\n",
       " 707: \"b'Enchanted April (1991)'\",\n",
       " 708: \"b'Sex, Lies, and Videotape (1989)'\",\n",
       " 709: \"b'Strictly Ballroom (1992)'\",\n",
       " 710: \"b'Better Off Dead... (1985)'\",\n",
       " 711: \"b'Substance of Fire, The (1996)'\",\n",
       " 712: \"b'Tin Men (1987)'\",\n",
       " 713: \"b'Othello (1995)'\",\n",
       " 714: \"b'Carrington (1995)'\",\n",
       " 715: \"b'To Die For (1995)'\",\n",
       " 716: \"b'Home for the Holidays (1995)'\",\n",
       " 717: \"b'Juror, The (1996)'\",\n",
       " 718: \"b'In the Bleak Midwinter (1995)'\",\n",
       " 719: \"b'Canadian Bacon (1994)'\",\n",
       " 720: \"b'First Knight (1995)'\",\n",
       " 721: \"b'Mallrats (1995)'\",\n",
       " 722: \"b'Nine Months (1995)'\",\n",
       " 723: \"b'Boys on the Side (1995)'\",\n",
       " 724: \"b'Circle of Friends (1995)'\",\n",
       " 725: \"b'Exit to Eden (1994)'\",\n",
       " 726: \"b'Fluke (1995)'\",\n",
       " 727: \"b'Immortal Beloved (1994)'\",\n",
       " 728: \"b'Junior (1994)'\",\n",
       " 729: \"b'Nell (1994)'\",\n",
       " 730: \"b'Queen Margot (Reine Margot, La) (1994)'\",\n",
       " 731: \"b'Corrina, Corrina (1994)'\",\n",
       " 732: \"b'Dave (1993)'\",\n",
       " 733: \"b'Go Fish (1994)'\",\n",
       " 734: \"b'Made in America (1993)'\",\n",
       " 735: \"b'Philadelphia (1993)'\",\n",
       " 736: \"b'Shadowlands (1993)'\",\n",
       " 737: \"b'Sirens (1994)'\",\n",
       " 738: \"b'Threesome (1994)'\",\n",
       " 739: \"b'Pretty Woman (1990)'\",\n",
       " 740: \"b'Jane Eyre (1996)'\",\n",
       " 741: \"b'Last Supper, The (1995)'\",\n",
       " 742: \"b'Ransom (1996)'\",\n",
       " 743: \"b'Crow: City of Angels, The (1996)'\",\n",
       " 744: \"b'Michael Collins (1996)'\",\n",
       " 745: \"b'Ruling Class, The (1972)'\",\n",
       " 746: \"b'Real Genius (1985)'\",\n",
       " 747: \"b'Benny & Joon (1993)'\",\n",
       " 748: \"b'Saint, The (1997)'\",\n",
       " 749: \"b'MatchMaker, The (1997)'\",\n",
       " 750: \"b'Amistad (1997)'\",\n",
       " 751: \"b'Tomorrow Never Dies (1997)'\",\n",
       " 752: \"b'Replacement Killers, The (1998)'\",\n",
       " 753: \"b'Burnt By the Sun (1994)'\",\n",
       " 754: \"b'Red Corner (1997)'\",\n",
       " 755: \"b'Jumanji (1995)'\",\n",
       " 756: \"b'Father of the Bride Part II (1995)'\",\n",
       " 757: \"b'Across the Sea of Time (1995)'\",\n",
       " 758: \"b'Lawnmower Man 2: Beyond Cyberspace (1996)'\",\n",
       " 759: \"b'Fair Game (1995)'\",\n",
       " 760: \"b'Screamers (1995)'\",\n",
       " 761: \"b'Nick of Time (1995)'\",\n",
       " 762: \"b'Beautiful Girls (1996)'\",\n",
       " 763: \"b'Happy Gilmore (1996)'\",\n",
       " 764: \"b'If Lucy Fell (1996)'\",\n",
       " 765: \"b'Boomerang (1992)'\",\n",
       " 766: \"b'Man of the Year (1995)'\",\n",
       " 767: \"b'Addiction, The (1995)'\",\n",
       " 768: \"b'Casper (1995)'\",\n",
       " 769: \"b'Congo (1995)'\",\n",
       " 770: \"b'Devil in a Blue Dress (1995)'\",\n",
       " 771: \"b'Johnny Mnemonic (1995)'\",\n",
       " 772: \"b'Kids (1995)'\",\n",
       " 773: \"b'Mute Witness (1994)'\",\n",
       " 774: \"b'Prophecy, The (1995)'\",\n",
       " 775: \"b'Something to Talk About (1995)'\",\n",
       " 776: \"b'Three Wishes (1995)'\",\n",
       " 777: \"b'Castle Freak (1995)'\",\n",
       " 778: \"b'Don Juan DeMarco (1995)'\",\n",
       " 779: \"b'Drop Zone (1994)'\",\n",
       " 780: \"b'Dumb & Dumber (1994)'\",\n",
       " 781: \"b'French Kiss (1995)'\",\n",
       " 782: \"b'Little Odessa (1994)'\",\n",
       " 783: \"b'Milk Money (1994)'\",\n",
       " 784: \"b'Beyond Bedlam (1993)'\",\n",
       " 785: \"b'Only You (1994)'\",\n",
       " 786: \"b'Perez Family, The (1995)'\",\n",
       " 787: \"b'Roommates (1995)'\",\n",
       " 788: \"b'Relative Fear (1994)'\",\n",
       " 789: \"b'Swimming with Sharks (1995)'\",\n",
       " 790: \"b'Tommy Boy (1995)'\",\n",
       " 791: \"b'Baby-Sitters Club, The (1995)'\",\n",
       " 792: \"b'Bullets Over Broadway (1994)'\",\n",
       " 793: \"b'Crooklyn (1994)'\",\n",
       " 794: \"b'It Could Happen to You (1994)'\",\n",
       " 795: \"b'Richie Rich (1994)'\",\n",
       " 796: \"b'Speechless (1994)'\",\n",
       " 797: \"b'Timecop (1994)'\",\n",
       " 798: \"b'Bad Company (1995)'\",\n",
       " 799: \"b'Boys Life (1995)'\",\n",
       " 800: \"b'In the Mouth of Madness (1995)'\",\n",
       " 801: \"b'Air Up There, The (1994)'\",\n",
       " 802: \"b'Hard Target (1993)'\",\n",
       " 803: \"b'Heaven & Earth (1993)'\",\n",
       " 804: \"b'Jimmy Hollywood (1994)'\",\n",
       " 805: \"b'Manhattan Murder Mystery (1993)'\",\n",
       " 806: \"b'Menace II Society (1993)'\",\n",
       " 807: \"b'Poetic Justice (1993)'\",\n",
       " 808: \"b'Program, The (1993)'\",\n",
       " 809: \"b'Rising Sun (1993)'\",\n",
       " 810: \"b'Shadow, The (1994)'\",\n",
       " 811: \"b'Thirty-Two Short Films About Glenn Gould (1993)'\",\n",
       " 812: \"b'Andre (1994)'\",\n",
       " 813: \"b'Celluloid Closet, The (1995)'\",\n",
       " 814: \"b'Great Day in Harlem, A (1994)'\",\n",
       " 815: \"b'One Fine Day (1996)'\",\n",
       " 816: \"b'Candyman: Farewell to the Flesh (1995)'\",\n",
       " 817: \"b'Frisk (1995)'\",\n",
       " 818: \"b'Girl 6 (1996)'\",\n",
       " 819: \"b'Eddie (1996)'\",\n",
       " 820: \"b'Space Jam (1996)'\",\n",
       " 821: \"b'Mrs. Winterbourne (1996)'\",\n",
       " 822: \"b'Faces (1968)'\",\n",
       " 823: \"b'Mulholland Falls (1996)'\",\n",
       " 824: \"b'Great White Hype, The (1996)'\",\n",
       " 825: \"b'Arrival, The (1996)'\",\n",
       " 826: \"b'Phantom, The (1996)'\",\n",
       " 827: \"b'Daylight (1996)'\",\n",
       " 828: \"b'Alaska (1996)'\",\n",
       " 829: \"b'Fled (1996)'\",\n",
       " 830: \"b'Power 98 (1995)'\",\n",
       " 831: \"b'Escape from L.A. (1996)'\",\n",
       " 832: \"b'Bogus (1996)'\",\n",
       " 833: \"b'Bulletproof (1996)'\",\n",
       " 834: \"b'Halloween: The Curse of Michael Myers (1995)'\",\n",
       " 835: \"b'Gay Divorcee, The (1934)'\",\n",
       " 836: \"b'Ninotchka (1939)'\",\n",
       " 837: \"b'Meet John Doe (1941)'\",\n",
       " 838: \"b'In the Line of Duty 2 (1987)'\",\n",
       " 839: \"b'Loch Ness (1995)'\",\n",
       " 840: \"b'Last Man Standing (1996)'\",\n",
       " 841: \"b'Glimmer Man, The (1996)'\",\n",
       " 842: \"b'Pollyanna (1960)'\",\n",
       " 843: \"b'Shaggy Dog, The (1959)'\",\n",
       " 844: \"b'Freeway (1996)'\",\n",
       " 845: \"b'That Thing You Do! (1996)'\",\n",
       " 846: \"b'To Gillian on Her 37th Birthday (1996)'\",\n",
       " 847: \"b'Looking for Richard (1996)'\",\n",
       " 848: \"b'Murder, My Sweet (1944)'\",\n",
       " 849: \"b'Days of Thunder (1990)'\",\n",
       " 850: \"b'Perfect Candidate, A (1996)'\",\n",
       " 851: \"b'Two or Three Things I Know About Her (1966)'\",\n",
       " 852: \"b'Bloody Child, The (1996)'\",\n",
       " 853: \"b'Braindead (1992)'\",\n",
       " 854: \"b'Bad Taste (1987)'\",\n",
       " 855: \"b'Diva (1981)'\",\n",
       " 856: \"b'Night on Earth (1991)'\",\n",
       " 857: \"b'Paris Was a Woman (1995)'\",\n",
       " 858: \"b'Amityville: Dollhouse (1996)'\",\n",
       " 859: 'b\"April Fool\\'s Day (1986)\"',\n",
       " 860: \"b'Believers, The (1987)'\",\n",
       " 861: \"b'Nosferatu a Venezia (1986)'\",\n",
       " 862: \"b'Jingle All the Way (1996)'\",\n",
       " 863: \"b'Garden of Finzi-Contini, The (Giardino dei Finzi-Contini, Il) (1970)'\",\n",
       " 864: \"b'My Fellow Americans (1996)'\",\n",
       " 865: \"b'Ice Storm, The (1997)'\",\n",
       " 866: \"b'Michael (1996)'\",\n",
       " 867: \"b'Whole Wide World, The (1996)'\",\n",
       " 868: \"b'Hearts and Minds (1996)'\",\n",
       " 869: \"b'Fools Rush In (1997)'\",\n",
       " 870: \"b'Touch (1997)'\",\n",
       " 871: \"b'Vegas Vacation (1997)'\",\n",
       " 872: \"b'Love Jones (1997)'\",\n",
       " 873: \"b'Picture Perfect (1997)'\",\n",
       " 874: \"b'Career Girls (1997)'\",\n",
       " 875: 'b\"She\\'s So Lovely (1997)\"',\n",
       " 876: \"b'Money Talks (1997)'\",\n",
       " 877: \"b'Excess Baggage (1997)'\",\n",
       " 878: \"b'That Darn Cat! (1997)'\",\n",
       " 879: \"b'Peacemaker, The (1997)'\",\n",
       " 880: \"b'Soul Food (1997)'\",\n",
       " 881: \"b'Money Talks (1997)'\",\n",
       " 882: \"b'Washington Square (1997)'\",\n",
       " 883: \"b'Telling Lies in America (1997)'\",\n",
       " 884: \"b'Year of the Horse (1997)'\",\n",
       " 885: \"b'Phantoms (1998)'\",\n",
       " 886: \"b'Life Less Ordinary, A (1997)'\",\n",
       " 887: 'b\"Eve\\'s Bayou (1997)\"',\n",
       " 888: \"b'One Night Stand (1997)'\",\n",
       " 889: \"b'Tango Lesson, The (1997)'\",\n",
       " 890: \"b'Mortal Kombat: Annihilation (1997)'\",\n",
       " 891: \"b'Bent (1997)'\",\n",
       " 892: \"b'Flubber (1997)'\",\n",
       " 893: \"b'For Richer or Poorer (1997)'\",\n",
       " 894: \"b'Home Alone 3 (1997)'\",\n",
       " 895: \"b'Scream 2 (1997)'\",\n",
       " 896: \"b'Sweet Hereafter, The (1997)'\",\n",
       " 897: \"b'Time Tracers (1995)'\",\n",
       " 898: \"b'Postman, The (1997)'\",\n",
       " 899: \"b'Winter Guest, The (1997)'\",\n",
       " 900: \"b'Kundun (1997)'\",\n",
       " 901: \"b'Mr. Magoo (1997)'\",\n",
       " 902: \"b'Big Lebowski, The (1998)'\",\n",
       " 903: \"b'Afterglow (1997)'\",\n",
       " 904: \"b'Ma vie en rose (My Life in Pink) (1997)'\",\n",
       " 905: \"b'Great Expectations (1998)'\",\n",
       " 906: \"b'Oscar & Lucinda (1997)'\",\n",
       " 907: \"b'Vermin (1998)'\",\n",
       " 908: \"b'Half Baked (1998)'\",\n",
       " 909: \"b'Dangerous Beauty (1998)'\",\n",
       " 910: \"b'Nil By Mouth (1997)'\",\n",
       " 911: \"b'Twilight (1998)'\",\n",
       " 912: \"b'U.S. Marshalls (1998)'\",\n",
       " 913: \"b'Love and Death on Long Island (1997)'\",\n",
       " 914: \"b'Wild Things (1998)'\",\n",
       " 915: \"b'Primary Colors (1998)'\",\n",
       " 916: \"b'Lost in Space (1998)'\",\n",
       " 917: \"b'Mercury Rising (1998)'\",\n",
       " 918: \"b'City of Angels (1998)'\",\n",
       " 919: \"b'City of Lost Children, The (1995)'\",\n",
       " 920: \"b'Two Bits (1995)'\",\n",
       " 921: \"b'Farewell My Concubine (1993)'\",\n",
       " 922: \"b'Dead Man (1995)'\",\n",
       " 923: \"b'Raise the Red Lantern (1991)'\",\n",
       " 924: \"b'White Squall (1996)'\",\n",
       " 925: \"b'Unforgettable (1996)'\",\n",
       " 926: \"b'Down Periscope (1996)'\",\n",
       " 927: \"b'Flower of My Secret, The (Flor de mi secreto, La) (1995)'\",\n",
       " 928: \"b'Craft, The (1996)'\",\n",
       " 929: \"b'Harriet the Spy (1996)'\",\n",
       " 930: \"b'Chain Reaction (1996)'\",\n",
       " 931: \"b'Island of Dr. Moreau, The (1996)'\",\n",
       " 932: \"b'First Kid (1996)'\",\n",
       " 933: \"b'Funeral, The (1996)'\",\n",
       " 934: 'b\"Preacher\\'s Wife, The (1996)\"',\n",
       " 935: \"b'Paradise Road (1997)'\",\n",
       " 936: \"b'Brassed Off (1996)'\",\n",
       " 937: \"b'Thousand Acres, A (1997)'\",\n",
       " 938: \"b'Smile Like Yours, A (1997)'\",\n",
       " 939: \"b'Murder in the First (1995)'\",\n",
       " 940: \"b'Airheads (1994)'\",\n",
       " 941: \"b'With Honors (1994)'\",\n",
       " 942: 'b\"What\\'s Love Got to Do with It (1993)\"',\n",
       " 943: \"b'Killing Zoe (1994)'\",\n",
       " 944: \"b'Renaissance Man (1994)'\",\n",
       " 945: \"b'Charade (1963)'\",\n",
       " 946: \"b'Fox and the Hound, The (1981)'\",\n",
       " 947: \"b'Big Blue, The (Grand bleu, Le) (1988)'\",\n",
       " 948: \"b'Booty Call (1997)'\",\n",
       " 949: \"b'How to Make an American Quilt (1995)'\",\n",
       " 950: \"b'Georgia (1995)'\",\n",
       " 951: \"b'Indian in the Cupboard, The (1995)'\",\n",
       " 952: \"b'Blue in the Face (1995)'\",\n",
       " 953: \"b'Unstrung Heroes (1995)'\",\n",
       " 954: \"b'Unzipped (1995)'\",\n",
       " 955: \"b'Before Sunrise (1995)'\",\n",
       " 956: 'b\"Nobody\\'s Fool (1994)\"',\n",
       " 957: \"b'Pushing Hands (1992)'\",\n",
       " 958: \"b'To Live (Huozhe) (1994)'\",\n",
       " 959: \"b'Dazed and Confused (1993)'\",\n",
       " 960: \"b'Naked (1993)'\",\n",
       " 961: \"b'Orlando (1993)'\",\n",
       " 962: \"b'Ruby in Paradise (1993)'\",\n",
       " 963: \"b'Some Folks Call It a Sling Blade (1993)'\",\n",
       " 964: \"b'Month by the Lake, A (1995)'\",\n",
       " 965: \"b'Funny Face (1957)'\",\n",
       " 966: \"b'Affair to Remember, An (1957)'\",\n",
       " 967: \"b'Little Lord Fauntleroy (1936)'\",\n",
       " 968: \"b'Inspector General, The (1949)'\",\n",
       " 969: \"b'Winnie the Pooh and the Blustery Day (1968)'\",\n",
       " 970: \"b'Hear My Song (1991)'\",\n",
       " 971: \"b'Mediterraneo (1991)'\",\n",
       " 972: \"b'Passion Fish (1992)'\",\n",
       " 973: \"b'Grateful Dead (1995)'\",\n",
       " 974: \"b'Eye for an Eye (1996)'\",\n",
       " 975: \"b'Fear (1996)'\",\n",
       " 976: \"b'Solo (1996)'\",\n",
       " 977: \"b'Substitute, The (1996)'\",\n",
       " 978: 'b\"Heaven\\'s Prisoners (1996)\"',\n",
       " 979: \"b'Trigger Effect, The (1996)'\",\n",
       " 980: \"b'Mother Night (1996)'\",\n",
       " 981: \"b'Dangerous Ground (1997)'\",\n",
       " 982: \"b'Maximum Risk (1996)'\",\n",
       " 983: 'b\"Rich Man\\'s Wife, The (1996)\"',\n",
       " 984: \"b'Shadow Conspiracy (1997)'\",\n",
       " 985: \"b'Blood & Wine (1997)'\",\n",
       " 986: \"b'Turbulence (1997)'\",\n",
       " 987: \"b'Underworld (1997)'\",\n",
       " 988: \"b'Beautician and the Beast, The (1997)'\",\n",
       " 989: 'b\"Cats Don\\'t Dance (1997)\"',\n",
       " 990: \"b'Anna Karenina (1997)'\",\n",
       " 991: \"b'Keys to Tulsa (1997)'\",\n",
       " 992: \"b'Head Above Water (1996)'\",\n",
       " 993: \"b'Hercules (1997)'\",\n",
       " 994: \"b'Last Time I Committed Suicide, The (1997)'\",\n",
       " 995: \"b'Kiss Me, Guido (1997)'\",\n",
       " 996: \"b'Big Green, The (1995)'\",\n",
       " 997: \"b'Stuart Saves His Family (1995)'\",\n",
       " 998: \"b'Cabin Boy (1994)'\",\n",
       " 999: \"b'Clean Slate (1994)'\",\n",
       " 1000: \"b'Lightning Jack (1994)'\",\n",
       " ...}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that retrieves all the ids and titles for movies containing 'text' in its title\n",
    "def returnItemId(text, ids):\n",
    "    \"\"\"\n",
    "    :param text: string to be looked for in movies titles\n",
    "    :param ids: dicttionary of {id:title}\n",
    "    \n",
    "    :return: a list of (id,title) if text found in titles, and an empty list otherwise.\n",
    "    \"\"\"\n",
    "    # convert input text to lowercase\n",
    "    text_ = text.lower()\n",
    "    # find occurances\n",
    "    search = [(k, v.lower().find(text_))for k, v in list(ids.items())]#me devuelve 0 o 1 segunencuentre una coincidencia o no\n",
    "    # Get the IDs corresponding to the given text\n",
    "    index = [k for k, v in search if v > -1]\n",
    "    \n",
    "    # Return a list with the id and the name\n",
    "    out = []\n",
    "    for i in index:\n",
    "        out.append((i,ids[i]))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(240, \"b'Beavis and Butt-head Do America (1996)'\"),\n",
       " (435, \"b'Butch Cassidy and the Sundance Kid (1969)'\"),\n",
       " (580,\n",
       "  \"b'Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)'\"),\n",
       " (1401, \"b'M. Butterfly (1993)'\"),\n",
       " (1459, \"b'Madame Butterfly (1995)'\"),\n",
       " (1614, \"b'Reluctant Debutante, The (1958)'\"),\n",
       " (1621, \"b'Butterfly Kiss (1995)'\"),\n",
       " (1645, \"b'Butcher Boy, The (1998)'\"),\n",
       " (1650, \"b'Butcher Boy, The (1998)'\")]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnItemId('but', item_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data consistency (always double check everything!!!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1682\n",
      "1664\n"
     ]
    }
   ],
   "source": [
    "# check whether titles are unique or not. They are not!!!\n",
    "print(len(set(item_dict.keys())))\n",
    "print(len(set(item_dict.values())))\n",
    "# con esto confirmamos q hay repetidos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One work around: create another dict that consolidates ids with the same movie title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Chasing Amy (1997)' [246, 268]\n",
      "b'Kull the Conqueror (1997)' [266, 680]\n",
      "b\"Ulee's Gold (1997)\" [297, 303]\n",
      "b'Fly Away Home (1996)' [304, 500]\n",
      "b'Ice Storm, The (1997)' [305, 865]\n",
      "b'Deceiver (1997)' [309, 1606]\n",
      "b'Desperate Measures (1998)' [329, 348]\n",
      "b'Body Snatchers (1993)' [573, 670]\n",
      "b'Substance of Fire, The (1996)' [711, 1658]\n",
      "b'Money Talks (1997)' [876, 881]\n",
      "b'That Darn Cat! (1997)' [878, 1003]\n",
      "b'Hugo Pool (1997)' [1175, 1617]\n",
      "b'Chairman of the Board (1998)' [1234, 1654]\n",
      "b'Designated Mourner, The (1997)' [1256, 1257]\n",
      "b'Hurricane Streets (1998)' [1395, 1607]\n",
      "b'Sliding Doors (1998)' [1429, 1680]\n",
      "b'Nightwatch (1997)' [1477, 1625]\n",
      "b'Butcher Boy, The (1998)' [1645, 1650]\n"
     ]
    }
   ],
   "source": [
    "### duplicates_item_dict ###\n",
    "# Las claves en \"duplicates_item_dict\" son los nombres de las películas\n",
    "# Los valores son una lista de los ids (que pueden ser uno solo, o varios)\n",
    "duplicates_item_dict = {}\n",
    "\n",
    "for id , name in list(item_dict.items()):\n",
    "    if name not in duplicates_item_dict:\n",
    "        duplicates_item_dict[name] = [id]\n",
    "    else:\n",
    "        duplicates_item_dict[name] = duplicates_item_dict[name] + [id]\n",
    "\n",
    "# show the duplicated titles\n",
    "for k, v in list(duplicates_item_dict.items()):\n",
    "    if len(v) > 1:\n",
    "        print(k,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a dict where the key are the original ids, and the values are the unique one. \n",
    "We will use this dictionary to remove duplicates in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_id_item_dict\n",
    "unique_id_item_dict = {}\n",
    "for index, lista_id in enumerate(duplicates_item_dict.values()):\n",
    "    for key in lista_id:\n",
    "        unique_id_item_dict[key] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: 0,\n",
       " 2: 1,\n",
       " 3: 2,\n",
       " 4: 3,\n",
       " 5: 4,\n",
       " 6: 5,\n",
       " 7: 6,\n",
       " 8: 7,\n",
       " 9: 8,\n",
       " 10: 9,\n",
       " 11: 10,\n",
       " 12: 11,\n",
       " 13: 12,\n",
       " 14: 13,\n",
       " 15: 14,\n",
       " 16: 15,\n",
       " 17: 16,\n",
       " 18: 17,\n",
       " 19: 18,\n",
       " 20: 19,\n",
       " 21: 20,\n",
       " 22: 21,\n",
       " 23: 22,\n",
       " 24: 23,\n",
       " 25: 24,\n",
       " 26: 25,\n",
       " 27: 26,\n",
       " 28: 27,\n",
       " 29: 28,\n",
       " 30: 29,\n",
       " 31: 30,\n",
       " 32: 31,\n",
       " 33: 32,\n",
       " 34: 33,\n",
       " 35: 34,\n",
       " 36: 35,\n",
       " 37: 36,\n",
       " 38: 37,\n",
       " 39: 38,\n",
       " 40: 39,\n",
       " 41: 40,\n",
       " 42: 41,\n",
       " 43: 42,\n",
       " 44: 43,\n",
       " 45: 44,\n",
       " 46: 45,\n",
       " 47: 46,\n",
       " 48: 47,\n",
       " 49: 48,\n",
       " 50: 49,\n",
       " 51: 50,\n",
       " 52: 51,\n",
       " 53: 52,\n",
       " 54: 53,\n",
       " 55: 54,\n",
       " 56: 55,\n",
       " 57: 56,\n",
       " 58: 57,\n",
       " 59: 58,\n",
       " 60: 59,\n",
       " 61: 60,\n",
       " 62: 61,\n",
       " 63: 62,\n",
       " 64: 63,\n",
       " 65: 64,\n",
       " 66: 65,\n",
       " 67: 66,\n",
       " 68: 67,\n",
       " 69: 68,\n",
       " 70: 69,\n",
       " 71: 70,\n",
       " 72: 71,\n",
       " 73: 72,\n",
       " 74: 73,\n",
       " 75: 74,\n",
       " 76: 75,\n",
       " 77: 76,\n",
       " 78: 77,\n",
       " 79: 78,\n",
       " 80: 79,\n",
       " 81: 80,\n",
       " 82: 81,\n",
       " 83: 82,\n",
       " 84: 83,\n",
       " 85: 84,\n",
       " 86: 85,\n",
       " 87: 86,\n",
       " 88: 87,\n",
       " 89: 88,\n",
       " 90: 89,\n",
       " 91: 90,\n",
       " 92: 91,\n",
       " 93: 92,\n",
       " 94: 93,\n",
       " 95: 94,\n",
       " 96: 95,\n",
       " 97: 96,\n",
       " 98: 97,\n",
       " 99: 98,\n",
       " 100: 99,\n",
       " 101: 100,\n",
       " 102: 101,\n",
       " 103: 102,\n",
       " 104: 103,\n",
       " 105: 104,\n",
       " 106: 105,\n",
       " 107: 106,\n",
       " 108: 107,\n",
       " 109: 108,\n",
       " 110: 109,\n",
       " 111: 110,\n",
       " 112: 111,\n",
       " 113: 112,\n",
       " 114: 113,\n",
       " 115: 114,\n",
       " 116: 115,\n",
       " 117: 116,\n",
       " 118: 117,\n",
       " 119: 118,\n",
       " 120: 119,\n",
       " 121: 120,\n",
       " 122: 121,\n",
       " 123: 122,\n",
       " 124: 123,\n",
       " 125: 124,\n",
       " 126: 125,\n",
       " 127: 126,\n",
       " 128: 127,\n",
       " 129: 128,\n",
       " 130: 129,\n",
       " 131: 130,\n",
       " 132: 131,\n",
       " 133: 132,\n",
       " 134: 133,\n",
       " 135: 134,\n",
       " 136: 135,\n",
       " 137: 136,\n",
       " 138: 137,\n",
       " 139: 138,\n",
       " 140: 139,\n",
       " 141: 140,\n",
       " 142: 141,\n",
       " 143: 142,\n",
       " 144: 143,\n",
       " 145: 144,\n",
       " 146: 145,\n",
       " 147: 146,\n",
       " 148: 147,\n",
       " 149: 148,\n",
       " 150: 149,\n",
       " 151: 150,\n",
       " 152: 151,\n",
       " 153: 152,\n",
       " 154: 153,\n",
       " 155: 154,\n",
       " 156: 155,\n",
       " 157: 156,\n",
       " 158: 157,\n",
       " 159: 158,\n",
       " 160: 159,\n",
       " 161: 160,\n",
       " 162: 161,\n",
       " 163: 162,\n",
       " 164: 163,\n",
       " 165: 164,\n",
       " 166: 165,\n",
       " 167: 166,\n",
       " 168: 167,\n",
       " 169: 168,\n",
       " 170: 169,\n",
       " 171: 170,\n",
       " 172: 171,\n",
       " 173: 172,\n",
       " 174: 173,\n",
       " 175: 174,\n",
       " 176: 175,\n",
       " 177: 176,\n",
       " 178: 177,\n",
       " 179: 178,\n",
       " 180: 179,\n",
       " 181: 180,\n",
       " 182: 181,\n",
       " 183: 182,\n",
       " 184: 183,\n",
       " 185: 184,\n",
       " 186: 185,\n",
       " 187: 186,\n",
       " 188: 187,\n",
       " 189: 188,\n",
       " 190: 189,\n",
       " 191: 190,\n",
       " 192: 191,\n",
       " 193: 192,\n",
       " 194: 193,\n",
       " 195: 194,\n",
       " 196: 195,\n",
       " 197: 196,\n",
       " 198: 197,\n",
       " 199: 198,\n",
       " 200: 199,\n",
       " 201: 200,\n",
       " 202: 201,\n",
       " 203: 202,\n",
       " 204: 203,\n",
       " 205: 204,\n",
       " 206: 205,\n",
       " 207: 206,\n",
       " 208: 207,\n",
       " 209: 208,\n",
       " 210: 209,\n",
       " 211: 210,\n",
       " 212: 211,\n",
       " 213: 212,\n",
       " 214: 213,\n",
       " 215: 214,\n",
       " 216: 215,\n",
       " 217: 216,\n",
       " 218: 217,\n",
       " 219: 218,\n",
       " 220: 219,\n",
       " 221: 220,\n",
       " 222: 221,\n",
       " 223: 222,\n",
       " 224: 223,\n",
       " 225: 224,\n",
       " 226: 225,\n",
       " 227: 226,\n",
       " 228: 227,\n",
       " 229: 228,\n",
       " 230: 229,\n",
       " 231: 230,\n",
       " 232: 231,\n",
       " 233: 232,\n",
       " 234: 233,\n",
       " 235: 234,\n",
       " 236: 235,\n",
       " 237: 236,\n",
       " 238: 237,\n",
       " 239: 238,\n",
       " 240: 239,\n",
       " 241: 240,\n",
       " 242: 241,\n",
       " 243: 242,\n",
       " 244: 243,\n",
       " 245: 244,\n",
       " 246: 245,\n",
       " 268: 245,\n",
       " 247: 246,\n",
       " 248: 247,\n",
       " 249: 248,\n",
       " 250: 249,\n",
       " 251: 250,\n",
       " 252: 251,\n",
       " 253: 252,\n",
       " 254: 253,\n",
       " 255: 254,\n",
       " 256: 255,\n",
       " 257: 256,\n",
       " 258: 257,\n",
       " 259: 258,\n",
       " 260: 259,\n",
       " 261: 260,\n",
       " 262: 261,\n",
       " 263: 262,\n",
       " 264: 263,\n",
       " 265: 264,\n",
       " 266: 265,\n",
       " 680: 265,\n",
       " 267: 266,\n",
       " 269: 267,\n",
       " 270: 268,\n",
       " 271: 269,\n",
       " 272: 270,\n",
       " 273: 271,\n",
       " 274: 272,\n",
       " 275: 273,\n",
       " 276: 274,\n",
       " 277: 275,\n",
       " 278: 276,\n",
       " 279: 277,\n",
       " 280: 278,\n",
       " 281: 279,\n",
       " 282: 280,\n",
       " 283: 281,\n",
       " 284: 282,\n",
       " 285: 283,\n",
       " 286: 284,\n",
       " 287: 285,\n",
       " 288: 286,\n",
       " 289: 287,\n",
       " 290: 288,\n",
       " 291: 289,\n",
       " 292: 290,\n",
       " 293: 291,\n",
       " 294: 292,\n",
       " 295: 293,\n",
       " 296: 294,\n",
       " 297: 295,\n",
       " 303: 295,\n",
       " 298: 296,\n",
       " 299: 297,\n",
       " 300: 298,\n",
       " 301: 299,\n",
       " 302: 300,\n",
       " 304: 301,\n",
       " 500: 301,\n",
       " 305: 302,\n",
       " 865: 302,\n",
       " 306: 303,\n",
       " 307: 304,\n",
       " 308: 305,\n",
       " 309: 306,\n",
       " 1606: 306,\n",
       " 310: 307,\n",
       " 311: 308,\n",
       " 312: 309,\n",
       " 313: 310,\n",
       " 314: 311,\n",
       " 315: 312,\n",
       " 316: 313,\n",
       " 317: 314,\n",
       " 318: 315,\n",
       " 319: 316,\n",
       " 320: 317,\n",
       " 321: 318,\n",
       " 322: 319,\n",
       " 323: 320,\n",
       " 324: 321,\n",
       " 325: 322,\n",
       " 326: 323,\n",
       " 327: 324,\n",
       " 328: 325,\n",
       " 329: 326,\n",
       " 348: 326,\n",
       " 330: 327,\n",
       " 331: 328,\n",
       " 332: 329,\n",
       " 333: 330,\n",
       " 334: 331,\n",
       " 335: 332,\n",
       " 336: 333,\n",
       " 337: 334,\n",
       " 338: 335,\n",
       " 339: 336,\n",
       " 340: 337,\n",
       " 341: 338,\n",
       " 342: 339,\n",
       " 343: 340,\n",
       " 344: 341,\n",
       " 345: 342,\n",
       " 346: 343,\n",
       " 347: 344,\n",
       " 349: 345,\n",
       " 350: 346,\n",
       " 351: 347,\n",
       " 352: 348,\n",
       " 353: 349,\n",
       " 354: 350,\n",
       " 355: 351,\n",
       " 356: 352,\n",
       " 357: 353,\n",
       " 358: 354,\n",
       " 359: 355,\n",
       " 360: 356,\n",
       " 361: 357,\n",
       " 362: 358,\n",
       " 363: 359,\n",
       " 364: 360,\n",
       " 365: 361,\n",
       " 366: 362,\n",
       " 367: 363,\n",
       " 368: 364,\n",
       " 369: 365,\n",
       " 370: 366,\n",
       " 371: 367,\n",
       " 372: 368,\n",
       " 373: 369,\n",
       " 374: 370,\n",
       " 375: 371,\n",
       " 376: 372,\n",
       " 377: 373,\n",
       " 378: 374,\n",
       " 379: 375,\n",
       " 380: 376,\n",
       " 381: 377,\n",
       " 382: 378,\n",
       " 383: 379,\n",
       " 384: 380,\n",
       " 385: 381,\n",
       " 386: 382,\n",
       " 387: 383,\n",
       " 388: 384,\n",
       " 389: 385,\n",
       " 390: 386,\n",
       " 391: 387,\n",
       " 392: 388,\n",
       " 393: 389,\n",
       " 394: 390,\n",
       " 395: 391,\n",
       " 396: 392,\n",
       " 397: 393,\n",
       " 398: 394,\n",
       " 399: 395,\n",
       " 400: 396,\n",
       " 401: 397,\n",
       " 402: 398,\n",
       " 403: 399,\n",
       " 404: 400,\n",
       " 405: 401,\n",
       " 406: 402,\n",
       " 407: 403,\n",
       " 408: 404,\n",
       " 409: 405,\n",
       " 410: 406,\n",
       " 411: 407,\n",
       " 412: 408,\n",
       " 413: 409,\n",
       " 414: 410,\n",
       " 415: 411,\n",
       " 416: 412,\n",
       " 417: 413,\n",
       " 418: 414,\n",
       " 419: 415,\n",
       " 420: 416,\n",
       " 421: 417,\n",
       " 422: 418,\n",
       " 423: 419,\n",
       " 424: 420,\n",
       " 425: 421,\n",
       " 426: 422,\n",
       " 427: 423,\n",
       " 428: 424,\n",
       " 429: 425,\n",
       " 430: 426,\n",
       " 431: 427,\n",
       " 432: 428,\n",
       " 433: 429,\n",
       " 434: 430,\n",
       " 435: 431,\n",
       " 436: 432,\n",
       " 437: 433,\n",
       " 438: 434,\n",
       " 439: 435,\n",
       " 440: 436,\n",
       " 441: 437,\n",
       " 442: 438,\n",
       " 443: 439,\n",
       " 444: 440,\n",
       " 445: 441,\n",
       " 446: 442,\n",
       " 447: 443,\n",
       " 448: 444,\n",
       " 449: 445,\n",
       " 450: 446,\n",
       " 451: 447,\n",
       " 452: 448,\n",
       " 453: 449,\n",
       " 454: 450,\n",
       " 455: 451,\n",
       " 456: 452,\n",
       " 457: 453,\n",
       " 458: 454,\n",
       " 459: 455,\n",
       " 460: 456,\n",
       " 461: 457,\n",
       " 462: 458,\n",
       " 463: 459,\n",
       " 464: 460,\n",
       " 465: 461,\n",
       " 466: 462,\n",
       " 467: 463,\n",
       " 468: 464,\n",
       " 469: 465,\n",
       " 470: 466,\n",
       " 471: 467,\n",
       " 472: 468,\n",
       " 473: 469,\n",
       " 474: 470,\n",
       " 475: 471,\n",
       " 476: 472,\n",
       " 477: 473,\n",
       " 478: 474,\n",
       " 479: 475,\n",
       " 480: 476,\n",
       " 481: 477,\n",
       " 482: 478,\n",
       " 483: 479,\n",
       " 484: 480,\n",
       " 485: 481,\n",
       " 486: 482,\n",
       " 487: 483,\n",
       " 488: 484,\n",
       " 489: 485,\n",
       " 490: 486,\n",
       " 491: 487,\n",
       " 492: 488,\n",
       " 493: 489,\n",
       " 494: 490,\n",
       " 495: 491,\n",
       " 496: 492,\n",
       " 497: 493,\n",
       " 498: 494,\n",
       " 499: 495,\n",
       " 501: 496,\n",
       " 502: 497,\n",
       " 503: 498,\n",
       " 504: 499,\n",
       " 505: 500,\n",
       " 506: 501,\n",
       " 507: 502,\n",
       " 508: 503,\n",
       " 509: 504,\n",
       " 510: 505,\n",
       " 511: 506,\n",
       " 512: 507,\n",
       " 513: 508,\n",
       " 514: 509,\n",
       " 515: 510,\n",
       " 516: 511,\n",
       " 517: 512,\n",
       " 518: 513,\n",
       " 519: 514,\n",
       " 520: 515,\n",
       " 521: 516,\n",
       " 522: 517,\n",
       " 523: 518,\n",
       " 524: 519,\n",
       " 525: 520,\n",
       " 526: 521,\n",
       " 527: 522,\n",
       " 528: 523,\n",
       " 529: 524,\n",
       " 530: 525,\n",
       " 531: 526,\n",
       " 532: 527,\n",
       " 533: 528,\n",
       " 534: 529,\n",
       " 535: 530,\n",
       " 536: 531,\n",
       " 537: 532,\n",
       " 538: 533,\n",
       " 539: 534,\n",
       " 540: 535,\n",
       " 541: 536,\n",
       " 542: 537,\n",
       " 543: 538,\n",
       " 544: 539,\n",
       " 545: 540,\n",
       " 546: 541,\n",
       " 547: 542,\n",
       " 548: 543,\n",
       " 549: 544,\n",
       " 550: 545,\n",
       " 551: 546,\n",
       " 552: 547,\n",
       " 553: 548,\n",
       " 554: 549,\n",
       " 555: 550,\n",
       " 556: 551,\n",
       " 557: 552,\n",
       " 558: 553,\n",
       " 559: 554,\n",
       " 560: 555,\n",
       " 561: 556,\n",
       " 562: 557,\n",
       " 563: 558,\n",
       " 564: 559,\n",
       " 565: 560,\n",
       " 566: 561,\n",
       " 567: 562,\n",
       " 568: 563,\n",
       " 569: 564,\n",
       " 570: 565,\n",
       " 571: 566,\n",
       " 572: 567,\n",
       " 573: 568,\n",
       " 670: 568,\n",
       " 574: 569,\n",
       " 575: 570,\n",
       " 576: 571,\n",
       " 577: 572,\n",
       " 578: 573,\n",
       " 579: 574,\n",
       " 580: 575,\n",
       " 581: 576,\n",
       " 582: 577,\n",
       " 583: 578,\n",
       " 584: 579,\n",
       " 585: 580,\n",
       " 586: 581,\n",
       " 587: 582,\n",
       " 588: 583,\n",
       " 589: 584,\n",
       " 590: 585,\n",
       " 591: 586,\n",
       " 592: 587,\n",
       " 593: 588,\n",
       " 594: 589,\n",
       " 595: 590,\n",
       " 596: 591,\n",
       " 597: 592,\n",
       " 598: 593,\n",
       " 599: 594,\n",
       " 600: 595,\n",
       " 601: 596,\n",
       " 602: 597,\n",
       " 603: 598,\n",
       " 604: 599,\n",
       " 605: 600,\n",
       " 606: 601,\n",
       " 607: 602,\n",
       " 608: 603,\n",
       " 609: 604,\n",
       " 610: 605,\n",
       " 611: 606,\n",
       " 612: 607,\n",
       " 613: 608,\n",
       " 614: 609,\n",
       " 615: 610,\n",
       " 616: 611,\n",
       " 617: 612,\n",
       " 618: 613,\n",
       " 619: 614,\n",
       " 620: 615,\n",
       " 621: 616,\n",
       " 622: 617,\n",
       " 623: 618,\n",
       " 624: 619,\n",
       " 625: 620,\n",
       " 626: 621,\n",
       " 627: 622,\n",
       " 628: 623,\n",
       " 629: 624,\n",
       " 630: 625,\n",
       " 631: 626,\n",
       " 632: 627,\n",
       " 633: 628,\n",
       " 634: 629,\n",
       " 635: 630,\n",
       " 636: 631,\n",
       " 637: 632,\n",
       " 638: 633,\n",
       " 639: 634,\n",
       " 640: 635,\n",
       " 641: 636,\n",
       " 642: 637,\n",
       " 643: 638,\n",
       " 644: 639,\n",
       " 645: 640,\n",
       " 646: 641,\n",
       " 647: 642,\n",
       " 648: 643,\n",
       " 649: 644,\n",
       " 650: 645,\n",
       " 651: 646,\n",
       " 652: 647,\n",
       " 653: 648,\n",
       " 654: 649,\n",
       " 655: 650,\n",
       " 656: 651,\n",
       " 657: 652,\n",
       " 658: 653,\n",
       " 659: 654,\n",
       " 660: 655,\n",
       " 661: 656,\n",
       " 662: 657,\n",
       " 663: 658,\n",
       " 664: 659,\n",
       " 665: 660,\n",
       " 666: 661,\n",
       " 667: 662,\n",
       " 668: 663,\n",
       " 669: 664,\n",
       " 671: 665,\n",
       " 672: 666,\n",
       " 673: 667,\n",
       " 674: 668,\n",
       " 675: 669,\n",
       " 676: 670,\n",
       " 677: 671,\n",
       " 678: 672,\n",
       " 679: 673,\n",
       " 681: 674,\n",
       " 682: 675,\n",
       " 683: 676,\n",
       " 684: 677,\n",
       " 685: 678,\n",
       " 686: 679,\n",
       " 687: 680,\n",
       " 688: 681,\n",
       " 689: 682,\n",
       " 690: 683,\n",
       " 691: 684,\n",
       " 692: 685,\n",
       " 693: 686,\n",
       " 694: 687,\n",
       " 695: 688,\n",
       " 696: 689,\n",
       " 697: 690,\n",
       " 698: 691,\n",
       " 699: 692,\n",
       " 700: 693,\n",
       " 701: 694,\n",
       " 702: 695,\n",
       " 703: 696,\n",
       " 704: 697,\n",
       " 705: 698,\n",
       " 706: 699,\n",
       " 707: 700,\n",
       " 708: 701,\n",
       " 709: 702,\n",
       " 710: 703,\n",
       " 711: 704,\n",
       " 1658: 704,\n",
       " 712: 705,\n",
       " 713: 706,\n",
       " 714: 707,\n",
       " 715: 708,\n",
       " 716: 709,\n",
       " 717: 710,\n",
       " 718: 711,\n",
       " 719: 712,\n",
       " 720: 713,\n",
       " 721: 714,\n",
       " 722: 715,\n",
       " 723: 716,\n",
       " 724: 717,\n",
       " 725: 718,\n",
       " 726: 719,\n",
       " 727: 720,\n",
       " 728: 721,\n",
       " 729: 722,\n",
       " 730: 723,\n",
       " 731: 724,\n",
       " 732: 725,\n",
       " 733: 726,\n",
       " 734: 727,\n",
       " 735: 728,\n",
       " 736: 729,\n",
       " 737: 730,\n",
       " 738: 731,\n",
       " 739: 732,\n",
       " 740: 733,\n",
       " 741: 734,\n",
       " 742: 735,\n",
       " 743: 736,\n",
       " 744: 737,\n",
       " 745: 738,\n",
       " 746: 739,\n",
       " 747: 740,\n",
       " 748: 741,\n",
       " 749: 742,\n",
       " 750: 743,\n",
       " 751: 744,\n",
       " 752: 745,\n",
       " 753: 746,\n",
       " 754: 747,\n",
       " 755: 748,\n",
       " 756: 749,\n",
       " 757: 750,\n",
       " 758: 751,\n",
       " 759: 752,\n",
       " 760: 753,\n",
       " 761: 754,\n",
       " 762: 755,\n",
       " 763: 756,\n",
       " 764: 757,\n",
       " 765: 758,\n",
       " 766: 759,\n",
       " 767: 760,\n",
       " 768: 761,\n",
       " 769: 762,\n",
       " 770: 763,\n",
       " 771: 764,\n",
       " 772: 765,\n",
       " 773: 766,\n",
       " 774: 767,\n",
       " 775: 768,\n",
       " 776: 769,\n",
       " 777: 770,\n",
       " 778: 771,\n",
       " 779: 772,\n",
       " 780: 773,\n",
       " 781: 774,\n",
       " 782: 775,\n",
       " 783: 776,\n",
       " 784: 777,\n",
       " 785: 778,\n",
       " 786: 779,\n",
       " 787: 780,\n",
       " 788: 781,\n",
       " 789: 782,\n",
       " 790: 783,\n",
       " 791: 784,\n",
       " 792: 785,\n",
       " 793: 786,\n",
       " 794: 787,\n",
       " 795: 788,\n",
       " 796: 789,\n",
       " 797: 790,\n",
       " 798: 791,\n",
       " 799: 792,\n",
       " 800: 793,\n",
       " 801: 794,\n",
       " 802: 795,\n",
       " 803: 796,\n",
       " 804: 797,\n",
       " 805: 798,\n",
       " 806: 799,\n",
       " 807: 800,\n",
       " 808: 801,\n",
       " 809: 802,\n",
       " 810: 803,\n",
       " 811: 804,\n",
       " 812: 805,\n",
       " 813: 806,\n",
       " 814: 807,\n",
       " 815: 808,\n",
       " 816: 809,\n",
       " 817: 810,\n",
       " 818: 811,\n",
       " 819: 812,\n",
       " 820: 813,\n",
       " 821: 814,\n",
       " 822: 815,\n",
       " 823: 816,\n",
       " 824: 817,\n",
       " 825: 818,\n",
       " 826: 819,\n",
       " 827: 820,\n",
       " 828: 821,\n",
       " 829: 822,\n",
       " 830: 823,\n",
       " 831: 824,\n",
       " 832: 825,\n",
       " 833: 826,\n",
       " 834: 827,\n",
       " 835: 828,\n",
       " 836: 829,\n",
       " 837: 830,\n",
       " 838: 831,\n",
       " 839: 832,\n",
       " 840: 833,\n",
       " 841: 834,\n",
       " 842: 835,\n",
       " 843: 836,\n",
       " 844: 837,\n",
       " 845: 838,\n",
       " 846: 839,\n",
       " 847: 840,\n",
       " 848: 841,\n",
       " 849: 842,\n",
       " 850: 843,\n",
       " 851: 844,\n",
       " 852: 845,\n",
       " 853: 846,\n",
       " 854: 847,\n",
       " 855: 848,\n",
       " 856: 849,\n",
       " 857: 850,\n",
       " 858: 851,\n",
       " 859: 852,\n",
       " 860: 853,\n",
       " 861: 854,\n",
       " 862: 855,\n",
       " 863: 856,\n",
       " 864: 857,\n",
       " 866: 858,\n",
       " 867: 859,\n",
       " 868: 860,\n",
       " 869: 861,\n",
       " 870: 862,\n",
       " 871: 863,\n",
       " 872: 864,\n",
       " 873: 865,\n",
       " 874: 866,\n",
       " 875: 867,\n",
       " 876: 868,\n",
       " 881: 868,\n",
       " 877: 869,\n",
       " 878: 870,\n",
       " 1003: 870,\n",
       " 879: 871,\n",
       " 880: 872,\n",
       " 882: 873,\n",
       " 883: 874,\n",
       " 884: 875,\n",
       " 885: 876,\n",
       " 886: 877,\n",
       " 887: 878,\n",
       " 888: 879,\n",
       " 889: 880,\n",
       " 890: 881,\n",
       " 891: 882,\n",
       " 892: 883,\n",
       " 893: 884,\n",
       " 894: 885,\n",
       " 895: 886,\n",
       " 896: 887,\n",
       " 897: 888,\n",
       " 898: 889,\n",
       " 899: 890,\n",
       " 900: 891,\n",
       " 901: 892,\n",
       " 902: 893,\n",
       " 903: 894,\n",
       " 904: 895,\n",
       " 905: 896,\n",
       " 906: 897,\n",
       " 907: 898,\n",
       " 908: 899,\n",
       " 909: 900,\n",
       " 910: 901,\n",
       " 911: 902,\n",
       " 912: 903,\n",
       " 913: 904,\n",
       " 914: 905,\n",
       " 915: 906,\n",
       " 916: 907,\n",
       " 917: 908,\n",
       " 918: 909,\n",
       " 919: 910,\n",
       " 920: 911,\n",
       " 921: 912,\n",
       " 922: 913,\n",
       " 923: 914,\n",
       " 924: 915,\n",
       " 925: 916,\n",
       " 926: 917,\n",
       " 927: 918,\n",
       " 928: 919,\n",
       " 929: 920,\n",
       " 930: 921,\n",
       " 931: 922,\n",
       " 932: 923,\n",
       " 933: 924,\n",
       " 934: 925,\n",
       " 935: 926,\n",
       " 936: 927,\n",
       " 937: 928,\n",
       " 938: 929,\n",
       " 939: 930,\n",
       " 940: 931,\n",
       " 941: 932,\n",
       " 942: 933,\n",
       " 943: 934,\n",
       " 944: 935,\n",
       " 945: 936,\n",
       " 946: 937,\n",
       " 947: 938,\n",
       " 948: 939,\n",
       " 949: 940,\n",
       " 950: 941,\n",
       " 951: 942,\n",
       " 952: 943,\n",
       " 953: 944,\n",
       " 954: 945,\n",
       " 955: 946,\n",
       " 956: 947,\n",
       " 957: 948,\n",
       " 958: 949,\n",
       " 959: 950,\n",
       " 960: 951,\n",
       " 961: 952,\n",
       " 962: 953,\n",
       " 963: 954,\n",
       " 964: 955,\n",
       " 965: 956,\n",
       " 966: 957,\n",
       " 967: 958,\n",
       " 968: 959,\n",
       " 969: 960,\n",
       " 970: 961,\n",
       " 971: 962,\n",
       " 972: 963,\n",
       " 973: 964,\n",
       " 974: 965,\n",
       " 975: 966,\n",
       " 976: 967,\n",
       " 977: 968,\n",
       " 978: 969,\n",
       " 979: 970,\n",
       " 980: 971,\n",
       " 981: 972,\n",
       " 982: 973,\n",
       " 983: 974,\n",
       " 984: 975,\n",
       " 985: 976,\n",
       " 986: 977,\n",
       " 987: 978,\n",
       " 988: 979,\n",
       " 989: 980,\n",
       " 990: 981,\n",
       " 991: 982,\n",
       " 992: 983,\n",
       " 993: 984,\n",
       " 994: 985,\n",
       " 995: 986,\n",
       " 996: 987,\n",
       " 997: 988,\n",
       " ...}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_id_item_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create another dict mapping moving titles to this new unique id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unique_item_dict\n",
    "unique_item_dict = {unique_id_item_dict[k]: v for k, v in item_dict.items()}\n",
    "\n",
    "assert(len(set(unique_item_dict.keys())) == \n",
    "       len(set(unique_item_dict.values())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use our `returnItemId()` mehtod safely =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(239, \"b'Beavis and Butt-head Do America (1996)'\"),\n",
       " (431, \"b'Butch Cassidy and the Sundance Kid (1969)'\"),\n",
       " (575,\n",
       "  \"b'Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)'\"),\n",
       " (1390, \"b'M. Butterfly (1993)'\"),\n",
       " (1448, \"b'Madame Butterfly (1995)'\"),\n",
       " (1601, \"b'Reluctant Debutante, The (1958)'\"),\n",
       " (1607, \"b'Butterfly Kiss (1995)'\"),\n",
       " (1630, \"b'Butcher Boy, The (1998)'\")]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnItemId('but', unique_item_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(240, \"b'Beavis and Butt-head Do America (1996)'\"),\n",
       " (435, \"b'Butch Cassidy and the Sundance Kid (1969)'\"),\n",
       " (580,\n",
       "  \"b'Englishman Who Went Up a Hill, But Came Down a Mountain, The (1995)'\"),\n",
       " (1401, \"b'M. Butterfly (1993)'\"),\n",
       " (1459, \"b'Madame Butterfly (1995)'\"),\n",
       " (1614, \"b'Reluctant Debutante, The (1958)'\"),\n",
       " (1621, \"b'Butterfly Kiss (1995)'\"),\n",
       " (1645, \"b'Butcher Boy, The (1998)'\"),\n",
       " (1650, \"b'Butcher Boy, The (1998)'\")]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "returnItemId('but', item_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Train and test sets\n",
    "\n",
    "GroupLens provides several splits of the dataset, so that we can check the goodness of our algorithms. See the README file for more  details. Here we will use one of such splits.\n",
    "\n",
    "Please notice that we have to correct for the non-unique movie's id issue!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allbut.pl  u1.base  u2.test  u4.base  u5.test  ub.base\tu.genre  u.occupation\r\n",
      "mku.sh\t   u1.test  u3.base  u4.test  ua.base  ub.test\tu.info\t u.user\r\n",
      "README\t   u2.base  u3.test  u5.base  ua.test  u.data\tu.item\r\n"
     ]
    }
   ],
   "source": [
    "# ls data_root\n",
    "!ls $data_root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\t1\t5\t874965758\r\n",
      "1\t2\t3\t876893171\r\n",
      "1\t3\t4\t878542960\r\n",
      "1\t4\t3\t876893119\r\n",
      "1\t5\t3\t889751712\r\n",
      "1\t6\t5\t887431973\r\n",
      "1\t7\t4\t875071561\r\n",
      "1\t8\t1\t875072484\r\n",
      "1\t9\t5\t878543541\r\n",
      "1\t10\t3\t875693118\r\n"
     ]
    }
   ],
   "source": [
    "# trainfile\n",
    "trainfile = os.path.join(data_root, 'ua.base')\n",
    "!head $trainfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 943 users, 1680 items and 90570 pairs in the train set\n"
     ]
    }
   ],
   "source": [
    "# train dataframe\n",
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "trainfile = os.path.join(data_root, 'ua.base')\n",
    "train = pd.read_csv(trainfile, sep='\\t', names=columns)\n",
    "print(f'There are %s users, %s items and %s pairs in the train set' % (train.user_id.unique().shape[0], \\\n",
    "                                                                       train.item_id.unique().shape[0], \\\n",
    "                                                                       train.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 943 users, 1129 items and 9430 pairs in the train set\n"
     ]
    }
   ],
   "source": [
    "# test dataframe\n",
    "columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "testfile = os.path.join(data_root, 'ua.test')\n",
    "test = pd.read_csv(testfile, sep='\\t', names=columns)\n",
    "print(f'There are %s users, %s items and %s pairs in the train set' % (test.user_id.unique().shape[0], \\\n",
    "                                                                       test.item_id.unique().shape[0], \\\n",
    "                                                                       test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting for non-unique movies id "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Reminder of lambda functions in Python: is a way of calling short functions (1 line of code), without having to define the function in a separted cell.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define lambda function\n",
    "def func(x):\n",
    "    return unique_id_item_dict[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     19\n",
       "1     32\n",
       "2     60\n",
       "3    116\n",
       "4    154\n",
       "Name: item_id, dtype: int64"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex\n",
    "test['item_id'].apply(lambda z: func(z)).head()\n",
    "test['item_id'].apply(lambda z: unique_id_item_dict[z]).head()\n",
    "test['item_id'].apply(func).head()\n",
    "#3 formas de hacer lo mismo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 1662 unique items in the train set\n"
     ]
    }
   ],
   "source": [
    "# Apply lambda to train\n",
    "train['item_id'] = train['item_id'].apply(lambda x: unique_id_item_dict[x])\n",
    "print('Now there are %s unique items in the train set' % train.item_id.unique().shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now there are 1119 unique items in the test set\n"
     ]
    }
   ],
   "source": [
    "# Apply lambda to test\n",
    "test['item_id'] = test['item_id'].apply(func)\n",
    "print('Now there are %s unique items in the test set' % test.item_id.unique().shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='popular'></a>\n",
    "## 2. Most popular movies\n",
    "\n",
    "Recommending popular items is a simple, yet quite effective baseline for recommendation. Indeed, most RS suffer from a strong *popularity bias*, i.e. they tend to recommend popular items more frequently than they should -just because suggesting what is popular is effective!-. There is a lot of research  devote to understand this behaviour and to develop recipies to avoid it. \n",
    "\n",
    "Movies can be ranked according to different popularity metrics:\n",
    "* Most rated movie (it is assumed that this is the most watched movie)\n",
    "* Most positively rated movie (rating > 4.0)\n",
    "* Highest rated movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Most rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "0    392\n",
       "1    121\n",
       "2     85\n",
       "3    198\n",
       "4     79\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# group the train dataset by item and count the number of users using Pandas\n",
    "most_rated = train.groupby('item_id')['user_id'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "0    392\n",
       "1    121\n",
       "2     85\n",
       "3    198\n",
       "4     79\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_rated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sort in descending order\n",
    "most_rated_sorted = most_rated.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "item_id\n",
       "49     495\n",
       "99     443\n",
       "180    439\n",
       "257    412\n",
       "284    400\n",
       "Name: user_id, dtype: int64"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_rated_sorted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1662"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print shape\n",
    "most_rated_sorted.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Int64Index([  49,   99,  180,  257,  284,  292,    0,  286,  120,  173,\n",
       "            ...\n",
       "            1525, 1551,  807, 1532, 1535, 1537, 1546, 1548, 1550, 1663],\n",
       "           dtype='int64', name='item_id', length=1662)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indices\n",
    "most_rated_sorted.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2996/2762707652.py:2: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  most_rated_movies = np.zeros(shape=(most_rated_sorted.shape[0], 3), dtype=np.object)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[\"b'Star Wars (1977)'\", 495],\n",
       "       [\"b'Fargo (1996)'\", 443],\n",
       "       [\"b'Return of the Jedi (1983)'\", 439],\n",
       "       [\"b'Contact (1997)'\", 412],\n",
       "       [\"b'English Patient, The (1996)'\", 400],\n",
       "       [\"b'Liar Liar (1997)'\", 398],\n",
       "       [\"b'Toy Story (1995)'\", 392],\n",
       "       [\"b'Scream (1996)'\", 386],\n",
       "       [\"b'Independence Day (ID4) (1996)'\", 384],\n",
       "       [\"b'Raiders of the Lost Ark (1981)'\", 379]], dtype=object)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a numpy array of mostRatedMovies with structure [id, title, frequency]\n",
    "most_rated_movies = np.zeros(shape=(most_rated_sorted.shape[0], 3), dtype=np.object)\n",
    "\n",
    "for i, index in enumerate(most_rated_sorted.index):\n",
    "    id_ = index\n",
    "    freg_ = most_rated_sorted[id_]\n",
    "    title_ = unique_item_dict[id_]\n",
    "    most_rated_movies[i] = [id_, title_, freg_]\n",
    "\n",
    "most_rated_movies[:10, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Most positively rated movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter movies rated with rating >=4.0. Then group by item, count the number of users and sort in descending order.\n",
    "positive_rated = train[train.rating >= 4.0] \\\n",
    "    .groupby('item_id')['user_id'].count() \\\n",
    "    .sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2996/129789981.py:2: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  most_positive_rated = np.zeros(shape=(positive_rated.shape[0], 3), dtype=np.object)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[\"b'Star Wars (1977)'\", 428],\n",
       "       [\"b'Fargo (1996)'\", 354],\n",
       "       [\"b'Return of the Jedi (1983)'\", 331],\n",
       "       [\"b'Raiders of the Lost Ark (1981)'\", 316],\n",
       "       [\"b'Silence of the Lambs, The (1991)'\", 310],\n",
       "       [\"b'Godfather, The (1972)'\", 298],\n",
       "       [\"b'Contact (1997)'\", 276],\n",
       "       [\"b'Toy Story (1995)'\", 275],\n",
       "       [\"b'Empire Strikes Back, The (1980)'\", 264],\n",
       "       [\"b'Pulp Fiction (1994)'\", 262]], dtype=object)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a numpy array of positiveRatedMovies with structure [id, title, frequency]\n",
    "most_positive_rated = np.zeros(shape=(positive_rated.shape[0], 3), dtype=np.object)\n",
    "\n",
    "for i, index in enumerate(positive_rated.index):\n",
    "    id_ = index\n",
    "    freg_ = positive_rated[id_]\n",
    "    title_ = unique_item_dict[id_]\n",
    "    most_positive_rated[i] = [id_, title_, freg_]\n",
    "\n",
    "most_positive_rated[:10, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Highest mean rating movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtaine the highest rated movies, with a minium number of users/ratings.\n",
    "min_ratings = 50\n",
    "\n",
    "# group the ratings by item and stack them in a list\n",
    "list_rated_movies = train.groupby('item_id')['rating'].apply(list).reset_index()\n",
    "\n",
    "# filter movies with a minimum number of ratings\n",
    "filtered_list_rated_movies = list_rated_movies[list_rated_movies.rating.apply(lambda x: len(x) > min_ratings)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>[5, 4, 4, 4, 3, 1, 5, 5, 3, 5, 5, 5, 3, 5, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>[3, 3, 2, 3, 5, 1, 3, 3, 4, 4, 3, 2, 2, 3, 4, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>[4, 2, 3, 4, 3, 2, 2, 1, 3, 3, 5, 3, 3, 3, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>[3, 5, 4, 5, 5, 5, 3, 5, 4, 2, 4, 4, 3, 3, 2, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>[3, 1, 2, 3, 4, 4, 4, 3, 3, 2, 4, 3, 3, 4, 3, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   item_id                                             rating\n",
       "0        0  [5, 4, 4, 4, 3, 1, 5, 5, 3, 5, 5, 5, 3, 5, 4, ...\n",
       "1        1  [3, 3, 2, 3, 5, 1, 3, 3, 4, 4, 3, 2, 2, 3, 4, ...\n",
       "2        2  [4, 2, 3, 4, 3, 2, 2, 1, 3, 3, 5, 3, 3, 3, 2, ...\n",
       "3        3  [3, 5, 4, 5, 5, 5, 3, 5, 4, 2, 4, 4, 3, 3, 2, ...\n",
       "4        4  [3, 1, 2, 3, 4, 4, 4, 3, 3, 2, 4, 3, 3, 4, 3, ..."
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_list_rated_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain the mean of the list of rating per movie\n",
    "mean_movies = filtered_list_rated_movies.rating.apply(lambda x: np.mean(np.array(x))).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2996/4192822962.py:2: DeprecationWarning: `np.object` is a deprecated alias for the builtin `object`. To silence this warning, use `object` by itself. Doing this will not modify any behavior and is safe. \n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  mean_rate_movies = np.zeros(shape=(mean_movies.shape[0], 3), dtype=np.object)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[\"b'Wallace & Gromit: The Best of Aardman Animation (1996)'\",\n",
       "        4.491525423728813],\n",
       "       [\"b'Close Shave, A (1995)'\", 4.480769230769231],\n",
       "       [\"b'Wrong Trousers, The (1993)'\", 4.4766355140186915],\n",
       "       ['b\"Schindler\\'s List (1993)\"', 4.4758364312267656],\n",
       "       [\"b'Casablanca (1942)'\", 4.459821428571429],\n",
       "       [\"b'Shawshank Redemption, The (1994)'\", 4.457364341085271],\n",
       "       [\"b'Usual Suspects, The (1995)'\", 4.386454183266932],\n",
       "       [\"b'Rear Window (1954)'\", 4.374358974358974],\n",
       "       [\"b'Star Wars (1977)'\", 4.365656565656566],\n",
       "       [\"b'12 Angry Men (1957)'\", 4.327433628318584]], dtype=object)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a numpy array of meanRateMovies with structure [id, title, frequency]\n",
    "mean_rate_movies = np.zeros(shape=(mean_movies.shape[0], 3), dtype=np.object)\n",
    "\n",
    "for i, index in enumerate(mean_movies.index):\n",
    "    id_ = index\n",
    "    freg_ = mean_movies[id_]\n",
    "    title_ = unique_item_dict[id_]\n",
    "    mean_rate_movies[i] = [id_, title_, freg_]\n",
    "\n",
    "mean_rate_movies[:10, 1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: Which method is better?? How to measure a recommender system? \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class  = \"alert alert-info\"> \n",
    "** IMPORTANT QUESTION **: When might be useful to recommend popular items?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='metrics'></a>\n",
    "## 3. Metrics for recommender systems\n",
    "\n",
    "As we have seen, even with the simplest solution --aka, recommending popular items-- is difficult to known which technique performs better. For this, there are a number of metrics that allow one to measure the goodness of a recommender system. \n",
    "\n",
    "Metrics can be design for measuring the relevance or accuracy of a recommendation, but they can be created for evaluating the novelty of a recommendation, or its diversity. \n",
    "\n",
    "For now, we will focus on relevance and accuracy. Several metrics exist:\n",
    "* Accuracy: rmse, mae.\n",
    "* Not ranked: Recall@k, Precision@k.\n",
    "* With rank disccount: map@k, ndcg@k.\n",
    "* With rank ordering: mean percentile rank.\n",
    "\n",
    "We will be definiing some of them whitin this class. For the moment, let's talk about precision and recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Precision and recall\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg\" alt=\"Precision and Recall in IR\" style=\"float: right; width: 300px\"/>\n",
    "\n",
    "The concept of precision and recall comes form the world of information retrieval, have a look at the wikipedia:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "From this entry:\n",
    "\n",
    " * \"**precision** (also called positive predictive value) is the fraction of retrieved instances that are relevant\".\n",
    " * \"**recall** (also known as sensitivity) is the fraction of relevant instances that are retrieved\".\n",
    "\n",
    "<br />\n",
    "<div class  = \"alert alert-info\"> \n",
    "** QUESTION **: how do we know if some movie, unknown to the user, is relevant?\n",
    "</div>\n",
    "\n",
    "In other words, we cannot measure a false positive --something recommended that was not relevant--. In this regard, only recall-oriented metrics have an actual meaning in RS. Nonetheless, its common practice to define both metrics in RS as follows:\n",
    " \n",
    "### $$\\mathrm{recall}@N = \\frac{\\sum_{k=1}^N rel(k)}{\\sum_{i\\in \\mathcal{I}_u} 1}$$\n",
    "### $$\\mathrm{precision}@N = \\frac{\\sum_{k=1}^N rel(k)}{N}$$\n",
    "\n",
    "Here, $\\mathcal{I}_u$ is the set of items adopted by user $u$, and $rel(k)$ is the relevance of a recommendation at position k in the list of recommendations. For ratings, the relevance could be defined as those movies rated above a certain threshold, e.g. $r_{ui}>4.0$. \n",
    "\n",
    "**Important to note: since precision is pretty much the same as recall in RS, metrcis usch as the *area under the ROC curve* doesn't have any meaning!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "As an example, consider a user that watched the following films:\n",
    "<br /><br />\n",
    "'Designated Mourner, The (1997)'\n",
    "<br />\n",
    "'Money Talks (1997)'\n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br />\n",
    "'Batman Forever (1995)'\n",
    "<br /><br />\n",
    "The recommended items were: \n",
    "<br /><br />\n",
    "'Batman (1989)' \n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br /><br />\n",
    "**What would be the recall and precision @1? and @2?**\n",
    "<br />\n",
    "**What do you think of recommending Batman? Is a bad or a good recommendation?**\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please notice that there isn't any actual difference between precision and recall in the context of RS: both measure the relevance of the recommendations, and tell nothing about items recommended that haven't been adopted by the user. Thus, it make sense to define a normalized recall as:\n",
    "\n",
    "### $$\\mathrm{recall}@N = \\frac{\\sum_{i=1}^N rel_i}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1})$$\n",
    "\n",
    "This way, results are normalized to 1 always."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**Exercise** Implement the above definition of recall\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_at_n(N, test, recommended, train=None):\n",
    "    \"\"\"\n",
    "    :param N: number of recommendations\n",
    "    :param test: list of movies seen by user in test\n",
    "    :param train: list of movies seen by user in train. This has to be removed from the recommended list \n",
    "    :param recommended: list of movies recommended\n",
    "    \n",
    "    :return the recall\n",
    "    \"\"\"\n",
    "    if train is not None:\n",
    "        rec_true = []\n",
    "        for r in recommended:\n",
    "            if r not in train:\n",
    "                rec_true.append(r)\n",
    "    else:\n",
    "        rec_true = recommended\n",
    "        \n",
    "    intersecction = len(set(test) & set(rec_true[:N]))\n",
    "    return intersecction / float(np.minimum(N, len(test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = ['Designated Mourner, The (1997)', 'Money Talks (1997)', 'Madame Butterfly (1995)', 'Batman Forever (1995)']\n",
    "recommended = ['Batman (1989)', 'Madame Butterfly (1995)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@1\n",
    "recall_at_n(1, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#@2\n",
    "recall_at_n(2, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n",
      "0.25\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# Check it's well normalized\n",
    "print(recall_at_n(3, seen, recommended))\n",
    "print(recall_at_n(10, seen, recommended))\n",
    "print(recall_at_n(100, seen, recommended))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, use this implementation to measure the efficiency of the popularity baselines in the test set. Use the top-5 movies, for instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"b'Star Wars (1977)'\", 495],\n",
       "       [\"b'Fargo (1996)'\", 443],\n",
       "       [\"b'Return of the Jedi (1983)'\", 439],\n",
       "       [\"b'Contact (1997)'\", 412],\n",
       "       [\"b'English Patient, The (1996)'\", 400]], dtype=object)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_rated_movies[:5,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"b'Star Wars (1977)'\", 428],\n",
       "       [\"b'Fargo (1996)'\", 354],\n",
       "       [\"b'Return of the Jedi (1983)'\", 331],\n",
       "       [\"b'Raiders of the Lost Ark (1981)'\", 316],\n",
       "       [\"b'Silence of the Lambs, The (1991)'\", 310]], dtype=object)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_positive_rated[:5,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[\"b'Wallace & Gromit: The Best of Aardman Animation (1996)'\",\n",
       "        4.491525423728813],\n",
       "       [\"b'Close Shave, A (1995)'\", 4.480769230769231],\n",
       "       [\"b'Wrong Trousers, The (1993)'\", 4.4766355140186915],\n",
       "       ['b\"Schindler\\'s List (1993)\"', 4.4758364312267656],\n",
       "       [\"b'Casablanca (1942)'\", 4.459821428571429]], dtype=object)"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rate_movies[:5,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>874965758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>876893171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>878542960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>876893119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>889751712</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating  timestamp\n",
       "0        1        0       5  874965758\n",
       "1        1        1       3  876893171\n",
       "2        1        2       4  878542960\n",
       "3        1        3       3  876893119\n",
       "4        1        4       3  889751712"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Since `recall_at_n` takes both train and test list per user, we need to create a dataset with the list of movies seen in train and test*\n",
    "\n",
    "Thus, get the list of movies per user in train and test, and join the two dataframes. For the join, use the pandas method `merge`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 2, 5, 6, 8, 11, 12, 13, 14, 15, 17, 18, 21...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 13, 24, 99, 110, 126, 236, 241, 254, 256, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[180, 259, 315, 317, 318, 324, 326, 337, 339, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[10, 257, 269, 298, 299, 321, 324, 326, 355, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[23, 28, 39, 41, 49, 61, 69, 88, 94, 99, 100, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>939</td>\n",
       "      <td>[8, 14, 117, 126, 219, 221, 236, 254, 256, 272...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>938</th>\n",
       "      <td>940</td>\n",
       "      <td>[6, 7, 11, 49, 55, 81, 88, 94, 95, 97, 146, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>941</td>\n",
       "      <td>[0, 292, 296, 298, 404, 451, 910, 997]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>942</td>\n",
       "      <td>[30, 49, 70, 78, 94, 96, 98, 123, 130, 171, 17...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>943</td>\n",
       "      <td>[1, 11, 21, 22, 23, 26, 27, 30, 40, 41, 49, 53...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>942 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id                                            item_id\n",
       "0          1  [0, 2, 5, 6, 8, 11, 12, 13, 14, 15, 17, 18, 21...\n",
       "1          2  [0, 13, 24, 99, 110, 126, 236, 241, 254, 256, ...\n",
       "2          3  [180, 259, 315, 317, 318, 324, 326, 337, 339, ...\n",
       "3          4  [10, 257, 269, 298, 299, 321, 324, 326, 355, 3...\n",
       "4          5  [23, 28, 39, 41, 49, 61, 69, 88, 94, 99, 100, ...\n",
       "..       ...                                                ...\n",
       "937      939  [8, 14, 117, 126, 219, 221, 236, 254, 256, 272...\n",
       "938      940  [6, 7, 11, 49, 55, 81, 88, 94, 95, 97, 146, 16...\n",
       "939      941             [0, 292, 296, 298, 404, 451, 910, 997]\n",
       "940      942  [30, 49, 70, 78, 94, 96, 98, 123, 130, 171, 17...\n",
       "941      943  [1, 11, 21, 22, 23, 26, 27, 30, 40, 41, 49, 53...\n",
       "\n",
       "[942 rows x 2 columns]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get movies in train per user. For this, group by user and get a list of item ids.\n",
    "trainUsersGrouped = train[train.rating >= 4.0].groupby('user_id')['item_id'].apply(list).reset_index()\n",
    "trainUsersGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[19, 32, 60, 159, 170, 201, 264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[12, 49, 250, 290, 295]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[325, 328]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[49, 259, 286, 292, 295, 350, 353, 357]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[0, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929</th>\n",
       "      <td>939</td>\n",
       "      <td>[120, 257, 405, 472, 682, 984, 1044, 1180]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>930</th>\n",
       "      <td>940</td>\n",
       "      <td>[65, 270, 312, 650]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>931</th>\n",
       "      <td>941</td>\n",
       "      <td>[6, 14, 116, 123, 146, 180, 256, 257, 471, 984]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>932</th>\n",
       "      <td>942</td>\n",
       "      <td>[116, 199, 260, 419, 423, 483, 579, 599]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>933</th>\n",
       "      <td>943</td>\n",
       "      <td>[10, 57, 110, 185, 214, 231, 352, 801]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>934 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id                                          item_id\n",
       "0          1                 [19, 32, 60, 159, 170, 201, 264]\n",
       "1          2                          [12, 49, 250, 290, 295]\n",
       "2          3                                       [325, 328]\n",
       "3          4          [49, 259, 286, 292, 295, 350, 353, 357]\n",
       "4          5                                          [0, 16]\n",
       "..       ...                                              ...\n",
       "929      939       [120, 257, 405, 472, 682, 984, 1044, 1180]\n",
       "930      940                              [65, 270, 312, 650]\n",
       "931      941  [6, 14, 116, 123, 146, 180, 256, 257, 471, 984]\n",
       "932      942         [116, 199, 260, 419, 423, 483, 579, 599]\n",
       "933      943           [10, 57, 110, 185, 214, 231, 352, 801]\n",
       "\n",
       "[934 rows x 2 columns]"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same with test data\n",
    "testUsersGrouped = test[test.rating >= 4.0].groupby('user_id')['item_id'].apply(list).reset_index()\n",
    "testUsersGrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the join\n",
    "joined = pd.merge(trainUsersGrouped, testUsersGrouped, how='inner', on='user_id', suffixes=('_train_', '_test_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id_train_</th>\n",
       "      <th>item_id_test_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[0, 2, 5, 6, 8, 11, 12, 13, 14, 15, 17, 18, 21...</td>\n",
       "      <td>[19, 32, 60, 159, 170, 201, 264]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[0, 13, 24, 99, 110, 126, 236, 241, 254, 256, ...</td>\n",
       "      <td>[12, 49, 250, 290, 295]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[180, 259, 315, 317, 318, 324, 326, 337, 339, ...</td>\n",
       "      <td>[325, 328]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[10, 257, 269, 298, 299, 321, 324, 326, 355, 3...</td>\n",
       "      <td>[49, 259, 286, 292, 295, 350, 353, 357]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[23, 28, 39, 41, 49, 61, 69, 88, 94, 99, 100, ...</td>\n",
       "      <td>[0, 16]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                     item_id_train_  \\\n",
       "0        1  [0, 2, 5, 6, 8, 11, 12, 13, 14, 15, 17, 18, 21...   \n",
       "1        2  [0, 13, 24, 99, 110, 126, 236, 241, 254, 256, ...   \n",
       "2        3  [180, 259, 315, 317, 318, 324, 326, 337, 339, ...   \n",
       "3        4  [10, 257, 269, 298, 299, 321, 324, 326, 355, 3...   \n",
       "4        5  [23, 28, 39, 41, 49, 61, 69, 88, 94, 99, 100, ...   \n",
       "\n",
       "                             item_id_test_  \n",
       "0         [19, 32, 60, 159, 170, 201, 264]  \n",
       "1                  [12, 49, 250, 290, 295]  \n",
       "2                               [325, 328]  \n",
       "3  [49, 259, 286, 292, 295, 350, 353, 357]  \n",
       "4                                  [0, 16]  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access test values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [0, 2, 5, 6, 8, 11, 12, 13, 14, 15, 17, 18, 21...\n",
       "1    [0, 13, 24, 99, 110, 126, 236, 241, 254, 256, ...\n",
       "2    [180, 259, 315, 317, 318, 324, 326, 337, 339, ...\n",
       "3    [10, 257, 269, 298, 299, 321, 324, 326, 355, 3...\n",
       "4    [23, 28, 39, 41, 49, 61, 69, 88, 94, 99, 100, ...\n",
       "dtype: object"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ex, concatenate both train and test list:\n",
    "joined.apply(lambda x: x[1] + x[2], axis = 1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.142857\n",
       "1    0.200000\n",
       "2    0.000000\n",
       "3    0.375000\n",
       "4    0.500000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate the recall of the mostRatedMovies recommendation, for each user:\n",
    "joined.apply(lambda l: recall_at_n(N=15, test=l[2], recommended=most_rated_movies[:,0], train=l[1]), axis=1).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*As you can see, some users have a quite large recall (0.5), while for others is small (e.g, 0.14). Let's calculate the mean.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23281797015737052"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topN = 30\n",
    "# calculate the average recall across all users\n",
    "recall_per_user = joined.apply(lambda l: recall_at_n(N=topN, test=l[2], recommended=most_rated_movies[:,0], train=l[1]), axis=1)\n",
    "\n",
    "recall_per_user.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2302500764759865"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall_per_user (positiveRatedMovies case)\n",
    "recall_per_user = joined.apply(lambda l: recall_at_n(N=topN, test=l[2], recommended=most_positive_rated[:,0], train=l[1]), axis=1)\n",
    "\n",
    "recall_per_user.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13298239352843227"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recall_per_user (meanRateMovies case)\n",
    "recall_per_user = joined.apply(lambda l: recall_at_n(N=topN, test=l[2], recommended=mean_rate_movies[:,0], train=l[1]), axis=1)\n",
    "\n",
    "recall_per_user.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Mean Averaged Precision (MAP) -- Advanced material\n",
    "\n",
    "Previous metrics did not account for the ranking of the recommendation, i.e. the relative position of a movie within the sorted list of recommendations. **But orders matters!** Metrics like MAP, MRR or NDCG try to tackle down this problem. \n",
    "\n",
    "From the blog *http://fastml.com/what-you-wanted-to-know-about-mean-average-precision/*:\n",
    "\n",
    "> Here’s another way to understand average precision. Wikipedia says AP is used to score document retrieval. You can think of it this way: you type something in Google and it shows you 10 results. It’s probably best if all of them were relevant. If only some are relevant, say five of them, then it’s much better if the relevant ones are shown first. It would be bad if first five were irrelevant and good ones only started from sixth, wouldn’t it? AP score reflects this.\n",
    "\n",
    "Implementation taken from:\n",
    "\n",
    "https://github.com/benhamner/Metrics/blob/master/Python/ml_metrics/average_precision.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Precision \n",
    "\n",
    "The Average Precision is definied as:\n",
    "\n",
    "### $$\\mathrm{AP}@N = \\frac{\\sum_{k=1}^N P(k) \\times rel(k)}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1)}$$\n",
    "\n",
    "where $P(k)$ is the precision at cut-off in the item list, i.e. the ratio of the number of recommended items adopted, up to the position k, over the number k. Thus:\n",
    "\n",
    "### $$\\mathrm{AP}@N = \\frac{\\sum_{k=1}^N \\left(\\sum_{i=1}^k rel(i)\\right)/k \\times rel(k)}{\\mathrm{min}(N, \\sum_{i\\in \\mathcal{I}_u} 1)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Following the example above, consider a user that watched the following films:\n",
    "<br /><br />\n",
    "'Designated Mourner, The (1997)'\n",
    "<br />\n",
    "'Money Talks (1997)'\n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br />\n",
    "'Batman Forever (1995)'\n",
    "<br /><br />\n",
    "The recommended items were: \n",
    "<br /><br />\n",
    "'Batman (1989)' \n",
    "<br />\n",
    "'Madame Butterfly (1995)'\n",
    "<br /><br />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "**Calculate AP@1**\n",
    "<br /><br />\n",
    "First, *rel(1)=0*, because Batman was not viewed. Also, *P(1) = 0*. Thus, AP@1=0.\n",
    "<br />\n",
    "**Calculate AP@2**\n",
    "<br /><br />\n",
    "As before, *rel(1)=0*, so the first term does not contribute. For the second term, *rel(2)=1*, so that *P(2)=0.5*. The numerator is hence:\n",
    "<br /><br />\n",
    "$P(1)*rel(1)+P(2)*rel(2)=0*0+0.5*1$\n",
    "<br /><br />\n",
    "For the denominator, $N=2$ and $\\sum_{i\\in \\mathcal{I}_u} 1)=4$, thus:\n",
    "<br /><br />\n",
    "AP@2 = 0.5/2 = 0.25\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now implement it =)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apk(N, test, recommended, train=None):\n",
    "    \"\"\"\n",
    "    Computes the average precision at N given recommendations.\n",
    "    \n",
    "    :param N: number of recommendations\n",
    "    :param test: list of movies seen by user in test\n",
    "    :param train: list of movies seen by user in train. This has to be removed from the recommended list \n",
    "    :param recommended: list of movies recommended\n",
    "    \n",
    "    :return The average precision at N over the test set\n",
    "    \"\"\"\n",
    "    if train is not None:\n",
    "        rec_true = []\n",
    "        for r in recommended:\n",
    "            if r not in train:\n",
    "                rec_true.append(r)\n",
    "    else:\n",
    "        rec_true = recommended\n",
    "     \n",
    "    predicted = rec_true[:N]\n",
    "    score = 0.0\n",
    "    num_hits = 0.0\n",
    "    \n",
    "    for i, p in enumerate(predicted):\n",
    "        if p in test and p not in predicted[:i]:\n",
    "            num_hits += 1.0\n",
    "            score += num_hits / (i + 1.0)\n",
    "    \n",
    "    return score / min(len(test), N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = ['Designated Mourner, The (1997)', 'Money Talks (1997)', 'Madame Butterfly (1995)', 'Batman Forever (1995)']\n",
    "recommended = ['Madame Butterfly (1995)', 'Batman (1989)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @1\n",
    "apk(1, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @2\n",
    "apk(2, seen, recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# @3\n",
    "apk(3, seen, recommended)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MAP\n",
    "\n",
    "Mean avergae precision is nothing else than the AP averaged across users ;)\n",
    "\n",
    "Apply it to popularity baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map@1=0.13\n",
      "map@30=0.06\n"
     ]
    }
   ],
   "source": [
    "# mostRatedMovies\n",
    "topN = 1\n",
    "predictions = most_rated_movies[:, 0]\n",
    "m1 = joined.apply(lambda l: apk(topN, l[2], predictions, l[1]), axis=1).mean()\n",
    "print('map@%s=%.2f' % (topN, m1))\n",
    "\n",
    "topN = 30\n",
    "predictions = most_rated_movies[:, 0]\n",
    "m1 = joined.apply(lambda l: apk(topN, l[2], predictions, l[1]), axis=1).mean()\n",
    "print('map@%s=%.2f' % (topN, m1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map@1=0.13\n",
      "map@30=0.07\n"
     ]
    }
   ],
   "source": [
    "# positiveRatedMovies\n",
    "topN = 1\n",
    "predictions = most_positive_rated[:, 0]\n",
    "m1 = joined.apply(lambda l: apk(topN, l[2], predictions, l[1]), axis=1).mean()\n",
    "print('map@%s=%.2f' % (topN, m1))\n",
    "\n",
    "topN = 30\n",
    "predictions = most_positive_rated[:, 0]\n",
    "m1 = joined.apply(lambda l: apk(topN, l[2], predictions, l[1]), axis=1).mean()\n",
    "print('map@%s=%.2f' % (topN, m1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "map@1=0.01\n",
      "map@30=0.02\n"
     ]
    }
   ],
   "source": [
    "# meanRateMovies\n",
    "topN = 1\n",
    "predictions = mean_rate_movies[:, 0]\n",
    "m1 = joined.apply(lambda l: apk(topN, l[2], predictions, l[1]), axis=1).mean()\n",
    "print('map@%s=%.2f' % (topN, m1))\n",
    "\n",
    "topN = 30\n",
    "predictions = mean_rate_movies[:, 0]\n",
    "m1 = joined.apply(lambda l: apk(topN, l[2], predictions, l[1]), axis=1).mean()\n",
    "print('map@%s=%.2f' % (topN, m1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://courses.edx.org/c4x/BerkeleyX/CS100.1x/asset/Collaborative_filtering.gif\" alt=\"collaborative filtering\" style=\"float: right; width: 300px\"/>\n",
    "\n",
    "## 4. Collaborative Filtering <a id='cf'></a>\n",
    "\n",
    "Perhaps, one of the most succesful techniques for making personalized recommendations are the so called *collaborative filtering* (CF) algorithms. CF is a method of making automatic predictions (filtering) about the interests of a user by collecting preferences or taste information from many users (collaborating). The underlying assumption of the collaborative filtering approach is that if a person A has the same opinion as a person B on an issue, A is more likely to have B's opinion on a different issue X than to have the opinion on X of a person chosen randomly. \n",
    "\n",
    "The image at the right (from Wikipedia) shows an example of user's preference prediction using collaborative filtering. At first, people rate different items (like videos, images, games). After that, the system is making predictions about a user's rating for an item, which the user has not rated yet. These predictions are built upon the existing ratings of other users, who have similar ratings with the active user. For instance, in the image at the right the system has made a prediction, that the active user will not like the video.\n",
    "\n",
    "In this part we will see three kinds of CF, of increasing complexity:\n",
    "\n",
    "4.1 [CF with co-occurrence](#copurchase)\n",
    "\n",
    "4.2 [Memory-based CF](#memory-base)\n",
    "\n",
    "4.3 [Model-based CF](#model-base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='copurchase'></a>\n",
    "## 4.1 Co-occurrence Matrix\n",
    "\n",
    "The idea is to recommend movies similar to the movies already seen by a user. A measurement of similarity among items is obtained from the co-occurrence matrix. This is nothing else than the adjacency matrix of the graph of items created by users!!!\n",
    "\n",
    "<table border=\"0\" style=\"width:825px;border:0px;\">\n",
    "<tr>\n",
    "    <td> \n",
    "        <img src=\"https://lucidworks.com/wp-content/uploads/2015/08/Les-Miserables-Co-Occurrence.png\" style=\"width: 500px\"/>\n",
    "    </td>\n",
    "    <td> \n",
    "        <img src=\"https://lucidworks.com/wp-content/uploads/2015/08/midnight-club-graph.png\" style=\"width: 400px\"/>\n",
    "    </td>\n",
    "</tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary of movies per user\n",
    "moviesPerUser = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the number of items in train\n",
    "n_items = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# co-ocurrance matrix will have shape=[n_items,n_items]\n",
    "coMatrix = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the matrix\n",
    "plt.matshow(coMatrix, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "**QUESTION:** Can you think of a better way of visualizaing this matrix? Try to rescale it, or to rearrenge it follwoing some criteria (for instance, popularity!).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_indexing = mostRatedMovies[:, 0].astype(np.int)\n",
    "coMatrix_sorted = coMatrix[:,popular_indexing]\n",
    "coMatrix_sorted_total = coMatrix_sorted[popular_indexing, :]\n",
    "log_scale = np.log(coMatrix_sorted_total+1.0)\n",
    "plt.matshow(log_scale, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Making predictions using the co-occurrence matrix\n",
    "\n",
    "This kind of recommendations, based on item similarity, provide a measure of the closeness of one item to another. In order to make a recommendation for a user, we have to proceed as follows:\n",
    "\n",
    "* First, define a function that returns the top-N closest items to a given one.\n",
    "* Then, for a list of items adopted by a specific user, select the top-N items from the lists of top-N closest items to each adopted item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrance_similarity(item_id, coocurrance, ntop=10):\n",
    "    \"\"\"\n",
    "    Returns the top-N most similar items to a given one, based on the coocurrance matrix\n",
    "    \n",
    "    :param item_id: id of input item\n",
    "    :param cooccurrance: 2-dim numpy array with the co-occurance matrix\n",
    "    :param ntop: number of items to be retrieved\n",
    "    \n",
    "    :return top-N most similar items to the given item_id\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First column are indices, while second one is the frequency of co-ocurrance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with he movie ID!!!!\n",
    "queryMovieId = 180\n",
    "Ntop = 5\n",
    "print('For item \"%s\" top-%s recommendations are:' % (unique_item_dict[queryMovieId], Ntop))\n",
    "\n",
    "similarItems = co_occurrance_similarity(queryMovieId, coMatrix, Ntop)\n",
    "# let's print out the first Ntop recommendations\n",
    "for r in similarItems:\n",
    "    print(unique_item_dict[int(r[0])], r[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let use this function to make recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occurrance_recommendation(items_id, cooccurrance, ntop=10):\n",
    "    \"\"\"\n",
    "    Obtain the list of ntop recommendations based on a list of items (user history of views)\n",
    "    \n",
    "    :param items_id: list of items ids\n",
    "    :param coocurrence: co-ocurrence matrix (numpy 2-dim array)\n",
    "    :param ntop: top-K items to be retrieved\n",
    "    \n",
    "    :return list of ntop items recommended\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get users in train with their movies\n",
    "trainUsersGrouped = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntop = 5\n",
    "# Get the recommendations for all users using the apply method\n",
    "predictions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positiveRatedMovies[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that, unlike previous popularity based models, the recommendation is now (slightly) different from one user to another**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (seen, recom) in zip(testUsersGrouped.values[:3, 1], predictions[:3]):\n",
    "    print(\"*\"*6)\n",
    "    print(\"Seen items: \")\n",
    "    print([unique_item_dict[i] for i in seen])\n",
    "    print(\"Recommended items: \")\n",
    "    print([unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evalute the recommendation\n",
    "\n",
    "For this, first add a column to `trainUserGrouped` with the predictions using the apply procedure above.\n",
    "Next, join this dataframe with the test dataframe, and get the `recall_at_n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topN = 30\n",
    "# add a prediction column to train\n",
    "trainUsersGrouped['prediction'] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join the df with train and predictions with the test df\n",
    "joined = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to see the help of method recall_at_n\n",
    "\n",
    "# recall_at_n??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average recall across all users\n",
    "recall = ...\n",
    "\n",
    "print(\"recall@%s=%.3f\"%(topN, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average recall across all users\n",
    "recall = ...\n",
    "\n",
    "print(\"recall@%s=%.3f\"%(topN, recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do the same for different top k values. It might be convenient to define a function!\n",
    "def evaluate_recall(topN, trainGrouped, testGrouped, coMatrix, popularity_baseline):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate for N in [3,10,30,50,100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Do the same analysis for the map metric*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following line to see the help of method apk\n",
    "\n",
    "# apk??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_map(topN, trainGrouped, testGrouped, coMatrix, popularity_baseline):\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate for N in [3,10,30,50,100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "Compare this results to those obtained with the popularity model. Was it so bad?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments:\n",
    "* The dataset we are using here is very simple. Indeed, you can see that the popularity baseline achieves quite decent metric values. This won't happen in a real world dataset! The reason it happens here is because the dataset is quite small, and quite biased towards popular items. \n",
    "\n",
    "* Recall does not account for the order of the recommendation, while map does. This explains why the co-occurrance model performs better after the first 10 recommendations in terms of map (ordering), while recall values are always better for the popularity based model.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Oher distances\n",
    "\n",
    "So far, we have defined the *closeness* of two items as the number of users shared. However, it would make make sense to define it relative the total number of users that have watch a movie. This can be done with the [Jaccard similarity index](https://en.wikipedia.org/wiki/Jaccard_index):\n",
    "\n",
    "$$J(i,j)=\\frac{|i\\cap j|}{|i|+|j|-|i\\cap j|}\\in [0,1]$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-success\">\n",
    "Build the Jaccard similarity matrix from the co-occurrance matrix. Notice that $CoM(i,j) = |i\\cap j|$ and $CoM(k,k) = |k|$. In addition, if both $|i|=0$ and $|j|=0$, the similarity is defined as 1 (this is a convention).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the diagonal of CoMatrix provides the number of visualizations of each movie\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the matrix\n",
    "plt.matshow(jaccard, fignum=1000, cmap=plt.cm.binary)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "popular_indexing = mostRatedMovies[:, 0].astype(np.int)\n",
    "jaccard_sorted = jaccard[:,popular_indexing]\n",
    "jaccard_sorted_total = jaccard_sorted[popular_indexing, :]\n",
    "\n",
    "# Remove ones:\n",
    "jaccard_sorted_total[jaccard_sorted_total == 1.0] = 0.0\n",
    "cax = plt.matshow(jaccard_sorted_total, fignum=1000, cmap=plt.cm.coolwarm)\n",
    "plt.gcf().colorbar(cax, ticks=[0, 0.1, 0.2, 0.25])\n",
    "plt.clim(0, 0.25)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queryMovieId = 40\n",
    "Ntop = 5\n",
    "print('For item \"%s\" top-%s similar items are:' % (unique_item_dict[queryMovieId], Ntop))\n",
    "\n",
    "similarItems = co_occurrance_similarity(queryMovieId, jaccard, Ntop)\n",
    "# let's print out the first Ntop recommendations\n",
    "for r in similarItems:\n",
    "    print(unique_item_dict[r[0]], r[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntop = 10\n",
    "# Calculate the predictoins with Jaccard\n",
    "predictions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (seen, recom) in zip(testUsersGrouped.values[:3, 1], predictions[:3]):\n",
    "    print(\"*\"*6)\n",
    "    print(\"Seen items: \")\n",
    "    print([unique_item_dict[i] for i in seen])\n",
    "    print(\"Recommended items: \")\n",
    "    print([unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall at [3,10,30,50,100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map at [3,10,30,50,100]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that the co-occurance model with Jaccard distance works better than the one counting occurances. Indeed, it has higher recall values than the popularity model for large top-N**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-info\">\n",
    "** QUESTION **: Can you think of any other way of using the graph of items?\n",
    "Some hints:\n",
    "\n",
    "<br></br>\n",
    "Page Rank\n",
    "<br></br>\n",
    "Shortest-path\n",
    "<br></br>\n",
    "Clustering methods: eigenvalues, spectral mehtods, etc.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='memory-base'></a>\n",
    "## 4.2. Memory-Based Collaborative Filtering (CF)\n",
    "\n",
    "Although the methods developed so far return a list of recommended items, they cannot be used to make an actual prediction regarding the rating. A quite different approach would be to calculate the unknown rating, $r_{ui}$, as the averaged of some other ratings, thta are somehow close to either the user or the item in question. \n",
    "\n",
    "Thus, one approach is to take\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K}\\sum_{j\\in\\mathcal{I}'} \\mathrm{sim}(i,j) r_{u,j},$$\n",
    "\n",
    "where items $j\\in\\mathcal{I}'$ are taken from the set of $K$ closest items to $i$, or from the whole dataset. This is known as **item-item collaborative filtering**, and can be interpreted as *“users who liked this movie also liked …”*. See Amazon famous patent: https://www.google.com/patents/US7113917. Basically, this technique will take an item, find users who liked that item, and find other items that those users or similar users also liked. \n",
    "\n",
    "Similarly, one can define a **user-user filtering** where predictions are made as\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K} \\sum_{v\\in\\mathcal{U}'} \\mathrm{sim}(u,v) r_{v,i}.$$\n",
    "\n",
    "<img src=\"https://soundsuggest.files.wordpress.com/2013/06/utility_matrix.png\" alt=\"utility matrix\" style=\"float: right; width: 400px\"/>\n",
    "\n",
    "In this case, the recommendation would be more like *“users who are similar to you also liked …”*. Both techniques are part of the broad familiy of **Memory-Based Collaborative Filtering** approaches, or neighborhood-based algorithms.\n",
    "\n",
    "The similarity among users or items can be calculated in a variety of forms: Pearson's correlation, cosine distance, etc. Here we will use the cosine distance. For this, we will first create the utility user-item matrix. \n",
    "\n",
    "The utility matrix is a dense representation of the user-item intearction. We have been using the *long* format, where missing entries are obviated; now, we will use the *wide* format, i.e. the matrix representation (see the figure on the right). \n",
    "\n",
    "<br></br>\n",
    "<div class = \"alert alert-info\">\n",
    "** NOTE **: Long and wide formats have its benefits and drawbacks. Can you think of some of them?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.values[:,0:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put the train and test datasets in wide format (i.e., like a matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility matrix\n",
    "\n",
    "uMatrixTraining = ...\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uMatrixTesting = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a similarity measure: cosine similarity\n",
    "\n",
    "### $$\\mathrm{sim}({\\bf a},{\\bf b})=\\frac{{\\bf a}\\cdot{\\bf b}}{\\sqrt{{\\bf a}\\cdot{\\bf a}}\\sqrt{{\\bf  b}\\cdot{\\bf b}}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosineSimilarity(ratings, kind='user', epsilon=1e-9):\n",
    "    \"\"\"\n",
    "    Calculate the cosine distance along the row (columns) of a matrix for users (items)\n",
    "    \n",
    "    :param ratings: a n_user X n_items matrix\n",
    "    :param kind: string indicating whether we are in mode 'user' or 'item'\n",
    "    :param epsilon: a small value to avoid dividing by zero (optional, defaults to 1e-9)\n",
    "    \n",
    "    :return a square matrix with the similarities\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kind = 'item'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1. User-user CF\n",
    "\n",
    "*“Users who are similar to you also liked …”*\n",
    "\n",
    "### $$r_{u,i} = \\frac{1}{K} \\sum_{v\\in\\mathcal{U}'} \\mathrm{sim}(u,v) r_{v,i}.$$\n",
    "Consider user $x$:\n",
    "\n",
    "1. Find other users whose ratings are “similar” to $x$’s ratings, i.e. calculate the similarity among users\n",
    "2. Estimate missing ratings based on ratings of similar users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use cosine similarity\n",
    "userSimilarity = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cax = plt.matshow(userSimilarity, fignum=1000, cmap=plt.cm.coolwarm)\n",
    "plt.gcf().colorbar(cax, ticks=[0, 0.25, 0.5])\n",
    "plt.clim(0, 0.5)\n",
    "plt.gcf().set_size_inches(18.5, 10.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(userSimilarity), np.std(userSimilarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that if we multiply `userSimilarity` by `uMatrixTraining` we get the ratings weigthed with user similar similarity. Then, we have to normalize by the average similarity for each user*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(userSimilarity.shape, uMatrixTraining.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userItemCFpredictions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful: take a look at the values\n",
    "np.max(userItemCFpredictions), np.min(userItemCFpredictions), np.mean(userItemCFpredictions), np.std(userItemCFpredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Note that some users might give generally lower ratings than others, so that we could have also corrected for this effect as follows*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ = uMatrixTraining.sum(axis=1)\n",
    "len_ =np.count_nonzero(uMatrixTraining, axis=1)\n",
    "average_ratings = np.tile(sum_/ len_, n_items).reshape([n_items, n_users]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uMatrixTraining_shifted = uMatrixTraining - np.multiply(average_ratings, uMatrixTraining)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userItemCFpredictions_corrected = average_ratings + userSimilarity.dot(uMatrixTraining_shifted) / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now rating values are more reasonable\n",
    "np.max(userItemCFpredictions_corrected), np.min(userItemCFpredictions_corrected), np.mean(userItemCFpredictions_corrected), np.std(userItemCFpredictions_corrected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Item-Item CF\n",
    "\n",
    "*“Users who liked this movie also liked …”*\n",
    "\n",
    "Consider item $i$:\n",
    "\n",
    "1. For item $i$, find other similar items, i.e. calculate the similarity among items\n",
    "2. Estimate rating for item $i$ based on ratings for similar items\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use cosine similarity\n",
    "itemSimilarity = ...\n",
    "print(itemSimilarity.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemItemCFpredictions = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemItemCFpredictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(itemItemCFpredictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\">\n",
    "**QUESTION:** Is averaging across all users or items computationally efficent? \n",
    "<br></br>\n",
    "<br></br>\n",
    "This is why nearest-neighbourghs methods (**KNN**) exists\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.3 Show some recommendations\n",
    "\n",
    "In case of item-item CF, the recommendation is pretty much the same as with the co-occurence matrix. It's also quite simple to find similar items to a given one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Find movies similar to a given one using the item-item similarity matrix.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's print out the most similar items to a given item\n",
    "queryMovieId = ...\n",
    "print(\"Selected item is '%s'\" % unique_item_dict[queryMovieId])\n",
    "\n",
    "\n",
    "queryAnswer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Calculate the recommendations obtained with the item-item CF model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove relevant items seen in train from our prediction:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in np.random.randint(0, n_users, 3):\n",
    "    print(\"*\"*6)\n",
    "    print(\"User %s\" % u)\n",
    "    print(\"Seen items: \")\n",
    "    seen = uMatrixTesting[u,:]\n",
    "    print([unique_item_dict[i] for i,r in enumerate(seen) if r>4.0])\n",
    "    print(\"Recommended items: \")\n",
    "    recom = itemItemCFpredictions[u,:]\n",
    "    recom = np.argsort(recom)[::-1][:10]\n",
    "    print([unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "Do the same with the user-user CF model.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for u in np.random.randint(0, n_users, 3):\n",
    "    print(\"*\"*6)\n",
    "    print(\"User %s\" % u)\n",
    "    print(\"Seen items: \")\n",
    "    seen = uMatrixTesting[u,:]\n",
    "    print([unique_item_dict[i] for i,r in enumerate(seen) if r>4.0])\n",
    "    print(\"Recommended items: \")\n",
    "    recom = userItemCFpredictions[u,:]\n",
    "    recom = np.argsort(recom)[::-1][:10]\n",
    "    print([unique_item_dict[i] for i in recom])\n",
    "    print(\"Recommended items (shifted version): \")\n",
    "    recom = userItemCFpredictions_corrected[u,:]\n",
    "    recom = np.argsort(recom)[::-1][:10]\n",
    "    print([unique_item_dict[i] for i in recom])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As you can see, the recommendations are pretty bad... Let's measure that**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.4 Measure the recommendations\n",
    "\n",
    "Since we are predicting ratings, it might make sense to introduce a metric that accounts for this. In particular, the **Root Mean Square Error (RMSE)** is typically used for this purpose. \n",
    "\n",
    "### $$\\mathrm{RMSE}=\\sqrt{\\frac{1}{n_{\\mathrm{users}}n_{\\mathrm{items}}}\\sum_{u,i}\\left(r_{u,i}-\\hat{r}_{u,i}\\right)^2}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "def rmse(prediction, ground_truth):\n",
    "    \"\"\"\n",
    "    Return the Root Mean Squared Error of the prediction\n",
    "    \n",
    "    :param prediction: a 2-dim numpy array with the predictions\n",
    "    :param ground_truth: a 2-dim numpy array with the known ratings\n",
    "    \n",
    "    :return the RMSE\n",
    "    \"\"\"\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User-based CF RMSE=%.3f' %rmse(userItemCFpredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('User-based (shifted) CF RMSE=%.3f' %rmse(userItemCFpredictions_corrected, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Item-based CF RMSE=%.3f' %rmse(itemItemCFpredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\">\n",
    "**IMPORTANT TO NOTE**: RMSE was used in the RecSys community for many years to measure the accuracy \n",
    "of recommendations. However, it was demonstrated that high accuracy in predicting rating does not imply a good\n",
    "ranked list!!    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate ranking metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "userItemCFpredictions_sorted = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put together the rows of the test, train and predicted item ids. This can be done easily with the zip command.\n",
    "# Be careful, though, because zip returns an iterable, so that it will dissapear once it's consumed. \n",
    "# To avoid this behavior, convert it to a list.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the zipped list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "As explained in class, the curse of dimensionality impedes us to obtain a good similarity measure among users or items (in this excercise, dimensions are ~1000; in a real Recommender Systems, they could be millions, billions or evene trillions!). This is the reason the above implementations do not work.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='model-base'></a>\n",
    "## 4.3. Model-based CF or Latent factor models\n",
    "There are several model-based CF: from matrix factorizations to bayesian models, neural netwroks, etc. In all of them, we try to extract latent factors (vectors) that model user and item interactions. In contrast to previous methods, our hypothesis here is that the dimension of the latent spaces is rather small (in the order of a hundred dimensions). Then, we use this latent features to make a prediction:\n",
    "\n",
    "## $$r_{u,i} \\approx {\\bf f}_u^T\\cdot{\\bf f}_i$$\n",
    "\n",
    "The underlying assumption is that both users and items *live* in the same latent space, and that we can unravel such space. \n",
    "\n",
    "<img src=\"https://www.researchgate.net/profile/Tunca_Dogan/publication/235913413/figure/fig3/AS:299678856957952@1448460415040/Figure-3-The-distribution-of-the-points-in-the-Swiss-roll-dataset-in-3-D-space.png\n",
    "\" alt=\"swiss roll\" style=\"float: center; width: 300px\"/>\n",
    "\n",
    "\n",
    "Here we will use a couple of linear Matrix Factorization (MF) models:\n",
    "\n",
    "* Singular Value decomposition (SVD)\n",
    "* Alternating Least Squares (ALS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.1 Singular value decomposition\n",
    "\n",
    "The main idea is to reduce the dimensionality of the input space. This is pretty much the same as Eigen-decomposition or Principal Component Analysis (PCA)-\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f5/GaussianScatterPCA.svg/220px-GaussianScatterPCA.svg.png\n",
    "\" alt=\"dimensionaly reducion\" style=\"float: center; width: 500px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the help!!!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get SVD components from train matrix. Choose k.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# take a look at the different matrices\n",
    "\n",
    "# U should be an orthogonal matrix with the left singular vectors as columns\n",
    "\n",
    "# Check U is orthogonal\n",
    "\n",
    "# Same with V\n",
    "\n",
    "# s is a vector with the singular values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the recommendations\n",
    "\n",
    "We will reconstruct the utility matrix R (i.e., the recommendation matrix) as follows:\n",
    "\n",
    "### $$M\\approx U\\mathrm{diag}(s)V^T$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a diagonal matrix with the eigenvalues\n",
    "\n",
    "# make the prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dimensions are correct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model\n",
    "\n",
    "* RMSE\n",
    "* Recall@30\n",
    "* MAP@30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('SVD RMSE=%.3f' % rmse(svdPredictions, uMatrixTesting))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the zipped list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class = \"alert alert-danger\">\n",
    "**IMPORTANT TO NOTE**: RMSE was used in the RecSys community for many years to measure the accuracy \n",
    "of recommendations. However, it was demonstrated that high accuracy in predicting rating does not imply a good\n",
    "ranked list!!    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same with more dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in [10, 50, 100]:\n",
    "    print(\"*\"*30)\n",
    "    print(\"Using %s latent dimensions\" %dim)\n",
    "    # apply Singular Value Decomposition\n",
    "    u, s, vt = svds(uMatrixTraining, dim)\n",
    "    s_diag_matrix = np.diag(s)\n",
    "    # make the prediction\n",
    "    svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "    print('SVD RMSE=%.3f' % rmse(svdPredictions, uMatrixTesting))\n",
    "    # recall\n",
    "    svdPredictions_sorted = np.argsort(svdPredictions)[::-1]\n",
    "    predicted_ids = trainUsersGrouped.user_id.apply(lambda i: svdPredictions_sorted[i-1]).values\n",
    "    zipped = list(zip(test_ids, train_ids, predicted_ids))\n",
    "    for k in [10, 30, 50, 100]:\n",
    "        recall = np.mean([recall_at_n(k,test, recom, train)  for (test, train, recom) in zipped])\n",
    "        print(\"recall@%s=%.3f\" %(k, recall))\n",
    "        map_ = np.mean([apk(k,test, recom, train)  for (test, train, recom) in zipped])\n",
    "        print(\"map@%s=%.3f\" %(k, map_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implicit vs Explicit feedback\n",
    "\n",
    "In the above SVD matrix factorization we have tried to reconstructt the matrix of ratings. Howvever, it might be easier trying to model the matrix of preferences (i.e., wether the user likes or not a movie).\n",
    "\n",
    "For this, we will define a “selector” matrix $I$ for the training utility matrix $R$, which will contain 0 if the rating matrix has no rating entry, and 1 if the rating matrix contains an entry. And the smae for testing data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index matrix for training data\n",
    "\n",
    "# Index matrix for test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in [50]:\n",
    "    print(\"*\"*30)\n",
    "    print(\"Using %s latent dimensions\" %dim)\n",
    "    # apply Singular Value Decomposition\n",
    "    u, s, vt = svds(I, dim)\n",
    "    s_diag_matrix = np.diag(s)\n",
    "    # make the prediction\n",
    "    svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "    print('SVD RMSE=%.3f' % rmse(svdPredictions, I2))\n",
    "    # recall\n",
    "    svdPredictions_sorted = np.argsort(svdPredictions)[::-1]\n",
    "    predicted_ids = trainUsersGrouped.user_id.apply(lambda i: svdPredictions_sorted[i-1]).values\n",
    "    zipped = list(zip(test_ids, train_ids, predicted_ids))\n",
    "    for k in [10, 30, 50, 100]:\n",
    "        recall = np.mean([recall_at_n(k,test, recom, train)  for (test, train, recom) in zipped])\n",
    "        print(\"recall@%s=%.3f\" %(k, recall))\n",
    "        map_ = np.mean([apk(k,test, recom, train)  for (test, train, recom) in zipped])\n",
    "        print(\"map@%s=%.3f\" %(k, map_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overfitting\n",
    "\n",
    "As we introduce more dimensions in the model, we make it more prone to overfit. This can be observed in the decrease of error (RMSE) in train, while it increases in test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_train = []\n",
    "error_test = []\n",
    "dims = [1, 2, 5, 7, 10, 20, 40, 80, 120, 160, 200]\n",
    "for dim in dims:\n",
    "    print(\"*\"*30)\n",
    "    print(\"Using %s latent dimensions\" %dim)\n",
    "    # apply Singular Value Decomposition\n",
    "    u, s, vt = svds(I, dim)\n",
    "    s_diag_matrix = np.diag(s)\n",
    "    # make the prediction\n",
    "    svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "    error_train.append(rmse(svdPredictions, I))\n",
    "    error_test.append(rmse(svdPredictions, I2))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(dims, error_train, '--*b', label=\"train\")\n",
    "plt.semilogx(dims, error_test, '--*g', label=\"test\")\n",
    "plt.xlabel(\"Numer of latent dimensions\")\n",
    "plt.ylabel(\"Error\")\n",
    "plt.legend(loc=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note that after few dimensions (~10) the model starts to overfit (the error in test increases). Thus, we need to use other method to regularize the model (i.e., set restrictions/limitations to the model, so that it cannot overfit)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Alternating Least Squares (ALS)\n",
    "\n",
    "SVD can be very slow and computationally expensive. Besides, when addressing only the relatively few known entries we are highly prone to overfitting.\n",
    "\n",
    "An scalable alternative to SVD is ALS, which can include regularization terms to prevent overfitting. We will rename our variable to make them more similar to the ALS notation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS algorithm\n",
    "\n",
    "The ALS algorithm aims to estimate two unknown matrices which, when multiplied together, yield the rating matrix. The loss function you will use is the well-known sum of squared errors. The second term is for regularisation to prevent overfitting\n",
    "\n",
    "<img src=\"https://latex.codecogs.com/gif.latex?\\underset{Q*&space;,&space;P*}{min}\\sum_{(u,i)\\epsilon&space;K&space;}(r_{ui}-P_u^TQ_i)^2&plus;\\lambda(\\left&space;\\|&space;Q_i&space;\\right&space;\\|^2&space;&plus;&space;\\left&space;\\|&space;P_u&space;\\right&space;\\|^2)$&space;&space;$\" title=\"\\underset{q* , p*}{min}\\sum_{(u,i)\\epsilon K }(r_{ui}-q_i^Tp_u)^2+\\lambda(\\left \\| q_i \\right \\|^2 + \\left \\| p_u \\right \\|^2)\" />\n",
    "\n",
    "The Alternating Least Squares algorithm does this by first randomly filling the users matrix with values and then optimizing the value of the movies such that the error is minimized.  Then, it holds the movies matrix constant and optimizes the value of the user's matrix.  This alternation between which matrix to optimize is the reason for the \"alternating\" in the name. \n",
    "\n",
    "<img alt=\"factorization\" src=\"http://spark-mooc.github.io/web-assets/images/matrix_factorization.png\" style=\"width: 885px\"/>\n",
    "<br clear=\"all\"/>\n",
    "\n",
    "This optimization is what's being shown on the right in the image above.  Given a fixed set of user factors (i.e., values in the users matrix), we use the known ratings to find the best values for the movie factors using the optimization written at the bottom of the figure.  Then we \"alternate\" and pick the best user factors given fixed movie factors.\n",
    "\n",
    "It must be noticed that this is another way of reducing the dimensionality of the input matrix (like PCA, or more generally, SVD). This has important consequences:\n",
    "\n",
    "* ### Our decomposition is linear. We won't be able to catch non-linear relationships among users and items.\n",
    "* ### As in PCA or SVD, our features will correspond to directions of maximum variance in the data. Thus, the first feature will catch most of this variation, the second, a little bit more, and so on. It implies that the error in the reconstruction will not decrease dramatically when using more features!!! Keep this in mind.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def alsRmse(I,R,P,Q):\n",
    "    return np.sqrt(np.sum((I * (R - np.dot(P,Q.T)))**2)/len(R[R > 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iteration(user, fixed_vecs, counts, num_factors, reg_param, num_solve, verbose=False):\n",
    "    num_fixed = fixed_vecs.shape[0]\n",
    "    YTY = fixed_vecs.T.dot(fixed_vecs)\n",
    "    eye = np.eye(num_fixed)\n",
    "    lambda_eye = reg_param * np.eye(num_factors)\n",
    "    solve_vecs = np.zeros((num_solve, num_factors))\n",
    "\n",
    "    t = time.time()\n",
    "    for i in range(num_solve):\n",
    "        if user:\n",
    "            counts_i = counts[i]\n",
    "        else:\n",
    "            counts_i = counts[:, i].T\n",
    "        CuI = np.diag(counts_i)\n",
    "        pu = counts_i.copy()\n",
    "        pu[np.where(pu != 0)] = 1.0\n",
    "        YTCuIY = fixed_vecs.T.dot(CuI).dot(fixed_vecs)\n",
    "        YTCupu = fixed_vecs.T.dot(CuI + eye).dot(pu.T)\n",
    "        xu = np.linalg.solve(YTY + YTCuIY + lambda_eye, YTCupu)\n",
    "        solve_vecs[i] = xu\n",
    "        if verbose and i % 300 == 0:\n",
    "            print('Solved %i vecs in %d seconds' % (i, time.time() - t))\n",
    "            t = time.time()\n",
    "\n",
    "    return solve_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check performance by plotting train and test errors\n",
    "def check_als_performance(n_epochs, train_errors, test_errors):\n",
    "    plt.plot(range(n_epochs), train_errors, marker='o', label='Training Data');\n",
    "    plt.plot(range(n_epochs), test_errors, marker='v', label='Test Data');\n",
    "    plt.title('ALS-WR Learning Curve')\n",
    "    plt.xlabel('Number of Epochs');\n",
    "    plt.ylabel('RMSE');\n",
    "    plt.ylim(1, 5)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iterations = 20\n",
    "num_factors = 50\n",
    "lmbda = 0.1\n",
    "T = uMatrixTesting.copy()\n",
    "ptest = uMatrixTesting.copy()\n",
    "ptest[ptest > 0] = 1\n",
    "ptest[ptest == 0] = 0\n",
    "p = uMatrixTraining.copy()\n",
    "p[p > 0] = 1\n",
    "p[p == 0] = 0\n",
    "R = uMatrixTraining.copy()\n",
    "alpha = train.shape[0] / (n_users * n_items)\n",
    "C = alpha * R\n",
    "user_vectors = np.random.normal(size=(n_users, num_factors))\n",
    "item_vectors = np.random.normal(size=(n_items, num_factors))\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "for i in range(num_iterations):\n",
    "    print('Solving for user vectors...')\n",
    "    user_vectors = iteration(True, item_vectors, C, num_factors, lmbda, n_users)\n",
    "    print('Solving for item vectors...')\n",
    "    item_vectors = iteration(False, user_vectors, C, num_factors, lmbda, n_items)\n",
    "    print('iteration %i finished' % (i + 1))\n",
    "    \n",
    "    train_rmse = alsRmse(p,uMatrixTraining, user_vectors,item_vectors)\n",
    "    test_rmse = alsRmse(ptest,uMatrixTesting, user_vectors,item_vectors)\n",
    "    train_errors.append(train_rmse)\n",
    "    test_errors.append(test_rmse)\n",
    "    print(\"[Epoch %d/%d] train error: %f, test error: %f\" \n",
    "        %(i+1, num_iterations, train_rmse, test_rmse))\n",
    "    \n",
    "print(\"Algorithm converged\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_als_performance(num_iterations, train_errors, test_errors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS evaluation\n",
    "* RMSE\n",
    "* recall@[5, 10, 20, 50, 100, 200, 500]\n",
    "* map@[5, 10, 20, 50, 100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alsPredictions = np.dot(user_vectors, item_vectors.T)\n",
    "svdPredictions = np.dot(np.dot(u, s_diag_matrix), vt)\n",
    "print('ALS CF RMSE: ' + str(rmse(alsPredictions, uMatrixTesting)))\n",
    "print('SVD CF RMSE: ' + str(rmse(svdPredictions, uMatrixTesting)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evalutation\n",
    "alsPredictions_sorted = np.argsort(alsPredictions)[::-1]\n",
    "predicted_ids = trainUsersGrouped.user_id.apply(lambda i: alsPredictions_sorted[i-1]).values\n",
    "zipped = list(zip(test_ids, train_ids, predicted_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [10, 30, 50, 100]:\n",
    "    recall = np.mean([recall_at_n(k,test, recom, train)  for (test, train, recom) in zipped])\n",
    "    print(\"recall@%s=%.3f\" %(k, recall))\n",
    "    map_ = np.mean([apk(k,test, recom, train)  for (test, train, recom) in zipped])\n",
    "    print(\"map@%s=%.3f\" %(k, map_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at http://infolab.stanford.edu/~ullman/mmds/ch9.pdf"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
