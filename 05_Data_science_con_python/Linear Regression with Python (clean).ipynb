{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing a Linear Regression problem with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to implement our own linear regression example. \n",
    "\n",
    "In linear regression, our hypothesis function $h_\\theta$ is:\n",
    "\n",
    "$$h_\\theta(x) = \\theta_0 + \\theta_1x$$\n",
    "\n",
    "And, as we are doing regression, our cost function is: \n",
    "\n",
    "$$J(\\theta_0,\\theta_1) = \\frac{1}{m}\\sum_{i=1}^m(\\hat{y}_i-y_i)^2 = \\frac{1}{m}\\sum_{i=1}^m(h_\\theta(x_i)-y_i)^2 $$\n",
    "\n",
    "Nota that, the cost funtion is just the sum of all the square errors from our hypothesis ($\\hat{y}_i$) versus the data ($y_i$).\n",
    "\n",
    "The best parameters for our hypothesis will give us the **minimum cost function**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding a minimum for J"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a minimum of a function is equivalent to finding the parameters that make the gradient of that function to vanish. In other words:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = 0$$\n",
    "\n",
    "We will implement two ways of solving this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Gradient descent (Numerical method)\n",
    "\n",
    "\n",
    "From a starting point ($\\theta$), we will try to move to a new point $\\theta '$, decreasing the cost funtion ($J(\\theta)$). We will do this many times, up to we find a minimum (or close enough to it).\n",
    "\n",
    "#### Partial differentials of the cost function (using chain rule)\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial\\theta_0} = \\frac{2}{m}\\sum_{i=1}^m(h_\\theta(x_i)-y_i)$$\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial\\theta_1} = \\frac{2}{m}\\sum_{i=1}^m(h_\\theta(x_i)-y_i) \\cdot x_i$$\n",
    "\n",
    "Finally, we need to update iteratively the values for $\\theta_0$ and $\\theta_1$. Using Gradient Descent algorithm  with learning rate ($\\alpha$) until convergence criterion ($\\epsilon$) is achieved:\n",
    "\n",
    "         while (convergence==False):\n",
    "$$\\theta_0' = \\theta_0 - \\alpha \\frac{\\partial J}{\\partial\\theta_0} $$\n",
    "$$\\theta_1' = \\theta_1 - \\alpha \\frac{\\partial J}{\\partial\\theta_1} $$\n",
    "$$J' = J(\\theta_0',\\theta_1')$$\n",
    "$$\\Delta J = abs(J'-J)$$\n",
    "$$ convergence = (\\Delta J < \\epsilon)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Normal equations (Algebra)\n",
    "\n",
    "In matrix notation, we can implement our hypothesis as:\n",
    "\n",
    "$$h_\\theta (x^{(i)})=(x^{(i)})^T \\theta$$\n",
    "\n",
    "Note that, in this case, if we want to consider our hypothesis such $h(\\theta) = \\theta_0 + \\theta_i x^{(i)}$ where $x$ is a vector, for consistency, we need to introduce an additional \"constant feature\" in our data. In other words, we need to map our input data as follows:\n",
    "\n",
    "$$x_i \\rightarrow [1,x_i]$$\n",
    "\n",
    "we can express gradient of J as follows:\n",
    "\n",
    "$$\\nabla_{\\theta} J(\\theta) = X^T X \\theta - X^T \\vec{y}$$\n",
    "\n",
    "To minimize J, we set its derivatives to zero, therefore obtaining the **normal equations**:\n",
    "\n",
    "$$ X^T X \\theta = X^T \\vec{y}$$\n",
    "\n",
    "We can solve this equation for theta.\n",
    "\n",
    "As a final remark, we can extend this method to non linear hypothesis by extending our input data $x$ to the features we need. For example, for a parabolic fit:\n",
    "\n",
    "$$x_i \\rightarrow [1,x_i,x_i^2]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "Giving the data provided below (x->y), find the best equation fit, using:\n",
    "\n",
    "* Gradient Descent\n",
    "* Normal Equations\n",
    "\n",
    "Using your own python implementation, using numpy and scipy tools (**not scipy!**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "theta_0 = 2\n",
    "theta_1 = 5\n",
    "\n",
    "\n",
    "X = (np.random.randn(100) + 1) * 50\n",
    "#X=np.linspace(-50,150,100)\n",
    "jitter = 50 * np.random.randn(100)\n",
    "y = theta_0 + theta_1 * X + jitter\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "#ax.plot(X,5*X,'g-')\n",
    "ax.scatter(X, y)\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.array([1.0,4.97]) #theta=[theta_0,theta_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = theta_0 + theta_1 * X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xp = [col(1);X]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xp_i = [1,x_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#INDICIAL\n",
    "y_i_hat = theta_0 + theta_1*x_i = xp_i*[theta_0,theta_1]T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VECTORIAL\n",
    "y_hat = Xp*theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_evaluacion(x):\n",
    "    h_theta = theta_0 + theta_1 * x\n",
    "    return h_theta\n",
    "\n",
    "def f_coste_theta_0(x,y):\n",
    "    coste_theta_0 = (2 / theta_0) * ((sum(f_evaluacion(x)) - y) ** 2)\n",
    "    return coste_theta_0\n",
    "\n",
    "def f_coste_theta_1(x,y):\n",
    "    coste_theta_1 = (2 / theta_0) * ((sum(f_evaluacion(x)) - y) ** 2) * (f_evaluacion(x))\n",
    "    return coste_theta_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DESDE CERO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#punto de partida X e y (que son cada uno un array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lo primero es obtener la funcion de coste, para ello:\n",
    "#hipotesis: h_theta = theta_0 + theta_1 * x\n",
    "theta_0 = -1\n",
    "theta_1 = 1\n",
    "h = X*theta_1 + theta_0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Ahora le restamos a la h la y obteniendo el error:\n",
    "error = h - y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ahora lo elevamos al cuadrado para que no se nos anulen los errores\n",
    "error2 = error ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ahora calculamos la suma, q es el coste:\n",
    "cost = error2.sum()/len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X,y):\n",
    "    #return_function = error2.sum()/len(X)\n",
    "    #return_function = (error**2).sum()/len(X)\n",
    "    #return_function = ((h-y)**2).sum()/len(X)\n",
    "    #return_function = (((X*theta_1 + theta_0)-y)**2).sum()/len(X)\n",
    "    return_function = lambda theta_0,theta_1: (((X*theta_1 + theta_0)-y)**2).sum()/len(X)\n",
    "    return return_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_theta_0 = [-1,10,1000]\n",
    "l_theta_1 = [-2,5,10]\n",
    "\n",
    "\n",
    "#for theta_0 in l_theta_0:\n",
    "#    for theta_1 in theta_1:\n",
    "#            J = cost_function(X,y)\n",
    "#    print(J(theta_0, theta_1))\n",
    "        \n",
    "# o tambn:\n",
    "for theta_0,theta_1 in zip(l_theta_0,l_theta_1):\n",
    "    J = cost_function(X,y)\n",
    "    print(J(theta_0, theta_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_theta_0(X,y):\n",
    "    return_function = lambda theta_0,theta_1: ((2*(X*theta_1 + theta_0-y))).sum()/len(X)\n",
    "    return return_function\n",
    "\n",
    "def grad_theta_1(X,y):\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
