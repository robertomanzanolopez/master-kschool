{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Machine Learning with Python\n",
    "\n",
    "\n",
    "<img src=\"https://www.python.org/static/img/python-logo.png\" alt=\"yogen\" style=\"width: 200px; float: right;\"/>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"../assets/yogen-logo.png\" alt=\"yogen\" style=\"width: 200px; float: right;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives\n",
    "\n",
    "* Learn what is unsupervised machine learning\n",
    "\n",
    "* Group data with clustering\n",
    "\n",
    "* Find underlying linear patterns with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![scikit-learn cheat sheet](http://amueller.github.io/sklearn_tutorial/cheat_sheet.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Clustering\n",
    "\n",
    "A series of techniques for finding clusters within datasets.\n",
    "\n",
    "Clusters are groups of points that are closer to each other than to the rest.\n",
    "\n",
    "<img src=\"figs/clustering.png\" alt=\"Clustering\" style=\"height: 600px; float: left;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nxRMgPJgRkVa"
   },
   "source": [
    "# Generate random data\n",
    "\n",
    "We're going to generate random data normally distributed around three centers, with noise. Each cluster will have 200 points. We concatenate all three groups in a single dataframe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wm8WaN88mLoz"
   },
   "source": [
    "Please, note that the data we have created does not have a class. It is just a set of points. However, we DO know that they come from different distribution and our objective is to find out them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## K-means clustering\n",
    "\n",
    "Very simple algorithm, quite fast:\n",
    "\n",
    "- Throw K candidate cluster centers (_centroids_) randomly at the data.\n",
    "\n",
    "- Assign points to the closest centroid.\n",
    "\n",
    "- Update the centroid as the average of its observations.\n",
    "\n",
    "- Repeat 2,3 until convergence.\n",
    "\n",
    "\n",
    "\n",
    "```\n",
    "clustering (data, K):\n",
    "    Randomly initialize K cluster centroids (mu(1),..., mu(k))\n",
    "    # or select K random points from data\n",
    "    Repeat until convergence:\n",
    "        # assign cluster\n",
    "        for d in data:\n",
    "            assign d to closest cluster centroid\n",
    "        # recompute cluster centroid\n",
    "        for k = 1 to K:\n",
    "            mu(k) = mean of data points assigned to cluster k\n",
    "            \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "For 3 random starting points, calculate which is the closest for each of our points. \n",
    "\n",
    "Hint: check the `scipy.spatial.distance` module. `np.argmin` might also be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8c2TLG08fdPQ"
   },
   "source": [
    "Now we are going to compare the original 'cluster' where each point comes from, with the asigned cluster. In order to do that, we just create a vector with the original class (color) and use that to plot. \n",
    "\n",
    "Please, recall that this is something we can do here because we're creating a synthetic dataset, but normally we won't be able to do it, since we don't know how the data has been generated.\n",
    "\n",
    "Notice that class labels (kmeans) may not agree with original class number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's take some time to play with different number of (original) distributions and clusters and see the effect when number of clusters does not match true data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Practical: K-Means Clustering with sklearn\n",
    "\n",
    "Download `players_20.csv` from [here](https://www.kaggle.com/stefanoleone992/fifa-20-complete-player-dataset?select=players_20.csv)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The elbow method\n",
    "\n",
    "To choose a number of clusters in KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hierarchical clustering\n",
    "\n",
    "In hierarchichal clustering there are groups within groups. We can either subdivide the observations (_divisive clustering_) or we join those that are similar to each other (_agglomerative clustering_).\n",
    "\n",
    "We can track the order in which we join them up and represent it as a _dendrogram_.\n",
    "\n",
    "We don't need to specify the number of clusters beforehand. \n",
    "\n",
    "The distance between two observations is the _height_ of the branching point that separates them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://i0.wp.com/datascienceplus.com/wp-content/uploads/2016/01/hclust.png\" alt=\"Dendrogram\" style=\"height: 600px; float: left;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distance measures in clustering\n",
    "\n",
    "In any of these approaches, we need a measure of distance or similarity between points. \n",
    "\n",
    "In hierarchical clustering, we additionally need a measure of similarity between single points and groups of points.\n",
    "\n",
    "How will this measures be influenced by the scale of our variables?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Clustering in scikit-learn\n",
    "\n",
    "![Clustering algorithms in scikit-learn](https://scikit-learn.org/stable/_images/sphx_glr_plot_cluster_comparison_001.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "There's also a hierarchical version.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring quality of clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method\n",
    "\n",
    "We've already seen it. Not very theoretically solid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silhouette\n",
    "\n",
    "The Silhouette Coefficient for a sample is a function of the mean intra-cluster distance (a) and the mean nearest cluster distance (b), and it is defined as:\n",
    " \n",
    "$$ s(i) = \\frac{b(i) - a(i)}{max\\{a(i), b(i)\\}}$$\n",
    "\n",
    "It therefore varies between -1 for a sample that is closer to members of a different cluster than to its own and 1 for one that is a lot closer to members of its cluster.\n",
    "\n",
    "It has a great advantage over the elbow method: it can either go up or down as we increase the number of clusters.\n",
    "\n",
    "![2 clusters](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_silhouette_analysis_001.png)\n",
    "\n",
    "![3 clusters](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_silhouette_analysis_002.png)\n",
    "\n",
    "![4 clusters](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_silhouette_analysis_003.png)\n",
    "\n",
    "![5 clusters](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_silhouette_analysis_004.png)\n",
    "\n",
    "![6 clusters](https://scikit-learn.org/stable/_images/sphx_glr_plot_kmeans_silhouette_analysis_005.png)\n",
    "\n",
    "\n",
    "from https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Principal Component Analysis (PCA)\n",
    "\n",
    "A dimensionality reduction technique. It uses Singular Value Decomposition (SVD) of the data matrix to generate Principal Components: unit vectors that secuentially point in the direction that best fits the data, while being orthogonal to the previous ones.\n",
    "\n",
    "They are ordered by the amount of variance they explain.\n",
    "\n",
    "Those vectors can then be used to do a change of base. If we take fewer than the total, we will be doing _dimensionality reduction_; a proyection onto the subspace of the given dimension that conserves the most variance.\n",
    "\n",
    "![PCA as base change](https://intoli.com/blog/pca-and-svd/img/basic-pca.png)\n",
    "\n",
    "In three dimensions, but keeping only 2 components:\n",
    "\n",
    "![PCA in 3D](figs/pca_3D.png)\n",
    "\n",
    "from [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Now let's apply PCA to our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional References\n",
    "\n",
    "\n",
    "[An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "[The Elements of Statistical Learning](https://web.stanford.edu/~hastie/ElemStatLearn/)\n",
    "\n",
    "[Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do)\n",
    "\n",
    "[scikit-learn cheat sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "\n",
    "[A comparison of classifiers available in scikit-learn](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "[An amazing explanation of the kernel trick](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)\n",
    "\n",
    "[Clustering in scikit-learn](http://scikit-learn.org/stable/modules/clustering.html)\n",
    "\n",
    "[Ensemble methods in scikit-learn](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "\n",
    "[An example of customer segmentation](https://www.kaggle.com/fabiendaniel/customer-segmentation)\n",
    "\n",
    "[Hands-on ML](https://github.com/ageron/handson-ml2)\n",
    "\n",
    "[Silhouette analysis](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html)\n",
    "\n",
    "[PCA explained visually](https://setosa.io/ev/principal-component-analysis/)\n",
    "\n",
    "[A step by step explanation of PCA](https://builtin.com/data-science/step-step-explanation-principal-component-analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:master2022]",
   "language": "python",
   "name": "conda-env-master2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
