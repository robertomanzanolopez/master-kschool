{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Machine Learning with Python II\n",
    "\n",
    "\n",
    "<img src=\"https://www.python.org/static/img/python-logo.png\" alt=\"yogen\" style=\"width: 200px; float: right;\"/>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"../assets/yogen-logo.png\" alt=\"yogen\" style=\"width: 200px; float: right;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objectives\n",
    "\n",
    "* Learn the basic principles behind classification\n",
    "\n",
    "* Get to know some of the most commonly used algorithms for classification.\n",
    "\n",
    "\n",
    "* Learn the importance of metrics in classification, especially with unbalanced classes \n",
    "\n",
    "* Combine several weak learners to create a single strong one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction to Classification \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In the simplest case we want to predict pertenence to a class given some input variables. \n",
    "\n",
    "We can codify pertenence as 1, non-pertenence as 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"http://openclassroom.stanford.edu/MainFolder/courses/MachineLearning/exercises/ex8materials/ex8b_10.png\" alt=\"Classification\" style=\"width: 600px; float: left;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That means that we want to fit a function $p$ that, for a given value of X, produces a _probability_ of belonging to the class.\n",
    "\n",
    "$$p(X) = P(Y = 1 \\mid X)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What about linear regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Logistic Regression\n",
    "\n",
    "$$p(X) = \\frac{e^{\\beta_0 + \\beta_1 \\cdot X}}{1 + e^{\\beta_0 + \\beta_1 \\cdot X}}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Logistic Function](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/1200px-Logistic-curve.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics in classification\n",
    "\n",
    "A first approximation could be the % of examples that we got right. This is called _accuracy_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we had very few positive examples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Confusion Matrix\n",
    "\n",
    "![A confusion matrix](https://static.packt-cdn.com/products/9781838555078/graphics/C13314_06_05.jpg)\n",
    "\n",
    "from https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781838555078/6/ch06lvl1sec34/confusion-matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision and recall\n",
    "\n",
    "![Precision and recall](https://upload.wikimedia.org/wikipedia/commons/2/26/Precisionrecall.svg)\n",
    "\n",
    "Probably best to understand them as conditional probabilities:\n",
    "\n",
    "Precision: What is the probability that an example is actually positive, given I've predicted it to be positive?\n",
    "\n",
    "Recall: What is the probability of me calling an example positive, given it is actually positive?\n",
    "\n",
    "from https://en.wikipedia.org/wiki/Precision_and_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## F1 measure\n",
    "\n",
    "A good default choice because it combines both precission and recall:\n",
    "\n",
    "$$ F_1 = 2 \\cdot \\frac{precision \\cdot recall}{precision + recall}$$\n",
    "\n",
    "\n",
    "### $F_\\beta$\n",
    "\n",
    "F beta is a generalization of F1 that uses a (positive) weighting $\\beta$ so that recall is considered $\\beta$ times more important than precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision boundaries\n",
    "\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "x_min, x_max = df_pd['x'].min()-0.1, df_pd['x'].max()+0.1\n",
    "y_min, y_max = df_pd['y'].min()-0.1, df_pd['y'].max()+0.1  \n",
    "\n",
    "def plot_decision_boundaries(x, y, labels, model, \n",
    "                             x_min=x_min, \n",
    "                             x_max=x_max, \n",
    "                             y_min=y_min, \n",
    "                             y_max=y_max, \n",
    "                             grid_step=0.02):\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, grid_step),\n",
    "                         np.arange(y_min, y_max, grid_step))\n",
    "    \n",
    "    Z = model.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n",
    "\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    arr = plt.cm.coolwarm(np.arange(plt.cm.coolwarm.N))\n",
    "    arr_hsv = mpl.colors.rgb_to_hsv(arr[:,0:3])\n",
    "    arr_hsv[:,2] = arr_hsv[:,2] * 1.5\n",
    "    arr_hsv[:,1] = arr_hsv[:,1] * .5\n",
    "    arr_hsv = np.clip(arr_hsv, 0, 1)\n",
    "    arr[:,0:3] = mpl.colors.hsv_to_rgb(arr_hsv) \n",
    "    my_cmap = ListedColormap(arr)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(7,7))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=my_cmap)\n",
    "\n",
    "    ax.scatter(x, y, c=labels, cmap='coolwarm')\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.grid(False)\n",
    "    return ax\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Decision Trees\n",
    "\n",
    "[A visual explanation of Decision Trees](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Learn a series of if-else questions leading to a decision.\n",
    "\n",
    "Can be used both for regression and classification.\n",
    "\n",
    "In the classification setting, each node corresponds to a prediction with a certain confidence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "data to program versus program ![Titanic Decision Tree](https://upload.wikimedia.org/wikipedia/commons/f/f3/CART_tree_titanic_survivors.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Decision Tree](https://image.slidesharecdn.com/lecture02ml4ltmarinasantini2013-130827052029-phpapp02/95/lecture-02-machine-learning-for-language-technology-decision-trees-and-nearest-neighbors-10-638.jpg?cb=1378716784)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What is the meaning, then, of building the best tree we can?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Measures of quality\n",
    "\n",
    "For a decision tree that separates $K$ classes, we can measure its quality by:\n",
    "\n",
    "The Gini index\n",
    "$$G = \\sum_{k=1}^K \\hat{p}_{mk} (1 - \\hat{p}_{mk})$$\n",
    "\n",
    "Cross-entropy\n",
    "\n",
    "$$D = - \\sum_{k=1}^K \\hat{p}_{mk} \\log \\hat{p}_{mk}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![Splitting the feature space with a tree](https://upload.wikimedia.org/wikipedia/commons/8/87/Recursive_Splitting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Advantages and disadvantages of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Pros:\n",
    "\n",
    "- Easily interpretable\n",
    "\n",
    "Cons:\n",
    "\n",
    "- Not robust"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical: Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Reading the data in and preparing for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "https://www.kaggle.com/c/GiveMeSomeCredit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise: \n",
    "What is the overall probability of default?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dealing with missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### We are now ready to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To visualize trees, we can use `graphviz`\n",
    "\n",
    "```python\n",
    "import graphviz \n",
    "dot_data = tree.export_graphviz(classifier_tree, out_file=None) \n",
    "graph = graphviz.Source(dot_data)\n",
    "graph\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Exercise\n",
    "\n",
    "Show features in the decision tree, by order of importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "mmm it's something. Perhaps it has overfit? Let's check how it works on the training set to compare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Analogous in a way to K-fold cross-validation.\n",
    "\n",
    "Based on the bootstrap technique.\n",
    "\n",
    "We train B trees from a single training set by _bootstrapping_: constructing B subsets of the training set by sampling with replacement. The prediction for each point in the test set will be given by a vote or average of the B trees.\n",
    "\n",
    "What effect do you think this will have in terms of bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Out Of Bag (OOB) error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In order to estimate our error, we don't even need to do K-fold cross validation of to save a validation set: in effect, we have already done it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Every observation will have been used for a given tree only with about 1/3 probability. \n",
    "\n",
    "Those are the OOB observations, and we can use the prediction of those $B/3$ trees that did not use observation $i$ as an error measure for those trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Variable importance measures\n",
    "\n",
    "We can use the average amount by which the Gini index decreases when introducing a split on a particular variable as a variable importance measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical: Bagging\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.BaggingClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting is yet another tweak, and it is widely applicable. It manages to generate strong learners from weak ones.\n",
    "\n",
    "It consists of training models _sequentially_: we train a model on the residuals of the previous model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Random Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can further decorrelate the components of our ensemble model by decorrelating the trees more.\n",
    "\n",
    "In Random Forests, we do this by considering only a random subset m of the predictors (usually $m \\approx \\sqrt{p}$ for each split that we consider."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical: Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Hyperplanes and separating observations\n",
    "\n",
    "A hyperplane in a p-dimensional space is a _flat affine subspace of dimension p-1_.\n",
    "\n",
    "In 2D: a line. In 3D: a plane.\n",
    "\n",
    "It will have an equation of the form:\n",
    "\n",
    "$$\\beta_0 + \\beta_1 X_1+ \\dots + \\beta_p X_p = 0 $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use a hyperplane to separate observations belonging to different classes\n",
    "\n",
    "![2D hyperplanes](https://cdn-images-1.medium.com/max/1658/1*UGsHP6GeQmLBeteRz80OPw.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The points that, when fed into the plane equation, result in a number greater than 0 will be on one side of the hyperplane and those that have a number lower than 0 will be on the other side."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What problems do these hyperplanes (lines, in this example) have?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Maximal Margin Classifier\n",
    "\n",
    "The hyperplane that has the largest minimum distance to the training observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"https://houxianxu.github.io/images/SVM/2.png\" alt=\"Maximal margin classifier\" style=\"width: 600px; float: left;\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this image, we can see clearly where the term _support vectors_ comes from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the maximal margin classifier depends exclusively on a small number of observations.\n",
    "\n",
    "What does that mean in terms of bias and variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The non separable case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "It's the maximal margin classifier, adapted to have _soft_ margins.\n",
    "\n",
    "We will allow violations to the margin, or even to the class boundary. \n",
    "\n",
    "We call them $\\epsilon_i$ and express them in terms of margin length: if  $\\epsilon_i > 0$ then an observation is on the wrong side _of the margin_. If $\\epsilon_i > 1$, observation i is on the wrong side _of the hyperplane_. \n",
    "\n",
    "We will allow only a sum of violations \n",
    "\n",
    "$$C \\ge \\sum_{i=1}^n\\epsilon_i$$\n",
    "\n",
    "We can think of $C$ as a \"budget\" for margin violations \n",
    "\n",
    "The solution classifier is the hyperplane that maximizes the margin, provided $C$ is not overshot.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Classifier\n",
    "\n",
    "#### The role of $C$\n",
    "\n",
    "$C$ controls for us the bias-variance trade-off. Think of what happens when $C = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Practical: SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Non linear decision boundaries\n",
    "\n",
    "Sometimes the classes are not linearly separable\n",
    "\n",
    "\n",
    "<img src=\"http://cfss.uchicago.edu/persp009_svm_files/figure-html/svm-radial-1.png\" alt=\"Non-linearly-separable classes\" style=\"width: 600px; float: left;\"/>\n",
    "\n",
    "\n",
    "Do we have to give up the support vector classifier here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The kernel trick\n",
    "\n",
    "Turns out, we don't. We can transform our dataset so that we learn a separating hyperplane in a higher dimensional space. \n",
    "\n",
    "![Kernel trick](http://www.eric-kim.net/eric-kim-net/posts/1/imgs/data_2d_to_3d.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can think of kernel as the function that transforms the left space into the right one.\n",
    "\n",
    "We can now learn a hyperplane on the right, transformed, space and project it back on the original one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![Hyperplane with kernel trick](http://www.eric-kim.net/eric-kim-net/posts/1/imgs/data_2d_to_3d_hyperplane.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Classification with more than 2 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One versus one\n",
    "\n",
    "`sklearn.multiclass.OneVsOneClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### One versus all\n",
    "\n",
    "This is the most commonly used strategy for multiclass classification and is a fair default choice.\n",
    "\n",
    "`sklearn.multiclass.OneVsRestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Additional References\n",
    "\n",
    "\n",
    "[An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/)\n",
    "\n",
    "[Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do)\n",
    "\n",
    "[scikit-learn cheat sheet](https://s3.amazonaws.com/assets.datacamp.com/blog_assets/Scikit_Learn_Cheat_Sheet_Python.pdf)\n",
    "\n",
    "\n",
    "[A comparison of classifiers available in scikit-learn](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "[An amazing explanation of the kernel trick](http://www.eric-kim.net/eric-kim-net/posts/1/kernel_trick.html)\n",
    "\n",
    "[Ensemble methods in scikit-learn](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "\n",
    "[Writing custom metric functions in sklearn](https://scikit-learn-laboratory.readthedocs.io/en/latest/custom_metrics.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:master2022]",
   "language": "python",
   "name": "conda-env-master2022-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {
    "height": "533px",
    "width": "284px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
