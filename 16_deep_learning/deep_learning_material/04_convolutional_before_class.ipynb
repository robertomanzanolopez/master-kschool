{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tgelvf1_mXmF"
   },
   "source": [
    "# Deep Learning for Computer Vision using Convolutional Neural Networks\n",
    "\n",
    "With dense layers applied to images, we have learnt global patterns that can be exploited to make predictions. The main difference with **convolutional** layers, is that we will now learn local patterns. \n",
    "\n",
    "These local patterns have two important features:\n",
    "* They are translation invariant. It does not matter where in the image we see the pattern, the layer will be able to capture it and exploit it. In contrast, for a dense layer, if the same local pattern appears in two different location in the image, it would interpret them as two different patterns.\n",
    "* Convolutional layers can lear a spatial hierarchy of patterns. Imagine the problem of recognizing a *face*. A first layer would learn something about the *nose*, some other layers about the *eyes*, and so on. The aggregation \n",
    "\n",
    "![](./imgs/09_cnn_hierarchy.png)\n",
    "\n",
    "The input to a convolutional layer is a 3-D tensor: height, width and depth (the channels in the image). For RGB images, depth will be 3, for the *R*ed, *B*lue and *G*reen colors. For a black and white image, depth will be just 1.\n",
    "\n",
    "Each layer recognizes a patch (subset) of the image, with a specific pattern. When applied to the original input, the layer will filter the rest of the image, highlighting the pattern that has learnt. That is, the layer becomes a **features map**.\n",
    "\n",
    "## Convolution operation\n",
    "\n",
    "For a convolutional layer, we need to decide the size of the patches (commonly, 3x3), and the depth of the output of the feature map (it is no longer 3, and in fact, it will be a number larger than that -- 16, 32, 64). The output will be another tensor, that is the input to the next layers. These tensors will no longer be images; that is, 3D tensors with a depth of 3, etc. To transform the output into a spatial tensor, we can use padding (adding additional rows or columns).\n",
    "\n",
    "![](./imgs/10_convolution.png)\n",
    "\n",
    "### Padding\n",
    "\n",
    "The convolution operation will slide through the image, trying to cover different zones, to extract common patterns found in different locations. In the edges of the image, the layer will not be able to extract patches, because the regions will be smaller than the patch size. With padding, we make it possible for the layer to extract patches even in the edges of the image, thus using that part of the image too to try to identify a common pattern.\n",
    "\n",
    "###  Stride\n",
    "\n",
    "Another parameter that we must take into account is *striding*. The patches can overlap with other patches. The distance between two windows used to extract patches is called **stride**. For instance, with a patch size of 3x3, and a stride of 3, patches will not overlap. We will normally will try to avoid overlapping in the windows extracting the patches; unless that during the training and validation process, we need to change the parameters to obtain a better model.\n",
    "\n",
    "### Pooling\n",
    "\n",
    "In a network, we need to partially reduce the dimension of the data, layer after layer, so we can learn at the output layer a number, a vector of a certain size, etc. The convolution operation is in fact increasing the size of the output. How do we do reduce the size of the output learnt in each layer? By **pooling**.\n",
    "\n",
    "The most typical pooling operation is *max pooling*. For each patch learnt in the layer, we apply a window of 2x2 or less (smaller than the striding window), and then apply the max operation. For each 2x2 possible window, we keep the max in that window. This way, we are reducing the size of the patches, and the size of the output of the layer. By doing this reduction in the size of the output, we will also help the network to build a hierarchy of patterns.\n",
    "\n",
    "Other pooling operations are also possible: averaging, in different ways. For full details of what pooling operations are available in Keras see https://keras.io/layers/pooling/\n",
    "\n",
    "## Additional readings/videos\n",
    "\n",
    "* How CNNs work: https://www.youtube.com/watch?v=FmpDIaiMIeA\n",
    "* Deep Learning demystified: http://brohrer.github.io/deep_learning_demystified.html\n",
    "* Hot-dog? No hot-dog? http://mateos.io/blog/getting-some-hotdogs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NtjRK1XzmXmH"
   },
   "source": [
    "# Common functions and download data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 532,
     "status": "ok",
     "timestamp": 1531944075717,
     "user": {
      "displayName": "Israel Herraiz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "114232750363927198683"
     },
     "user_tz": -120
    },
    "id": "lHOFBgd7mgmQ",
    "outputId": "504fffe2-f534-4ef5-e2c7-0f8c3f84b505"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "plt.style.use('seaborn-talk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "4f4K4QzNmg5b"
   },
   "outputs": [],
   "source": [
    "def plot_metric(history, metric):\n",
    "    history_dict = history.history\n",
    "    values = history_dict[metric]\n",
    "    if 'val_' + metric in history_dict.keys():  \n",
    "        val_values = history_dict['val_' + metric]\n",
    "\n",
    "    epochs = range(1, len(values) + 1)\n",
    "\n",
    "    if 'val_' + metric in history_dict.keys():  \n",
    "        plt.plot(epochs, val_values, label='Validation')\n",
    "    plt.semilogy(epochs, values, label='Training')\n",
    "\n",
    "    if 'val_' + metric in history_dict.keys():  \n",
    "        plt.title('Training and validation %s' % metric)\n",
    "    else:\n",
    "        plt.title('Training %s' % metric)\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2598,
     "status": "ok",
     "timestamp": 1531944080327,
     "user": {
      "displayName": "Israel Herraiz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "114232750363927198683"
     },
     "user_tz": -120
    },
    "id": "GMyUGqnemkuJ",
    "outputId": "367b755e-6a2f-46ce-f7a3-fbf840661cd8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.1.6)\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.11.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.14.5)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (0.19.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Wm_hQY5dmm0s"
   },
   "outputs": [],
   "source": [
    "# Install the PyDrive wrapper & import libraries.\n",
    "# This only needs to be done once per notebook.\n",
    "!pip install -U -q PyDrive\n",
    "from pydrive.auth import GoogleAuth\n",
    "from pydrive.drive import GoogleDrive\n",
    "from google.colab import auth\n",
    "from oauth2client.client import GoogleCredentials\n",
    "\n",
    "# Authenticate and create the PyDrive client.\n",
    "# This only needs to be done once per notebook.\n",
    "auth.authenticate_user()\n",
    "gauth = GoogleAuth()\n",
    "gauth.credentials = GoogleCredentials.get_application_default()\n",
    "drive = GoogleDrive(gauth)\n",
    "\n",
    "# Download a file based on its file ID.\n",
    "#\n",
    "# A file ID looks like: laggVyWshwcyP6kEI-y_W3P8D26sz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1564,
     "status": "ok",
     "timestamp": 1531944083355,
     "user": {
      "displayName": "Israel Herraiz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "114232750363927198683"
     },
     "user_tz": -120
    },
    "id": "NEfuwzZumpb4",
    "outputId": "1292b094-e156-4231-f037-e56b4cecfb14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalab  dogs_cats  dogs_cats_small  test1.zip\ttrain.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "q82Bk-IKmq2Y"
   },
   "outputs": [],
   "source": [
    "file_id = '1nL7cgXGkNGS79FORsrCfrfcpzrBtoX8K'\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "downloaded.GetContentFile(\"train.zip\")\n",
    "\n",
    "file_id = '1edO-psKzj7gpYgf5PcDKyPgbFFJN09D3'\n",
    "downloaded = drive.CreateFile({'id': file_id})\n",
    "downloaded.GetContentFile(\"test1.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1557,
     "status": "ok",
     "timestamp": 1531937906361,
     "user": {
      "displayName": "Israel Herraiz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "114232750363927198683"
     },
     "user_tz": -120
    },
    "id": "4PPbk1lhnRsY",
    "outputId": "9ceb275b-8b13-4428-cb94-f393c6d5def7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalab  test1.zip  train.zip\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 15755,
     "status": "ok",
     "timestamp": 1531937931156,
     "user": {
      "displayName": "Israel Herraiz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "114232750363927198683"
     },
     "user_tz": -120
    },
    "id": "wwxDIeHenVhf",
    "outputId": "0a7e24bb-78c1-480a-9963-543436f3e02b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 1.1M\n",
      "drwxr-xr-x 2 root root 292K Sep 20  2013 test1\n",
      "drwxr-xr-x 2 root root 764K Sep 20  2013 train\n"
     ]
    }
   ],
   "source": [
    "!mkdir dogs_cats\n",
    "!cd dogs_cats && unzip -q ../train.zip\n",
    "!cd dogs_cats && unzip -q ../test1.zip\n",
    "!ls -hl dogs_cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "Qw8ybMwLnYHH"
   },
   "outputs": [],
   "source": [
    "original_dataset_dir=\"/content/dogs_cats/train/\"\n",
    "\n",
    "import os, shutil\n",
    "\n",
    "base_dir = \"/content/dogs_cats_small\"\n",
    "\n",
    "\n",
    "train_dir = os.path.join(base_dir, \"train\")\n",
    "validation_dir = os.path.join(base_dir, \"validation\")\n",
    "test_dir = os.path.join(base_dir, \"test\")\n",
    "\n",
    "\n",
    "train_cats_dir = os.path.join(train_dir, \"cats\")\n",
    "train_dogs_dir = os.path.join(train_dir, \"dogs\")\n",
    "\n",
    "validation_dogs_dir = os.path.join(validation_dir, \"dogs\")\n",
    "validation_cats_dir = os.path.join(validation_dir, \"cats\")\n",
    "\n",
    "test_dogs_dir = os.path.join(test_dir, \"dogs\")\n",
    "test_cats_dir = os.path.join(test_dir, \"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "FiSwnS0koij0"
   },
   "outputs": [],
   "source": [
    "!rm -rf dogs_cats_small/\n",
    "os.mkdir(base_dir)\n",
    "os.mkdir(train_dir)\n",
    "os.mkdir(validation_dir)\n",
    "os.mkdir(test_dir)\n",
    "os.mkdir(train_cats_dir)\n",
    "os.mkdir(train_dogs_dir)\n",
    "os.mkdir(validation_dogs_dir)\n",
    "os.mkdir(validation_cats_dir)\n",
    "os.mkdir(test_dogs_dir)\n",
    "os.mkdir(test_cats_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1481,
     "status": "ok",
     "timestamp": 1531938247717,
     "user": {
      "displayName": "Israel Herraiz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "114232750363927198683"
     },
     "user_tz": -120
    },
    "id": "lVVMrL-on_Y5",
    "outputId": "0494cc3c-8abc-43cf-89ed-4f03b14edbcc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs_cats_small\n",
      "dogs_cats_small/train\n",
      "dogs_cats_small/train/dogs\n",
      "dogs_cats_small/train/cats\n",
      "dogs_cats_small/validation\n",
      "dogs_cats_small/validation/dogs\n",
      "dogs_cats_small/validation/cats\n",
      "dogs_cats_small/test\n",
      "dogs_cats_small/test/dogs\n",
      "dogs_cats_small/test/cats\n"
     ]
    }
   ],
   "source": [
    "!find dogs_cats_small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "UtdYxWUFoCDN"
   },
   "outputs": [],
   "source": [
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "for fname in fnames:                                                       \n",
    "    src = os.path.join(original_dataset_dir, fname)                        \n",
    "    dst = os.path.join(train_cats_dir, fname)                              \n",
    "    shutil.copyfile(src, dst)                                              \n",
    "\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]               \n",
    "for fname in fnames:                                                       \n",
    "    src = os.path.join(original_dataset_dir, fname)                        \n",
    "    dst = os.path.join(validation_cats_dir, fname)                         \n",
    "    shutil.copyfile(src, dst)                                              \n",
    "\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]               \n",
    "for fname in fnames:                                                       \n",
    "    src = os.path.join(original_dataset_dir, fname)                        \n",
    "    dst = os.path.join(test_cats_dir, fname)                               \n",
    "    shutil.copyfile(src, dst)                                              \n",
    "\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]                     \n",
    "for fname in fnames:                                                       \n",
    "    src = os.path.join(original_dataset_dir, fname)                        \n",
    "    dst = os.path.join(train_dogs_dir, fname)                              \n",
    "    shutil.copyfile(src, dst)                                              \n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]               \n",
    "for fname in fnames:                                                       \n",
    "    src = os.path.join(original_dataset_dir, fname)                        \n",
    "    dst = os.path.join(validation_dogs_dir, fname)                         \n",
    "    shutil.copyfile(src, dst)                                              \n",
    "\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]               \n",
    "for fname in fnames:                                                       \n",
    "    src = os.path.join(original_dataset_dir, fname)                        \n",
    "    dst = os.path.join(test_dogs_dir, fname)                               \n",
    "    shutil.copyfile(src, dst) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1635,
     "status": "ok",
     "timestamp": 1531938252272,
     "user": {
      "displayName": "Israel Herraiz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "114232750363927198683"
     },
     "user_tz": -120
    },
    "id": "9utma06EoLs0",
    "outputId": "087e27de-3b9d-46d7-f90d-eb8a36ef6c02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs_cats_small/\n",
      "dogs_cats_small/train\n",
      "dogs_cats_small/train/dogs\n",
      "dogs_cats_small/train/dogs/dog.775.jpg\n",
      "dogs_cats_small/train/dogs/dog.595.jpg\n",
      "dogs_cats_small/train/dogs/dog.351.jpg\n",
      "dogs_cats_small/train/dogs/dog.795.jpg\n",
      "dogs_cats_small/train/dogs/dog.273.jpg\n",
      "dogs_cats_small/train/dogs/dog.216.jpg\n",
      "dogs_cats_small/train/dogs/dog.750.jpg\n",
      "find: ‘standard output’: Broken pipe\n",
      "find: write error\n"
     ]
    }
   ],
   "source": [
    "!find dogs_cats_small/ | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 722,
     "status": "ok",
     "timestamp": 1531944091857,
     "user": {
      "displayName": "Israel Herraiz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "114232750363927198683"
     },
     "user_tz": -120
    },
    "id": "P6O6PHuHque2",
    "outputId": "76441c3a-f528-41fb-ce02-4494f4762138"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15103847609512560740, name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 11285974221\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 7213464691253150237\n",
       " physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"]"
      ]
     },
     "execution_count": 111,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if we have a GPU\n",
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6udnOjpjoPKp"
   },
   "source": [
    "# Prepare data\n",
    "\n",
    "Because we are dealing with large images, we cannot just read them in a Numpy array. We will use generators, to consume the images as they are needed by the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 820,
     "status": "ok",
     "timestamp": 1531944212934,
     "user": {
      "displayName": "Israel Herraiz",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
      "userId": "114232750363927198683"
     },
     "user_tz": -120
    },
    "id": "yzMRtMsWoeGC",
    "outputId": "3d579e66-fae3-4bb8-c3a7-6d2e5d11caa4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGRCGie2o8xi"
   },
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7WQc8lUWtCyR"
   },
   "source": [
    "# Evaluate model\n",
    "\n",
    "Our model is a binary classifier. We can evaluate it as any other classifier.\n",
    "\n",
    "**EXERCISE 1**. Obtain the confusion matrix and associated metrics (precision, recall, F-score) for this classifier.\n",
    "\n",
    "**EXERCISE 2**. Plot the ROC curve and calculate the AUC score.\n",
    "\n",
    "**EXERCISE 3**. What is the best model you can obtain using the above evaluation parameters?\n",
    "\n",
    "\n",
    "In addition to this evaluation, for convolutional layers, we can attempt to plot each layer, applied to a image, to see what are the elements used by the model to find out the class the item belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vLCvC0jw__3r"
   },
   "source": [
    "# Data augmentation\n",
    "\n",
    "If we don't have enough images to train our model, we can manipulate our images to produce modifications, and to *augment* the training data.\n",
    "\n",
    "Because images are slightly different, this can help the network to learn some of the patterns better.\n",
    "\n",
    "See https://keras.io/preprocessing/image/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5_Q3QhtGxy5Z"
   },
   "source": [
    "# Reusing a pre-trained convnet\n",
    "\n",
    "Training a convolutional network is slow and tedious. And if we think of every day objects, some patterns will probably be useful for images of different types.\n",
    "\n",
    "Similarly to word2vec, Glove, and other pre-trained word embeddings, we can use pre-trained convolutional networks, to improve our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "OiHYp9ptC7sF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "In our classification problem, dogs are the *positive* case, and cats the *negative* (it could not be otherwise...).\n",
    "\n",
    "Obtain the ROC curve for the following classifiers:\n",
    " \n",
    "* Dense network\n",
    "* Convolutional layers\n",
    "* Convolutional layers with training data augmentation\n",
    "* Convolutional layers, using a pre-trained network, letting your network modify the weights\n",
    "* Convolutional layers, using a pre-trained network, with all the weights frozen\n",
    "\n",
    "Which one is the best classifier? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "04_convolutional_orig.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
