{"cells":[{"cell_type":"markdown","source":["# Clasificaci√≥n de texto con vectores de frecuencias\n","\n","[Pablo Carballeira] Partes de este c√≥digo han sido adaptadas del c√≥digo correspondiente a la especializaci√≥n Natural Language Processing de DeepLearning.AI, y este [tutorial](https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk)\n","\n","Puedes encontrar informaci√≥n sobre c√≥mo trabajar en Colab aqu√≠ (https://colab.research.google.com/notebooks/intro.ipynb):\n","\n","\n","\n"],"metadata":{"id":"hL6bxS2HjVtD"}},{"cell_type":"markdown","metadata":{"id":"KduRI7bjMrJv"},"source":["## Setup\n","\n","Importamos y descargamos algunas librer√≠as"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-T6nYFkMrJv"},"outputs":[],"source":["import nltk                                  # Python library for NLP\n","from nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\n","import matplotlib.pyplot as plt              # visualization library\n","import numpy as np                           # library for scientific computing and matrix operations\n","import pandas as pd                          # Library for Dataframes \n","\n","from nltk.tag import pos_tag\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('twitter_samples')"]},{"cell_type":"markdown","metadata":{"id":"g00lyyrCMrJm"},"source":["En este notebook nos centraremos en representar los tweets pre-procesados del notebook anterior como vectores matem√°ticos, sobre los que construir un clasificador de texto (clasificador de opini√≥n). El ejemplo aqu√≠ representado es sencillo, pero ayuda a entender como convertir el texto en vectores num√©ricos  sobre los que se pueden aplicar algoritmos de aprendizaje autom√°tico est√°ndar. \n","\n","Construiremos una funci√≥n `build_freqs()` que crear√° un diccionario donde podemos buscar cu√°ntas veces aparece una palabra del vocabulario en las listas de tweets positivos o negativos. Esto nos permite representar cada tweet con un vector de caracter√≠sticas que nos permitir√° clasificarlo como positivo o negativo. Analizaremos si este tipo de descriptores son √∫tiles para entrenar un clasificador de texto de manera eficiente. Finalmente, usaremos el dataset de Twitter para entrenar un clasificador real\n","\n","\n"]},{"cell_type":"markdown","source":["## Funci√≥n de pre-procesado\n","Utilizaremos la funcion  `process_tweet()` del notebook anterior para pre-procesar el texto de los tweets de forma adecuada\n"],"metadata":{"id":"wbN9hnssKDu2"}},{"cell_type":"code","source":["# download the stopwords for the process_tweet function\n","nltk.download('stopwords')\n","\n","import re\n","import string\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer\n","\n","\n","def process_tweet(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # remove hyperlinks\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and  # remove stopwords\n","                word not in string.punctuation):  # remove punctuation\n","            # tweets_clean.append(word)\n","            stem_word = stemmer.stem(word)  # stemming word\n","            tweets_clean.append(stem_word)\n","\n","    return tweets_clean"],"metadata":{"id":"T8CxuKpsVL8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"weJpU3aFMrJz"},"source":["## Twitter dataset\n","\n","Utilizamos el mismo dataset de tweets del notebook anterior: [Twitter dataset](http://www.nltk.org/howto/twitter.html#Using-a-Tweet-Corpus).\n","\n","El dataset se separa en tweets positivos y negativos. Contiene 5000 tweets positivos y 5000 tweets negativos exactamente. La coincidencia exacta entre el n√∫mero de muestras en cada clase no es una coincidencia. La intenci√≥n es tener un conjunto de datos equilibrado. Eso no refleja las distribuciones reales de clases positivas y negativas en Twitter. Los conjuntos de datos equilibrados simplifican el dise√±o de la mayor√≠a de los m√©todos de clasificaci√≥n de textos. Sin embargo, es importante ser consciente de que este equilibrio de clases es artificial. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VzioL1OgMrJz"},"outputs":[],"source":["# select the lists of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n","\n","# concatenate the lists, 1st part is the positive tweets followed by the negative\n","tweets = all_positive_tweets + all_negative_tweets\n","\n","# let's see how many tweets we have\n","print(\"Number of tweets: \", len(tweets))"]},{"cell_type":"markdown","source":["Etiquetamos los tweeets con 1's para tweets positivos y 0's para tweets negativos. Fij√©monos que la concatenaci√≥n responde a la estructura de la variable `tweets` definida arriba, donde los primeros 5000 tweets son positivos y los siguientes 5000 tweets son negativos.\n","\n","Podemos utilizar la librer√≠a de `numpy` para crear vectores de 0s y 1s facilmente:\n","\n","* `np.ones()` - create an array of 1's\n","* `np.zeros()` - create an array of 0's\n","* `np.append()` - concatenate arrays"],"metadata":{"id":"aiJeZkfEvg_H"}},{"cell_type":"markdown","metadata":{"id":"3KJOb4vwMrJ1"},"source":["### Ejercicio\n","Crea la variable labels que contenga las etiquetas asociadas a los 10000 tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOSOYVWVMrJ2"},"outputs":[],"source":["# make a numpy array representing labels of the tweets\n","# labels = ???"]},{"cell_type":"markdown","source":["## Funci√≥n **build_freqs()**"],"metadata":{"id":"NFV3CCkewKHW"}},{"cell_type":"markdown","metadata":{"id":"CL_444ONMrJ9"},"source":["Construyamos la funci√≥n **build_freqs()** que utilizaremos para calcular los vectores de frecuencias de cada palabra y cada tuit.\n","\n","Esta funci√≥n debe crear un  diccionario que contiene, para cada palabra, las frecuencias (conteo absoluto) de aparici√≥n de esa palabra en los tweets positivos y negativos pertenecientes al corpus (conjunto total de tuits). Es decir cuenta, con qu√© frecuencia una palabra  se asoci√≥ con una etiqueta positiva '1' o una etiqueta negativa '0'."]},{"cell_type":"markdown","metadata":{"id":"Y-p-7-M-MrJ-"},"source":["\n","\n","Cada clave del diccionario `freqs` debe ser una tupla de 2 elementos que contiene una pareja `(word, y)`. `word` es un token de un tuit procesado, mientras que `y` es un n√∫mero entero que representa la etiqueta: `1` para los tuits positivos y `0` para los tuits negativos. \n","\n","El valor asociado a esta clave es el n√∫mero de veces que esa palabra aparece en el corpus  de tweets asociado a la etiqueta correspondiente\n","\n","Por ejemplo:\n","``` \n","# \"followfriday\" appears 25 times in the positive tweets\n","('followfriday', 1.0): 25\n","\n","# \"shame\" appears 19 times in the negative tweets\n","('shame', 0.0): 19 \n","```"]},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Completa la funci√≥n, procesando el tweet, y aumentando la frecuencia correspondiente, para cada token del tuit. \n","\n","Ten en cuenta que cada palabra (token) puede estar o no presente en el diccionario. Si lo est√° debes incrementar el conteo. Si no, crear una nueva entrada en el dicionario."],"metadata":{"id":"SdwNdUgMNKjt"}},{"cell_type":"code","source":["def build_freqs(tweets, labels):\n","    \"\"\"Build frequencies.\n","    Input:\n","        tweets: a list of tweets\n","        labels: an m x 1 array with the sentiment label of each tweet\n","            (either 0 or 1)\n","    Output:\n","        freqs: a dictionary that maps each key (word, sentiment) to its\n","        frequency value\n","    \"\"\"\n","    # Convert np array to list since zip needs an iterable.\n","    # The squeeze is necessary or the list ends up with one element.\n","    # Also note that this is just a NOP if ys is already a list.\n","    labels_list = np.squeeze(labels).tolist()\n","\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","\n","    for y, tweet in zip(labels_list, tweets):\n","        # ???\n","\n","    return freqs\n"],"metadata":{"id":"RZtAahSmM47K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sg-y4ObxMrJ8"},"source":["## Frecuencias por palabra (tweets positivos y negativos)"]},{"cell_type":"markdown","metadata":{"id":"PbF2aycZMrJ-"},"source":["\n","Ahora, usamos el diccionario devuelto por la funci√≥n `build_freqs()` para calcular las frecuencias de cada palabra del vocabulario en los tweets positivos y negativos en todo el corpus de tweets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ba_9mySMrJ-"},"outputs":[],"source":["# create frequency dictionary\n","freqs = build_freqs(tweets, labels)\n","\n","# check data type\n","print(f'type(freqs) = {type(freqs)}')\n","\n","# check length of the dictionary\n","print(\"N√∫mero de palabras en el vocabulario: \", len(freqs))"]},{"cell_type":"markdown","source":["A continuaci√≥n mostramos algunos ejemplos de las frecuencias calculadas para algunas palabras representativas. F√≠jate que usamos las versiones normalizadas (radicalizadas) de cada t√©rmino. \n","\n","Guardamos los vectores en la variable data para usarlos mas adelante en una representaci√≥n gr√°fica."],"metadata":{"id":"1_JMUjwTzlQ8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPGBs7PWMrJ_"},"outputs":[],"source":["# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\n","keys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n","        '‚ù§', ':)', ':(', 'üòí', 'üò¨', 'üòÑ', 'üòç', '‚ôõ',\n","        'song', 'idea', 'power', 'play', 'magnific',  'guitar', 'tv', 'walk']\n","\n","# list representing our table of word counts.\n","# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\n","data = []\n","\n","# loop through our selected words\n","for word in keys:\n","    \n","    # initialize positive and negative counts\n","    pos = 0\n","    neg = 0\n","    \n","    # retrieve number of positive counts\n","    if (word, 1) in freqs:\n","        pos = freqs[(word, 1)]\n","        \n","    # retrieve number of negative counts\n","    if (word, 0) in freqs:\n","        neg = freqs[(word, 0)]\n","        \n","    # append the word counts to the table\n","    data.append([word, pos, neg])\n","\n","print(\"  word\",\"  pos\", \"  neg\")\n","data"]},{"cell_type":"markdown","metadata":{"id":"IKfSs9gLMrJ-"},"source":["F√≠jate como hay palabras claramente asociadas con tweets positivos, otras claramente asociadas con tweets negativos, y algunas neutras"]},{"cell_type":"markdown","source":["### Ejercicio\n","Prueba con otras palabras. Puedes buscar opciones en la siguiente lista de palabras y a√±adirlas a la lista anterior\n"],"metadata":{"id":"xEKraAR3Kc0z"}},{"cell_type":"code","source":["print(freqs)"],"metadata":{"id":"RKK3sV74KoM2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n2aOXhO9MrJ_"},"source":["## Representaci√≥n gr√°fica de las frecuencias de cada palabra\n","\n","Podemos usar un diagrama de dispersi√≥n para inspeccionar estas frecuencias de forma gr√°fica. \n","\n","Representaremos las frecuencias en  escala logar√≠tmica para reducir el rango  entre las frecuencias de algunas palabras (por ejemplo, `':)'` tiene 3568 conteos positivos mientras que solo 2 negativos).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-WH6N0lMrKB"},"outputs":[],"source":["fig, ax = plt.subplots(figsize = (8, 8))\n","\n","# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n","x = np.log([x[1] + 1 for x in data])  \n","\n","# do the same for the negative counts\n","y = np.log([x[2] + 1 for x in data]) \n","\n","# Plot a dot for each pair of words\n","ax.scatter(x, y)  \n","\n","# assign axis labels\n","plt.xlabel(\"Log Positive count\")\n","plt.ylabel(\"Log Negative count\")\n","\n","# Add the word as the label at the same position as you added the points just before\n","for i in range(0, len(data)):\n","    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n","\n","ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n","plt.show()"]},{"cell_type":"markdown","source":["La l√≠nea roja marca el l√≠mite entre las √°reas positivas y negativas. Las palabras cercanas a la l√≠nea roja se pueden clasificar como neutras.\n","\n","Este gr√°fico es f√°cil de interpretar. Muestra que los emoticonos `:)` y `:(` son muy importantes para el an√°lisis de sentimientos. Por lo tanto no debemos permitir que los pasos de pre-procesamiento eliminen estos s√≠mbolos.\n","\n","Si has a√±adido t√©rminos a la lista anterior. Es posible que haya ciertas sorpresas. Palabras que sem√°nticamente son neutras pueden tener un caracter positivo/negativo fuerte. Esto se debe a una distribuci√≥n artificialmente  desbalanceada, debido a que el dataset es de tama√±o limitado."],"metadata":{"id":"Vqb4b0eh9iqQ"}},{"cell_type":"markdown","source":["## Representacion de tuits con vectores\n","\n","Al igual que hemos hecho con las palabras, podemos representar cada tweet con un vector de frecuencias. En este caso, cada tuit estar√° representado por la suma de las frecuencias positivas y negativas de cada una de las palabras que contiene.\n","\n","Por ejemplo, el tuit 'happy song' se compone de los tokens `['happi','song']`, cuyas frecuencias podemos comprobar m√°s arriba. Las frecuencias positiva y negativa del tuit ser√°n las siguientes: \n","\n","``` \n","['happi','song']\n","pos: 211 + 22 = 233\n","neg: 25 + 27 = 52\n","``` \n","\n","\n","La siguiente funci√≥n debe sumar, por cada palabra del tweet su contribucion a la frecuencia positiva, y a frecuencia negativa"],"metadata":{"id":"ExjsR70uzx24"}},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Completa la siguiente funci√≥n, recorriendo los tokens de cada tuit, e incrementando los conteos positivo (x[0,0]) y negativo (x[0,1]) de forma adecuada, usando el vector de frecuencias de cada palabra.\n","\n","Ten en cuenta que puede que una palabra no exista en el diccionario. En ese caso la funci√≥n [get()](https://python-reference.readthedocs.io/en/latest/docs/dict/get.html), que puedes utilizar, debe devolver el valor 0"],"metadata":{"id":"OZWnbP4iSNP6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbfQCn8XYyPE"},"outputs":[],"source":["def extract_features(tweet, freqs):\n","    '''\n","    Input: \n","        tweet: a list of words for one tweet\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","    Output: \n","        x: a feature vector of dimension (1,2)\n","    '''\n","    # process_tweet tokenizes, stems, and removes stopwords\n","    word_l = process_tweet(tweet)\n","    \n","    # 3 elements in the form of a 1 x 3 vector\n","    x = np.zeros((1, 2)) \n","    \n","    # loop through each word in the list of words\n","    # ???\n"," \n","    assert(x.shape == (1, 2))\n","    return x"]},{"cell_type":"markdown","source":["Vamos a visualizar los vectores de frecuencias de algunos tuits del dataset"],"metadata":{"id":"xWT6MFfv8uxm"}},{"cell_type":"code","source":["import random\n","\n","# creating a list of column names\n","column_values = ['tweet', 'pos', 'neg', 'sentiment']\n","\n","df = pd.DataFrame(columns=column_values)\n","for i in range(20):\n","    sample = random.randint(0,10000)\n","    tweet = tweets[sample]\n","    vector = extract_features(tweet, freqs)\n","    df.loc[i] = [ tweet, vector[0][0], vector[0][1],labels[sample]]\n","\n","df.head(20)"],"metadata":{"id":"TQ4XWg_KWL9C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rB1aLGBcEOSM"},"source":["# Visualizando los vectores descriptores de los tweets\n","\n","En esta secci√≥n vamos a visualizar gr√°ficamente los vectores de caracter√≠sticas que utilizamos como descriptores para nuestro corpus de tweets, y analizar si contienen informaci√≥n relevante para entrenar un clasificador efecivo (por ejemplo un regresor log√≠stico)."]},{"cell_type":"markdown","source":["Cargamos los vectores de caracter√≠sticas que tenemos pre-calculados en un CSV para todo nuestro corpus de tweets"],"metadata":{"id":"Sz_wyPiej_qJ"}},{"cell_type":"code","source":["!mkdir /content/data\n","!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1950LLLR4jwvi-4o1sOnGiWxyz2xB0cSn' -O data/logistic_features.csv\n","\n","#https://drive.google.com/file/d/1950LLLR4jwvi-4o1sOnGiWxyz2xB0cSn/view?usp=sharing"],"metadata":{"id":"9ZBKibBKQ6Hk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-TwDAoAQEOSr"},"source":["Vamos a mostrar en una gr√°fica de dispersi√≥n los vectores de frecuencias correspondientes a cada uno de los tweets de nuestro corpus.  Adem√°s, coloreamos cada tweet, dependiendo de su clase. Los tweets positivos ser√°n verdes y los tweets negativos ser√°n rojos.\n","\n","\n","En esa misma gr√°fica comprobamos que ambas clases (positiva y negativa) son separables y que se puede entrenar un modelo sencillo para clasificar los tweets con una tasa de acierto muy elevada. La recta representa un plano que divide nuestro espacio de caracter√≠sticas en dos partes. Las muestras ubicadas por encima de ese plano se consideran positivas y las muestras ubicadas por debajo de ese plano se consideran negativas.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rutwa-UKEOSt"},"outputs":[],"source":["theta = [7e-08, 0.0005239, -0.00055517]\n","\n","# Equation for the separation plane\n","# It give a value in the negative axe as a function of a positive value\n","# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n","# s(pos, W) = (w0 - w1 * pos) / w2\n","def neg(theta, pos):\n","    return (-theta[0] - pos * theta[1]) / theta[2]\n","\n","# Equation for the direction of the sentiments change\n","# We don't care about the magnitude of the change. We are only interested \n","# in the direction. So this direction is just a perpendicular function to the \n","# separation plane\n","# df(pos, W) = pos * w2 / w1\n","def direction(theta, pos):\n","    return    pos * theta[2] / theta[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZHHjrUjEOSr"},"outputs":[],"source":["# Plot the samples using columns 1 and 2 of the matrix\n","fig, ax = plt.subplots(figsize = (8, 8))\n","colors = ['red', 'green']\n","\n","data = pd.read_csv('data/logistic_features.csv'); # Load a 3 columns csv file using pandas function\n","\n","# Each feature is labeled as bias, positive and negative\n","X = data[['bias', 'positive', 'negative']].values # Get only the numerical values of the dataframe\n","Y = data['sentiment'].values; # Put in Y the corresponding labels or sentiments\n","\n","# print(X.shape) # Print the shape of the X part\n","# print(X) # Print some rows of X\n","\n","# Color based on the sentiment Y\n","ax.scatter(np.log(X[:,1]), np.log(X[:,2]), c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\n","plt.xlabel(\"Positive (log scale)\")\n","plt.ylabel(\"Negative (log scale)\")\n","\n","# Now lets represent the logistic regression model in this chart. \n","maxpos = np.max(np.log(X[:,1]))\n","\n","offset = 2 # The pos value for the direction vectors origin\n","\n","# Plot a gray line that divides the 2 areas.\n","ax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n","\n","# Plot a green line pointing to the positive direction\n","ax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=0.25, head_length=0.25, fc='g', ec='g')\n","# Plot a red line pointing to the negative direction\n","ax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=0.25, head_length=0.25, fc='r', ec='r')\n","\n","plt.show()"]},{"cell_type":"markdown","source":["Para facilitar la interpretaci√≥n del gr√°fico hemos representado las caracter√≠sticas positivas y negativas de cada tuit en escala logar√≠tmica"],"metadata":{"id":"pMJFDs7IrULi"}},{"cell_type":"markdown","source":["Del gr√°fico se desprende que las caracter√≠sticas que hemos elegido para representar los tuits como vectores num√©ricos permiten una separaci√≥n casi perfecta entre tuits positivos y negativos. ¬°As√≠ que puede esperar una precisi√≥n muy alta para un modelo de clasificaci√≥n entrenado con estos vectores!"],"metadata":{"id":"dFpiaTdpmACQ"}},{"cell_type":"markdown","source":["# Entrenamiento de un clasificador con el dataset de Twitter\n","\n","Una vez analizado c√∫al es una representaci√≥n eficiente de la cadenas de texto para entrenar un clasificador de texto (an√°lisis de opini√≥n), vamos a poner esto en pr√°ctica, entrenando un clasificador con el dataset de Twiiter, y comprobar si el funcionamiento es tan bueno como cabr√≠a esperar"],"metadata":{"id":"cBC5MTnMsdxb"}},{"cell_type":"markdown","source":["Esta es una funci√≥n que vamos a utilizar para pre-procesar los tuits. F√≠jate que hay algunas diferencias con la funci√≥n que hemos definido antes. En particular, f√≠jate que se utiliza una funci√≥n pos_tag() que extrae la etiqueta morfol√≥gica de cada palabra.\n","\n","Es decir, se utilizan estas etiquetas morfol√≥gicas para hacer el pre-procesado de forma m√°s especializada. En concreto, se hace una lematizaci√≥n distinta en funci√≥n de esta etiqueta. Veremos m√°s sobre an√°lisis morfol√≥gico (Part-of Speech Tagging) m√°s adelante. \n","\n","Por ahora, vamos a ver qu√© pinta tiene estas etiquetas morfol√≥gicas"],"metadata":{"id":"C-kGf7fp_1yB"}},{"cell_type":"code","source":["tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","print(pos_tag(tweet_tokens[0]))"],"metadata":{"id":"DS3N-jYWGOUE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utilizamos esta funci√≥n para pre-procesar los tuits\n"],"metadata":{"id":"RWxuSL4GAHDs"}},{"cell_type":"code","source":["import re, string\n","\n","def remove_noise(tweet_tokens, stop_words = ()):\n","\n","    cleaned_tokens = []\n","\n","    for token, tag in pos_tag(tweet_tokens):\n","        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n","        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n","\n","        if tag.startswith(\"NN\"):\n","            pos = 'n'\n","        elif tag.startswith('VB'):\n","            pos = 'v'\n","        else:\n","            pos = 'a'\n","\n","        lemmatizer = WordNetLemmatizer()\n","        token = lemmatizer.lemmatize(token, pos)\n","\n","        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n","            cleaned_tokens.append(token.lower())\n","    return cleaned_tokens"],"metadata":{"id":"o1v9vbTot6D0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","\n","#print(remove_noise(tweet_tokens[0], stop_words))\n","\n","positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n","\n","positive_cleaned_tokens_list = []\n","negative_cleaned_tokens_list = []\n","\n","for tokens in positive_tweet_tokens:\n","    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n","\n","for tokens in negative_tweet_tokens:\n","    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"],"metadata":{"id":"7wpM48Ihtsdg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y adaptamos la extructura de los datos para el clasificador. El modelo que vamos a utilizr requiere no solo una lista de palabras en un tweet, sino tambi√©n un diccionario de Python con palabras como claves y True como valores."],"metadata":{"id":"JDfxHI46AMkM"}},{"cell_type":"code","source":["def get_tweets_for_model(cleaned_tokens_list):\n","    for tweet_tokens in cleaned_tokens_list:\n","        yield dict([token, True] for token in tweet_tokens)\n","\n","positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n","negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"],"metadata":{"id":"ahonGSCjumlj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Ahora vamos a entrenar el clasificador utilizando un 70% del dataset, y dejando el 30 % restante para validar los resultados. Completa la creacion de los conjuntos de entrenamiento y test (train_data) y (test_data) a partir del conjunto total de datos (dataset)\n","\n","Aseg√∫rate de que la distribuci√≥n de tuits positivos y negativos es adecuada. Para ello, f√≠jate en como se estructura la variable dataset"],"metadata":{"id":"jIaBp8pVC-hh"}},{"cell_type":"code","source":["import random\n","from nltk import NaiveBayesClassifier\n","from nltk import classify\n","\n","positive_dataset = [(tweet_dict, \"Positive\")\n","                     for tweet_dict in positive_tokens_for_model]\n","\n","negative_dataset = [(tweet_dict, \"Negative\")\n","                     for tweet_dict in negative_tokens_for_model]\n","\n","dataset = positive_dataset + negative_dataset\n","\n","# train_data = ???\n","# test_data = ???\n","\n","classifier = NaiveBayesClassifier.train(train_data)\n","\n","print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n","\n","print(classifier.show_most_informative_features(10))"],"metadata":{"id":"yDU1TQ8yshk1"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"02b_text_classification_example_twitter_stu.ipynb","provenance":[{"file_id":"1ALrxN1uwdzmZJUDvgguMkg7Xw2yLF0Z5","timestamp":1645615108499},{"file_id":"191eI6MbYa_cbs7W8DuuPVxT0Iy-A-bzC","timestamp":1644842966682}],"toc_visible":true,"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}