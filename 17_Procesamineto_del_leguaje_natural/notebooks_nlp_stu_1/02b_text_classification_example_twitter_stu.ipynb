{"cells":[{"cell_type":"markdown","source":["# Clasificación de texto con vectores de frecuencias\n","\n","[Pablo Carballeira] Partes de este código han sido adaptadas del código correspondiente a la especialización Natural Language Processing de DeepLearning.AI, y este [tutorial](https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk)\n","\n","Puedes encontrar información sobre cómo trabajar en Colab aquí (https://colab.research.google.com/notebooks/intro.ipynb):\n","\n","\n","\n"],"metadata":{"id":"hL6bxS2HjVtD"}},{"cell_type":"markdown","metadata":{"id":"KduRI7bjMrJv"},"source":["## Setup\n","\n","Importamos y descargamos algunas librerías"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A-T6nYFkMrJv"},"outputs":[],"source":["import nltk                                  # Python library for NLP\n","from nltk.corpus import twitter_samples      # sample Twitter dataset from NLTK\n","import matplotlib.pyplot as plt              # visualization library\n","import numpy as np                           # library for scientific computing and matrix operations\n","import pandas as pd                          # Library for Dataframes \n","\n","from nltk.tag import pos_tag\n","from nltk.stem.wordnet import WordNetLemmatizer\n","\n","nltk.download('wordnet')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('twitter_samples')"]},{"cell_type":"markdown","metadata":{"id":"g00lyyrCMrJm"},"source":["En este notebook nos centraremos en representar los tweets pre-procesados del notebook anterior como vectores matemáticos, sobre los que construir un clasificador de texto (clasificador de opinión). El ejemplo aquí representado es sencillo, pero ayuda a entender como convertir el texto en vectores numéricos  sobre los que se pueden aplicar algoritmos de aprendizaje automático estándar. \n","\n","Construiremos una función `build_freqs()` que creará un diccionario donde podemos buscar cuántas veces aparece una palabra del vocabulario en las listas de tweets positivos o negativos. Esto nos permite representar cada tweet con un vector de características que nos permitirá clasificarlo como positivo o negativo. Analizaremos si este tipo de descriptores son útiles para entrenar un clasificador de texto de manera eficiente. Finalmente, usaremos el dataset de Twitter para entrenar un clasificador real\n","\n","\n"]},{"cell_type":"markdown","source":["## Función de pre-procesado\n","Utilizaremos la funcion  `process_tweet()` del notebook anterior para pre-procesar el texto de los tweets de forma adecuada\n"],"metadata":{"id":"wbN9hnssKDu2"}},{"cell_type":"code","source":["# download the stopwords for the process_tweet function\n","nltk.download('stopwords')\n","\n","import re\n","import string\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer\n","\n","\n","def process_tweet(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n","    # remove hyperlinks\n","    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub(r'#', '', tweet)\n","    # tokenize tweets\n","    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:\n","        if (word not in stopwords_english and  # remove stopwords\n","                word not in string.punctuation):  # remove punctuation\n","            # tweets_clean.append(word)\n","            stem_word = stemmer.stem(word)  # stemming word\n","            tweets_clean.append(stem_word)\n","\n","    return tweets_clean"],"metadata":{"id":"T8CxuKpsVL8d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"weJpU3aFMrJz"},"source":["## Twitter dataset\n","\n","Utilizamos el mismo dataset de tweets del notebook anterior: [Twitter dataset](http://www.nltk.org/howto/twitter.html#Using-a-Tweet-Corpus).\n","\n","El dataset se separa en tweets positivos y negativos. Contiene 5000 tweets positivos y 5000 tweets negativos exactamente. La coincidencia exacta entre el número de muestras en cada clase no es una coincidencia. La intención es tener un conjunto de datos equilibrado. Eso no refleja las distribuciones reales de clases positivas y negativas en Twitter. Los conjuntos de datos equilibrados simplifican el diseño de la mayoría de los métodos de clasificación de textos. Sin embargo, es importante ser consciente de que este equilibrio de clases es artificial. \n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VzioL1OgMrJz"},"outputs":[],"source":["# select the lists of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n","\n","# concatenate the lists, 1st part is the positive tweets followed by the negative\n","tweets = all_positive_tweets + all_negative_tweets\n","\n","# let's see how many tweets we have\n","print(\"Number of tweets: \", len(tweets))"]},{"cell_type":"markdown","source":["Etiquetamos los tweeets con 1's para tweets positivos y 0's para tweets negativos. Fijémonos que la concatenación responde a la estructura de la variable `tweets` definida arriba, donde los primeros 5000 tweets son positivos y los siguientes 5000 tweets son negativos.\n","\n","Podemos utilizar la librería de `numpy` para crear vectores de 0s y 1s facilmente:\n","\n","* `np.ones()` - create an array of 1's\n","* `np.zeros()` - create an array of 0's\n","* `np.append()` - concatenate arrays"],"metadata":{"id":"aiJeZkfEvg_H"}},{"cell_type":"markdown","metadata":{"id":"3KJOb4vwMrJ1"},"source":["### Ejercicio\n","Crea la variable labels que contenga las etiquetas asociadas a los 10000 tweets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zOSOYVWVMrJ2"},"outputs":[],"source":["# make a numpy array representing labels of the tweets\n","# labels = ???"]},{"cell_type":"markdown","source":["## Función **build_freqs()**"],"metadata":{"id":"NFV3CCkewKHW"}},{"cell_type":"markdown","metadata":{"id":"CL_444ONMrJ9"},"source":["Construyamos la función **build_freqs()** que utilizaremos para calcular los vectores de frecuencias de cada palabra y cada tuit.\n","\n","Esta función debe crear un  diccionario que contiene, para cada palabra, las frecuencias (conteo absoluto) de aparición de esa palabra en los tweets positivos y negativos pertenecientes al corpus (conjunto total de tuits). Es decir cuenta, con qué frecuencia una palabra  se asoció con una etiqueta positiva '1' o una etiqueta negativa '0'."]},{"cell_type":"markdown","metadata":{"id":"Y-p-7-M-MrJ-"},"source":["\n","\n","Cada clave del diccionario `freqs` debe ser una tupla de 2 elementos que contiene una pareja `(word, y)`. `word` es un token de un tuit procesado, mientras que `y` es un número entero que representa la etiqueta: `1` para los tuits positivos y `0` para los tuits negativos. \n","\n","El valor asociado a esta clave es el número de veces que esa palabra aparece en el corpus  de tweets asociado a la etiqueta correspondiente\n","\n","Por ejemplo:\n","``` \n","# \"followfriday\" appears 25 times in the positive tweets\n","('followfriday', 1.0): 25\n","\n","# \"shame\" appears 19 times in the negative tweets\n","('shame', 0.0): 19 \n","```"]},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Completa la función, procesando el tweet, y aumentando la frecuencia correspondiente, para cada token del tuit. \n","\n","Ten en cuenta que cada palabra (token) puede estar o no presente en el diccionario. Si lo está debes incrementar el conteo. Si no, crear una nueva entrada en el dicionario."],"metadata":{"id":"SdwNdUgMNKjt"}},{"cell_type":"code","source":["def build_freqs(tweets, labels):\n","    \"\"\"Build frequencies.\n","    Input:\n","        tweets: a list of tweets\n","        labels: an m x 1 array with the sentiment label of each tweet\n","            (either 0 or 1)\n","    Output:\n","        freqs: a dictionary that maps each key (word, sentiment) to its\n","        frequency value\n","    \"\"\"\n","    # Convert np array to list since zip needs an iterable.\n","    # The squeeze is necessary or the list ends up with one element.\n","    # Also note that this is just a NOP if ys is already a list.\n","    labels_list = np.squeeze(labels).tolist()\n","\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","\n","    for y, tweet in zip(labels_list, tweets):\n","        # ???\n","\n","    return freqs\n"],"metadata":{"id":"RZtAahSmM47K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Sg-y4ObxMrJ8"},"source":["## Frecuencias por palabra (tweets positivos y negativos)"]},{"cell_type":"markdown","metadata":{"id":"PbF2aycZMrJ-"},"source":["\n","Ahora, usamos el diccionario devuelto por la función `build_freqs()` para calcular las frecuencias de cada palabra del vocabulario en los tweets positivos y negativos en todo el corpus de tweets."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_ba_9mySMrJ-"},"outputs":[],"source":["# create frequency dictionary\n","freqs = build_freqs(tweets, labels)\n","\n","# check data type\n","print(f'type(freqs) = {type(freqs)}')\n","\n","# check length of the dictionary\n","print(\"Número de palabras en el vocabulario: \", len(freqs))"]},{"cell_type":"markdown","source":["A continuación mostramos algunos ejemplos de las frecuencias calculadas para algunas palabras representativas. Fíjate que usamos las versiones normalizadas (radicalizadas) de cada término. \n","\n","Guardamos los vectores en la variable data para usarlos mas adelante en una representación gráfica."],"metadata":{"id":"1_JMUjwTzlQ8"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"dPGBs7PWMrJ_"},"outputs":[],"source":["# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\n","keys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n","        '❤', ':)', ':(', '😒', '😬', '😄', '😍', '♛',\n","        'song', 'idea', 'power', 'play', 'magnific',  'guitar', 'tv', 'walk']\n","\n","# list representing our table of word counts.\n","# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\n","data = []\n","\n","# loop through our selected words\n","for word in keys:\n","    \n","    # initialize positive and negative counts\n","    pos = 0\n","    neg = 0\n","    \n","    # retrieve number of positive counts\n","    if (word, 1) in freqs:\n","        pos = freqs[(word, 1)]\n","        \n","    # retrieve number of negative counts\n","    if (word, 0) in freqs:\n","        neg = freqs[(word, 0)]\n","        \n","    # append the word counts to the table\n","    data.append([word, pos, neg])\n","\n","print(\"  word\",\"  pos\", \"  neg\")\n","data"]},{"cell_type":"markdown","metadata":{"id":"IKfSs9gLMrJ-"},"source":["Fíjate como hay palabras claramente asociadas con tweets positivos, otras claramente asociadas con tweets negativos, y algunas neutras"]},{"cell_type":"markdown","source":["### Ejercicio\n","Prueba con otras palabras. Puedes buscar opciones en la siguiente lista de palabras y añadirlas a la lista anterior\n"],"metadata":{"id":"xEKraAR3Kc0z"}},{"cell_type":"code","source":["print(freqs)"],"metadata":{"id":"RKK3sV74KoM2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n2aOXhO9MrJ_"},"source":["## Representación gráfica de las frecuencias de cada palabra\n","\n","Podemos usar un diagrama de dispersión para inspeccionar estas frecuencias de forma gráfica. \n","\n","Representaremos las frecuencias en  escala logarítmica para reducir el rango  entre las frecuencias de algunas palabras (por ejemplo, `':)'` tiene 3568 conteos positivos mientras que solo 2 negativos).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T-WH6N0lMrKB"},"outputs":[],"source":["fig, ax = plt.subplots(figsize = (8, 8))\n","\n","# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n","x = np.log([x[1] + 1 for x in data])  \n","\n","# do the same for the negative counts\n","y = np.log([x[2] + 1 for x in data]) \n","\n","# Plot a dot for each pair of words\n","ax.scatter(x, y)  \n","\n","# assign axis labels\n","plt.xlabel(\"Log Positive count\")\n","plt.ylabel(\"Log Negative count\")\n","\n","# Add the word as the label at the same position as you added the points just before\n","for i in range(0, len(data)):\n","    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n","\n","ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n","plt.show()"]},{"cell_type":"markdown","source":["La línea roja marca el límite entre las áreas positivas y negativas. Las palabras cercanas a la línea roja se pueden clasificar como neutras.\n","\n","Este gráfico es fácil de interpretar. Muestra que los emoticonos `:)` y `:(` son muy importantes para el análisis de sentimientos. Por lo tanto no debemos permitir que los pasos de pre-procesamiento eliminen estos símbolos.\n","\n","Si has añadido términos a la lista anterior. Es posible que haya ciertas sorpresas. Palabras que semánticamente son neutras pueden tener un caracter positivo/negativo fuerte. Esto se debe a una distribución artificialmente  desbalanceada, debido a que el dataset es de tamaño limitado."],"metadata":{"id":"Vqb4b0eh9iqQ"}},{"cell_type":"markdown","source":["## Representacion de tuits con vectores\n","\n","Al igual que hemos hecho con las palabras, podemos representar cada tweet con un vector de frecuencias. En este caso, cada tuit estará representado por la suma de las frecuencias positivas y negativas de cada una de las palabras que contiene.\n","\n","Por ejemplo, el tuit 'happy song' se compone de los tokens `['happi','song']`, cuyas frecuencias podemos comprobar más arriba. Las frecuencias positiva y negativa del tuit serán las siguientes: \n","\n","``` \n","['happi','song']\n","pos: 211 + 22 = 233\n","neg: 25 + 27 = 52\n","``` \n","\n","\n","La siguiente función debe sumar, por cada palabra del tweet su contribucion a la frecuencia positiva, y a frecuencia negativa"],"metadata":{"id":"ExjsR70uzx24"}},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Completa la siguiente función, recorriendo los tokens de cada tuit, e incrementando los conteos positivo (x[0,0]) y negativo (x[0,1]) de forma adecuada, usando el vector de frecuencias de cada palabra.\n","\n","Ten en cuenta que puede que una palabra no exista en el diccionario. En ese caso la función [get()](https://python-reference.readthedocs.io/en/latest/docs/dict/get.html), que puedes utilizar, debe devolver el valor 0"],"metadata":{"id":"OZWnbP4iSNP6"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GbfQCn8XYyPE"},"outputs":[],"source":["def extract_features(tweet, freqs):\n","    '''\n","    Input: \n","        tweet: a list of words for one tweet\n","        freqs: a dictionary corresponding to the frequencies of each tuple (word, label)\n","    Output: \n","        x: a feature vector of dimension (1,2)\n","    '''\n","    # process_tweet tokenizes, stems, and removes stopwords\n","    word_l = process_tweet(tweet)\n","    \n","    # 3 elements in the form of a 1 x 3 vector\n","    x = np.zeros((1, 2)) \n","    \n","    # loop through each word in the list of words\n","    # ???\n"," \n","    assert(x.shape == (1, 2))\n","    return x"]},{"cell_type":"markdown","source":["Vamos a visualizar los vectores de frecuencias de algunos tuits del dataset"],"metadata":{"id":"xWT6MFfv8uxm"}},{"cell_type":"code","source":["import random\n","\n","# creating a list of column names\n","column_values = ['tweet', 'pos', 'neg', 'sentiment']\n","\n","df = pd.DataFrame(columns=column_values)\n","for i in range(20):\n","    sample = random.randint(0,10000)\n","    tweet = tweets[sample]\n","    vector = extract_features(tweet, freqs)\n","    df.loc[i] = [ tweet, vector[0][0], vector[0][1],labels[sample]]\n","\n","df.head(20)"],"metadata":{"id":"TQ4XWg_KWL9C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rB1aLGBcEOSM"},"source":["# Visualizando los vectores descriptores de los tweets\n","\n","En esta sección vamos a visualizar gráficamente los vectores de características que utilizamos como descriptores para nuestro corpus de tweets, y analizar si contienen información relevante para entrenar un clasificador efecivo (por ejemplo un regresor logístico)."]},{"cell_type":"markdown","source":["Cargamos los vectores de características que tenemos pre-calculados en un CSV para todo nuestro corpus de tweets"],"metadata":{"id":"Sz_wyPiej_qJ"}},{"cell_type":"code","source":["!mkdir /content/data\n","!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1950LLLR4jwvi-4o1sOnGiWxyz2xB0cSn' -O data/logistic_features.csv\n","\n","#https://drive.google.com/file/d/1950LLLR4jwvi-4o1sOnGiWxyz2xB0cSn/view?usp=sharing"],"metadata":{"id":"9ZBKibBKQ6Hk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-TwDAoAQEOSr"},"source":["Vamos a mostrar en una gráfica de dispersión los vectores de frecuencias correspondientes a cada uno de los tweets de nuestro corpus.  Además, coloreamos cada tweet, dependiendo de su clase. Los tweets positivos serán verdes y los tweets negativos serán rojos.\n","\n","\n","En esa misma gráfica comprobamos que ambas clases (positiva y negativa) son separables y que se puede entrenar un modelo sencillo para clasificar los tweets con una tasa de acierto muy elevada. La recta representa un plano que divide nuestro espacio de características en dos partes. Las muestras ubicadas por encima de ese plano se consideran positivas y las muestras ubicadas por debajo de ese plano se consideran negativas.\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rutwa-UKEOSt"},"outputs":[],"source":["theta = [7e-08, 0.0005239, -0.00055517]\n","\n","# Equation for the separation plane\n","# It give a value in the negative axe as a function of a positive value\n","# f(pos, neg, W) = w0 + w1 * pos + w2 * neg = 0\n","# s(pos, W) = (w0 - w1 * pos) / w2\n","def neg(theta, pos):\n","    return (-theta[0] - pos * theta[1]) / theta[2]\n","\n","# Equation for the direction of the sentiments change\n","# We don't care about the magnitude of the change. We are only interested \n","# in the direction. So this direction is just a perpendicular function to the \n","# separation plane\n","# df(pos, W) = pos * w2 / w1\n","def direction(theta, pos):\n","    return    pos * theta[2] / theta[1]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jZHHjrUjEOSr"},"outputs":[],"source":["# Plot the samples using columns 1 and 2 of the matrix\n","fig, ax = plt.subplots(figsize = (8, 8))\n","colors = ['red', 'green']\n","\n","data = pd.read_csv('data/logistic_features.csv'); # Load a 3 columns csv file using pandas function\n","\n","# Each feature is labeled as bias, positive and negative\n","X = data[['bias', 'positive', 'negative']].values # Get only the numerical values of the dataframe\n","Y = data['sentiment'].values; # Put in Y the corresponding labels or sentiments\n","\n","# print(X.shape) # Print the shape of the X part\n","# print(X) # Print some rows of X\n","\n","# Color based on the sentiment Y\n","ax.scatter(np.log(X[:,1]), np.log(X[:,2]), c=[colors[int(k)] for k in Y], s = 0.1)  # Plot a dot for each pair of words\n","plt.xlabel(\"Positive (log scale)\")\n","plt.ylabel(\"Negative (log scale)\")\n","\n","# Now lets represent the logistic regression model in this chart. \n","maxpos = np.max(np.log(X[:,1]))\n","\n","offset = 2 # The pos value for the direction vectors origin\n","\n","# Plot a gray line that divides the 2 areas.\n","ax.plot([0,  maxpos], [neg(theta, 0),   neg(theta, maxpos)], color = 'gray') \n","\n","# Plot a green line pointing to the positive direction\n","ax.arrow(offset, neg(theta, offset), offset, direction(theta, offset), head_width=0.25, head_length=0.25, fc='g', ec='g')\n","# Plot a red line pointing to the negative direction\n","ax.arrow(offset, neg(theta, offset), -offset, -direction(theta, offset), head_width=0.25, head_length=0.25, fc='r', ec='r')\n","\n","plt.show()"]},{"cell_type":"markdown","source":["Para facilitar la interpretación del gráfico hemos representado las características positivas y negativas de cada tuit en escala logarítmica"],"metadata":{"id":"pMJFDs7IrULi"}},{"cell_type":"markdown","source":["Del gráfico se desprende que las características que hemos elegido para representar los tuits como vectores numéricos permiten una separación casi perfecta entre tuits positivos y negativos. ¡Así que puede esperar una precisión muy alta para un modelo de clasificación entrenado con estos vectores!"],"metadata":{"id":"dFpiaTdpmACQ"}},{"cell_type":"markdown","source":["# Entrenamiento de un clasificador con el dataset de Twitter\n","\n","Una vez analizado cúal es una representación eficiente de la cadenas de texto para entrenar un clasificador de texto (análisis de opinión), vamos a poner esto en práctica, entrenando un clasificador con el dataset de Twiiter, y comprobar si el funcionamiento es tan bueno como cabría esperar"],"metadata":{"id":"cBC5MTnMsdxb"}},{"cell_type":"markdown","source":["Esta es una función que vamos a utilizar para pre-procesar los tuits. Fíjate que hay algunas diferencias con la función que hemos definido antes. En particular, fíjate que se utiliza una función pos_tag() que extrae la etiqueta morfológica de cada palabra.\n","\n","Es decir, se utilizan estas etiquetas morfológicas para hacer el pre-procesado de forma más especializada. En concreto, se hace una lematización distinta en función de esta etiqueta. Veremos más sobre análisis morfológico (Part-of Speech Tagging) más adelante. \n","\n","Por ahora, vamos a ver qué pinta tiene estas etiquetas morfológicas"],"metadata":{"id":"C-kGf7fp_1yB"}},{"cell_type":"code","source":["tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","print(pos_tag(tweet_tokens[0]))"],"metadata":{"id":"DS3N-jYWGOUE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Utilizamos esta función para pre-procesar los tuits\n"],"metadata":{"id":"RWxuSL4GAHDs"}},{"cell_type":"code","source":["import re, string\n","\n","def remove_noise(tweet_tokens, stop_words = ()):\n","\n","    cleaned_tokens = []\n","\n","    for token, tag in pos_tag(tweet_tokens):\n","        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n","                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n","        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n","\n","        if tag.startswith(\"NN\"):\n","            pos = 'n'\n","        elif tag.startswith('VB'):\n","            pos = 'v'\n","        else:\n","            pos = 'a'\n","\n","        lemmatizer = WordNetLemmatizer()\n","        token = lemmatizer.lemmatize(token, pos)\n","\n","        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n","            cleaned_tokens.append(token.lower())\n","    return cleaned_tokens"],"metadata":{"id":"o1v9vbTot6D0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","stop_words = stopwords.words('english')\n","\n","#print(remove_noise(tweet_tokens[0], stop_words))\n","\n","positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n","negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n","\n","positive_cleaned_tokens_list = []\n","negative_cleaned_tokens_list = []\n","\n","for tokens in positive_tweet_tokens:\n","    positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n","\n","for tokens in negative_tweet_tokens:\n","    negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))"],"metadata":{"id":"7wpM48Ihtsdg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y adaptamos la extructura de los datos para el clasificador. El modelo que vamos a utilizr requiere no solo una lista de palabras en un tweet, sino también un diccionario de Python con palabras como claves y True como valores."],"metadata":{"id":"JDfxHI46AMkM"}},{"cell_type":"code","source":["def get_tweets_for_model(cleaned_tokens_list):\n","    for tweet_tokens in cleaned_tokens_list:\n","        yield dict([token, True] for token in tweet_tokens)\n","\n","positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n","negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)"],"metadata":{"id":"ahonGSCjumlj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Ahora vamos a entrenar el clasificador utilizando un 70% del dataset, y dejando el 30 % restante para validar los resultados. Completa la creacion de los conjuntos de entrenamiento y test (train_data) y (test_data) a partir del conjunto total de datos (dataset)\n","\n","Asegúrate de que la distribución de tuits positivos y negativos es adecuada. Para ello, fíjate en como se estructura la variable dataset"],"metadata":{"id":"jIaBp8pVC-hh"}},{"cell_type":"code","source":["import random\n","from nltk import NaiveBayesClassifier\n","from nltk import classify\n","\n","positive_dataset = [(tweet_dict, \"Positive\")\n","                     for tweet_dict in positive_tokens_for_model]\n","\n","negative_dataset = [(tweet_dict, \"Negative\")\n","                     for tweet_dict in negative_tokens_for_model]\n","\n","dataset = positive_dataset + negative_dataset\n","\n","# train_data = ???\n","# test_data = ???\n","\n","classifier = NaiveBayesClassifier.train(train_data)\n","\n","print(\"Accuracy is:\", classify.accuracy(classifier, test_data))\n","\n","print(classifier.show_most_informative_features(10))"],"metadata":{"id":"yDU1TQ8yshk1"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"},"colab":{"name":"02b_text_classification_example_twitter_stu.ipynb","provenance":[{"file_id":"1ALrxN1uwdzmZJUDvgguMkg7Xw2yLF0Z5","timestamp":1645615108499},{"file_id":"191eI6MbYa_cbs7W8DuuPVxT0Iy-A-bzC","timestamp":1644842966682}],"toc_visible":true,"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}