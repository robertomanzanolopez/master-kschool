{"cells":[{"cell_type":"markdown","source":["# NLTK: Acceso a corpus de texto\n","\n","[Pablo Carballeira] Partes de este código han sido adaptadas del código de Victor Peinado https://github.com/vitojph/kschool-nlp-23\n","\n","Puedes encontrar información sobre cómo trabajar en Colab aquí (https://colab.research.google.com/notebooks/intro.ipynb)\n","\n","Este notebook se corresponde con el capítulo 2 del NLTK Book [Accessing Text Corpora and Lexical Resources](http://www.nltk.org/book/ch02.html). La lectura del capítulo es muy recomendable."],"metadata":{"id":"s7XRMyHrX3J5"}},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"qy4SsEbSkbmf"},"outputs":[],"source":["# install the requirements\n","!pip install nltk\n","!python -m nltk.downloader book\n","!python -m nltk.downloader cess_esp"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"UJjN38bQkbmt"},"source":["## Corpus no anotados: el Proyecto Gutenberg\n","\n","En notebooks anteriores, hemos utilizado un corpus muy sencillo de tuits para entrenar un clasificador de textos. Existen recursos más amplios y variados que estos corpus, y NLTK nos da un acceso sencillo a varios de ellos\n","\n","NLTK nos da acceso directo a varias colecciones de textos. Para empezar, vamos a juguetear un poco con los libros del [Proyecto Gutenberg](http://www.gutenberg.org), un repositorio público de libros libres y/o sin derechos de copyright en vigor.\n","Antes de nada, necesitamos importar el módulo `gutenberg` que está en la librería `nltk.corpus`."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"F1H5Y5Afkbmx"},"outputs":[],"source":["from nltk.corpus import gutenberg"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"51luxzT2kbmy"},"source":["Podemos listar el catálogo de libros del Proyecto Gutenberg disponibles desde NLTK a través del método `nltk.corpus.gutenberg.fileids`"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"Wa5Plb6lkbmz"},"outputs":[],"source":["print(gutenberg.fileids())"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"lJf5329Pkbm2"},"source":["Para cargar alguno de estos libros en variables y poder manipularlos directamente, podemos utilizar varios métodos.\n","\n","- `gutenberg.raw` recupera el texto como una única cadena de caracteres.\n","- `gutenberg.words` recupera el texto tokenizado en palabras. El método devuelve una lista palabras.\n","- `gutenberg.sents` recupera el texto segmentado por oraciones. El método devuelve una lista de oraciones. Cada oración es a su vez una lista de palabras.\n","- `gutenberg.paras` recupera el texto  segmentado por párrafos. El método devuelve una lista de párrafos.  Cada párrafo es una lista de oraciones, cada oración es a su vez una lista de palabras."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"apGEcGWhkbm4"},"outputs":[],"source":["# cargo la vesión 'cruda' de un par de libros. Como son libros del Proyecto Gutenberg, se trata de ficheros en texto plano\n","alice = gutenberg.raw(\"carroll-alice.txt\")\n","print(alice[:200])  # imprimo los primeros 200 caracteres del libro de Alicia\n","print(\"-\" * 70)\n","\n","bible = gutenberg.raw(\"bible-kjv.txt\")\n","print(bible[:200])  # imprimo los primeros 200 caracteres de la Biblia\n","print(\"-\" * 70)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"jv9recdbkbm5"},"outputs":[],"source":["# segmentamos el texto en palabras,simplemente teniendo en cuenta los espacios\n","bible_tokens = bible.split()\n","# cargamos la versión de la Biblia segmentado en palabras\n","bible_words = gutenberg.words(\"bible-kjv.txt\")\n","\n","# no da el mismo número de tokens\n","print(\"Nº elementos bible_tokens: \",len(bible_tokens))\n","print(\"Nº elementos bible_words: \",len(bible_words))\n","print(\"\\n\", \"-\" * 75, \"\\n\")\n","\n","print(bible_tokens[:100])\n","print(\"\\n\", \"-\" * 75, \"\\n\")\n","print(bible_words[:100])"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"WeoR-yMikbm7"},"outputs":[],"source":["# cargo la versión de Alicia segmentada en palabras\n","alice_words = gutenberg.words(\"carroll-alice.txt\")\n","print(alice_words[:20])  # imprimo las primeros 20 palabras\n","print(len(alice_words))"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"vZZ31RCUkbm9"},"outputs":[],"source":["# cargo la versión de Alicia segmentada en oraciones\n","alice_sents = gutenberg.sents(\"carroll-alice.txt\")\n","print(\"Alice tiene\", len(alice_sents), \"oraciones\")\n","print(alice_sents[2:5])  # imprimo la tercera, cuarta y quinta oración"]},{"cell_type":"markdown","source":["Fíjate que en este caso la estructura es una lista de listas de tokens"],"metadata":{"id":"kYbDkBY4fx9v"}},{"cell_type":"code","source":["print(alice_sents[0])\n","print(alice_sents[1])\n","print(alice_sents[2])"],"metadata":{"id":"F68SVs3Of_Xn"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"JRJmYejukbm-"},"outputs":[],"source":["# cargo la versión de Alicia segmentada en párrafos\n","alice_paras = gutenberg.paras(\"carroll-alice.txt\")\n","print(\"Alice tiene\", len(alice_paras), \"párrafos\")\n","\n","# imprimo los cinco primeros\n","for para in alice_paras[:5]:\n","    print(para)\n","    print(\"\\n\", \"-\" * 75, \"\\n\")"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"wH-RdI9_kbm_"},"source":["Fíjate en que cada método devuelve una estructura de datos diferente: desde una única cadena a listas de listas anidadas. Para que tengas claro las dimensiones de cada uno, podemos imprimir el número de caracteres, palabras, oraciones y párrafos del libro. "]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"84VkPEdGkbnA"},"outputs":[],"source":["print(len(alice), \"caracteres\")\n","print(len(alice_words), \"palabras\")\n","print(len(alice_sents), \"oraciones\")\n","print(len(alice_paras), \"párrafos\")"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"qnkKocwTkbnB"},"source":["Vamos a imprimir algunas estadísticas para todos los libros del Proyecto Gutenberg disponibles. Para cada libro, impriremos por pantalla el promedio de caracteres por palabra, el promedio de palabras por oración y el promedio de oraciones por párrafo."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"m0YWnbhEkbnB"},"outputs":[],"source":["# para cada libro que está disponible en el objeto gutenberg\n","for libro in gutenberg.fileids():\n","    caracteres = len(gutenberg.raw(libro))\n","    palabras = len(gutenberg.words(libro))\n","    oraciones = len(gutenberg.sents(libro))\n","    parrafos = len(gutenberg.paras(libro))\n","    print(\n","        libro[:-4], (20-len(libro[:-4]))*\" \",\n","        round(caracteres / palabras, 2),\n","        \"\\t\",\n","        round(palabras / oraciones, 2),\n","        \"\\t\",\n","        round(oraciones / parrafos, 2),\n","    )"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"6orwv5onkbnC"},"source":["El módulo `nltk.corpus` permite acceder a otras colecciones de textos en otras lenguas ([lista completa aquí](http://www.nltk.org/book/ch02.html#tab-corpora)). Vamos a probar con un corpus de noticias en castellano llamado `cess_esp` que incluye anotación morfo-sintáctica."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"in7cTBGYkbnC"},"outputs":[],"source":["from nltk.corpus import cess_esp\n","\n","# la versión en crudo de este corpus contiene información morfosintáctica con un formato que todavía no hemos visto.\n","# en este caso, pasamos directamente a trabajar con los textos segmentados\n","\n","# cargo el primer documento del corpus segmentado en palabras\n","palabras = cess_esp.words(cess_esp.fileids()[0])\n","print(palabras[0:50])\n","\n","print(\"\\n\", \"-\" * 75, \"\\n\")\n","\n","# y segmentado en oraciones\n","oraciones = cess_esp.sents(cess_esp.fileids()[0])\n","print(oraciones[:5])"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"skqDJ4jZkbnD"},"outputs":[],"source":["print(\" \".join(palabras[:100]))"]},{"cell_type":"markdown","source":["### Ejercicio: estadísticas de documentos en castellano\n","\n","De manera similar a como hemos hecho sacando estadísticas de las obras disponibles en el corpus `gutenberg`, vamos a calcular la longitud promedio de palabras y el número de palabras promedio por oración, para los diez primeros documentos de este corpus `cess_esp`.\n","\n","En este caso, el numero de caracteres debemos contarlo a partir de los tokens en `cess_esp.words(documento)` Utiliza una estructura con bucles anidados.\n"],"metadata":{"id":"PrdCSYsohaKn"}},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"WioS5a0DkbnD"},"outputs":[],"source":["# para cada documento que está entre los 10 primeros del corpus\n","for documento in cess_esp.fileids()[:10]:\n","    # carga el texto segmentado en palabras\n","    palabras = cess_esp.words(documento)\n","    # y en oraciones\n","    oraciones = cess_esp.sents(documento)\n","\n","    # longitud_promedio = ???\n","    \n","    # imprime el nombre del documento, la longitud de la palabra y el número de palabras por oración\n","    print(\n","        documento[:-4],\n","        (20-len(documento[:-4]))*\" \",\n","        round(longitud_promedio, 2),\n","        \"\\t\",\n","        round(len(palabras) / len(oraciones), 2),\n","    )"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"y0vhvEeykbnE"},"source":["Los libros del Proyecto Gutenberg constituyen el tipo de corpus más sencillo: no está anotado (no incluye ningún tipo de información lingüística) ni categorizado. \n","\n","## Corpus categorizados y anotados: el Corpus de Brown\n","\n","El Corpus de Brown fue el primer gran corpus orientado a tareas de PLN. Desarrollado en la Universidad de Brown, contiene más de un millón de palabras provenientes de 500 fuentes.  La principal catacterística de este corpus es que sus textos están categorizados por género. "]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"KAQP6dTdkbnE"},"outputs":[],"source":["from nltk.corpus import brown"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"eyNUzPEgkbnF"},"source":["Como en los libros del Proyecto Gutenberg, aquí también podemos imprimir los nombres de los ficheros. En este caso son poco significativos, nos nos dicen nada del contenido."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"8IaUtUMrkbnF"},"outputs":[],"source":["# Brown está formado por 500 documentos\n","print(len(brown.fileids()))\n","# imprimimos solos los 10 primeros\n","print(brown.fileids()[:10])"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"Efo2rimYkbnF"},"source":["### El corpus de Brown está categorizado por géneros\n","\n","Una de las principales diferencias con otros corpus vistos anteriormente es que el de Brown está categorizado: los textos están agrupados según su género o temática. Y en este caso, los nombres de las categorías sí nos permiten intuir el contenido de los textos."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"O9IFmv-ZkbnF"},"outputs":[],"source":["print(brown.categories())"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"xygjBpOSkbnG"},"source":["De manera similar a los libros del Proyecto Gutenberg, podemos acceder a los textos de este corpus a través de los métodos `brown.raw`, `brown.words`, `brown.sents` y `brown.paras`. Además, podemos acceder a una categoría de textos concretas si lo especificamos como argumento."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"eTZ0g8GTkbnG"},"outputs":[],"source":["news_words = brown.words(categories=\"news\")\n","scifi_sents = brown.sents(categories=\"science_fiction\")"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"T6uc4tw5kbnG"},"outputs":[],"source":["print(news_words[:50])\n","print(\"\\n\", \"-\" * 75, \"\\n\")\n","print(scifi_sents[:3])"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"a-oqXWCMkbnH"},"source":["Vamos a sacar provecho de la categorización de los textos de este corpus. Para ello, vamos a calcular la frecuencia de distribución de distintas palabras (más o menos específicas a cada tema). Para ello, vamos a calcular una distribución de frecuencia condicional que calcule la frecuencia de cada palabra para cada categoría.\n","\n","No te preocupes si no entiendes la sintaxis para crear tablas de frecuencias condicionales a través del objeto `ConditionalFreqDist`. Créeme, ese objeto calcula frecuencias de palabras atendiendo a la categoría en la que aparecen y crea una especie de diccionario de diccionarios."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"58nR-2YqkbnH"},"outputs":[],"source":["from nltk import ConditionalFreqDist\n","\n","words = \"the can might money boy plane knife boat\".split()\n","words_cfd = ConditionalFreqDist(\n","    (category, word)\n","    for category in brown.categories()\n","    for word in brown.words(categories=category)\n",")\n","\n","# la sintaxis anterior contiene varios bucles anidados y es equivalente a:\n","# for category in brown.categories():\n","#     for word in brown.words(categories=category):\n","#         ConditionalFreqDist(category, word)"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"ZK1lFx9nkbnH"},"source":["Una vez tenemos calculada la frecuencia de distribución condicional, podemos pintar los valores fácilmente en forma de tabla a través del método `.tabulate`, especificando como condiciones cada una de las categorías, y como muestras las palabras del inglés que hemos definido."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"0OEEneTZkbnH"},"outputs":[],"source":["words_cfd.tabulate(conditions=brown.categories(), samples=words)"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"rFGSHh3SkbnH"},"source":["Las cifras que hemos mostrado en las tablas anteriores se refieren a las frecuencias absolutas de cada palabra en cada categoría. Realizar comparaciones así no es acertado, porque es posible que cada categoría tenga un número de documentos (y de palabras) diferente. \n","\n","Vamos a comprobar si esto es cierto. ¿Está equilibrada la colección o tenemos algunos géneros sobrerrepresentados?"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"F__AQyAikbnI"},"outputs":[],"source":["for categoria in brown.categories():\n","    print(categoria, (15-len(categoria))*\" \", len(brown.words(categories=categoria)))"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"FxYJSEqVkbnI"},"source":["Como vemos, el número de palabras no está equilibado. Tenemos muchos más datos en las categorías `belles_lettres` y `learned` que en `science_fiction` o `humor`, por ejemplo.\n","\n","Calculemos a continuación la frecuencia relativa de estas palabras, atendiendo al género. Para ello, necesitamos dividir la frecuencia absoluta de cada palabra entre el número de palabras total de cada categoría."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"C_jb4QFzkbnI"},"outputs":[],"source":["for categoria in brown.categories():\n","    # ¿cuántas palabras tenemos en cada categoría?\n","    longitud = len(brown.words(categories=categoria))\n","    print(\"\\n\", categoria)\n","    print(\"\\n\", \"-\" * 75, \"\\n\")\n","    for palabra in words:\n","        print(palabra, \"->\", words_cfd[categoria][palabra] / longitud)"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"mqrYd1tIkbnI"},"source":["Vamos a repetir la operación de cálculo de frecuencias relativas reasignando estos valores en el propio objeto `words_cfd`, con el objetivo de utilizar el método `tabulate` para poder impirmir la tabla con los valors relativos."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"7_zYIxqlkbnJ"},"outputs":[],"source":["# lo primero, realizo una copia de mi distribución de frecuencias\n","import copy\n","\n","words_cfd_rel = copy.deepcopy(words_cfd)"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"dyBjeq0rkbnJ"},"outputs":[],"source":["# sustituyo los conteos de la tabla por sus frecuencias relativas (ojo, en tantos por 1.000.000)\n","for categoria in brown.categories():\n","    longitud = len(brown.words(categories=categoria))\n","    for palabra in words:\n","        words_cfd_rel[categoria][palabra] = (\n","            words_cfd[categoria][palabra] / longitud\n","        ) * 1000000"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"FmmLq57wkbnJ"},"outputs":[],"source":["# imprimo la tabla\n","words_cfd_rel.tabulate(conditions=brown.categories(), samples=words)"]},{"cell_type":"markdown","source":["Fijate en los resutados: la palabra \"the\" representa entre un 4% (40 000/1 000 000) y un 6% del total de palabras, y está muy uniformemente distribuida en todas las categorías (es una stopword). \n","\n","Sin embargo, una palabra como \"boy\" es mucho menos frecuente (0.06% como máximo), pero es mucho más habitual en catgorías como \"adventure\", \"fiction\", \"romance\". Empezamosa ver que las estadisticas de frecuencia de aparicion de ciertas palabras podría usarse como variable que puede servir para identificar la categoría de un documento. Veremos más sobre esto más adelante."],"metadata":{"id":"IczKeKMirGzg"}},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"KiqKfuDWkbnK"},"source":["### Brown es también un corpus anotado con información morfológica\n","\n","El corpus de Brown no solo está categorizado, también está anotado con información morfológica. Para acceder a la versión anotada del corpus, podemos utilizar los métodos: `brown.tagged_words`, `brown.tagged_sents` y `brown.tagged_\n","paras`"]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"PFeg5IsqkbnK"},"outputs":[],"source":["scifi_tagged_words = brown.tagged_words(categories=\"science_fiction\")\n","print(scifi_tagged_words[:50])"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"ZMgwxKpkkbnL"},"source":["Fíjate que cuando accedemos a la versión etiquetada del corpus, no obtenemos una simple lista de palabras sino una lista de tuplas, donde el primer elemento es la palabra en cuestión y el segundo es la etiqueta que indica la categoría gramatical de la palabra.\n","\n","Este conjunto etiquetas se ha convertido en casi un estándar para el inglés y se utilizan habitualmente para anotar cualquier recurso lingüístico en esa lengua. Tienes un resumen de las etiquetas aqui: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n","\n","Vamos a crear una nueva tabla de distribución de frecuencias condicional para calcular la frecuencia de aparición de las etiquetas, teniendo en cuenta la categoría."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"5MwcUwR5kbnL"},"outputs":[],"source":["from nltk import ConditionalFreqDist\n","\n","tags_cfd = ConditionalFreqDist(\n","    (category, item[1])\n","    for category in brown.categories()\n","    for item in brown.tagged_words(categories=category)\n",")"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"dFjZa7flkbnL"},"source":["Y ahora vamos a imprimir la tabla de frecuencias para cada categoría y para algunas de las etiquetas morfológicas: sustantivos en singular `NN`, verbos en presente `VB`, verbos en pasado simple `VBD`, participios pasados `VBN`, adjetivos `JJ`, preposiciones `IN`, y artículos `AT`. \n","\n","Recuerda: estas cifras no son directamente comparables entre categorías ya que éstas no están equilibradas. Hay categorías con más textos que otras."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"oD9YomLnkbnM"},"outputs":[],"source":["tags_cfd.tabulate(\n","    conditions=brown.categories(), samples=\"NN VB VBD VBN JJ IN AT\".split()\n",")"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"2_JVfxLRkbnN"},"source":["### Ejercicio 1: buscando adjetivos\n","\n","Creamos una lista de adjetivos (etiquetados como JJ) que aparezcan en la colección de textos categorizados como *hobbies*, e imprimimos por pantalla los 50 primeros que encontramos."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"CcNK96-ykbnN"},"outputs":[],"source":["# ???\n"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"ePDoKrOZkbnO"},"source":["### Ejercicio 2: buscando palabras largas\n","\n","Recorre cada categoría del corpus de Brown buscando adjetivos, e imprime solo aquellos que tengan una longitud de al menos 15 caracteres y que no sean palabras compuestas escritas con guiones ortográficos."]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"xkeGGeSDkbnO"},"outputs":[],"source":["# ???\n"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"sL7aM-u-kbnO"},"source":["### Ejercicio 3: Frecuencias de palabras en castellano\n","\n","Procesa una novela en castellano (p. ej. *Fortunata y Jacinta*), tokenizando y normalizando el texto, para calcular una distribución de frecuencias de las palabras."]},{"cell_type":"code","source":["# descargamos el libro usando wget\n","!mkdir data\n","!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1ojuBlxcoT88fvYz_UVnGqI4P8Pz7c-CO' -O data/fortunatayjacinta.txt"],"metadata":{"id":"vaB92qRixf1O"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"q6nAf3EqkbnO"},"outputs":[],"source":["# ???\n"]},{"cell_type":"markdown","metadata":{"Collapsed":"false","id":"HXTwwlr8kbnP"},"source":["### Ejercicio 4: Frecuencias de palabras en inglés\n","\n","Repite el proceso con un texto en inglés (p. ej. *Sherlock Holmes*)."]},{"cell_type":"code","source":["# descargamos el libro usando wget\n","!mkdir data\n","!wget --no-check-certificate 'https://docs.google.com/uc?export=download&id=1t8JP8JF_9wnhcQvdrrfcXSnNetV_iIyD' -O data/sherlockholmes.txt"],"metadata":{"id":"DnzBS7xH1Tyg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"Collapsed":"false","id":"ykco82AMkbnP"},"outputs":[],"source":["# ???\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"03_nltk-corpus_stu.ipynb","provenance":[{"file_id":"17KXajBfV2Ul3i35RHsEP9wc9M_jwS7jM","timestamp":1645615316202}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}