{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_nltk_text_pre-processing_stu.ipynb","provenance":[{"file_id":"1ZGQYBY9okhPzDz5cxktg79x3vywDZ3K4","timestamp":1645614845438}],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyPnGR2jeqqr/eCJi/Jz/BLE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Introducción al pre-procesado de texto para Procesamiento de Lenguaje Natural \n","[Pablo Carballeira] Partes de este código han sido adaptadas del código correspondiente a la especialización Natural Language Processing de DeepLearning.AI\n","\n","Puedes encontrar información sobre cómo trabajar en Colab aquí (https://colab.research.google.com/notebooks/intro.ipynb)\n","\n"],"metadata":{"id":"hL6bxS2HjVtD"}},{"cell_type":"markdown","source":["El preprocesamiento de datos es uno de los pasos críticos en cualquier proyecto de data science. Incluye limpiar y formatear los datos antes de introducirlos en un algoritmo de aprendizaje automático. Es fundamental en la mayoría de casos en PLN, y  los pasos de preprocesamiento suelen estar compuestos de las siguientes tareas:\n","\n","* Segmentación en palabras o tokenización\n","* Conversión a minúsculas\n","* Eliminación de palabras supérfluas (stopwords en inglés) y puntuación\n","* Normalización (radicalización y/o lematización)"],"metadata":{"id":"xBKa3093Fw4J"}},{"cell_type":"markdown","source":["En este notebook, y en varios de los siguientes, usaremos el paquete [Natural Language Toolkit (NLTK)](http://www.nltk.org/howto/twitter.html), una biblioteca Python de código abierto para el procesamiento del lenguaje natural.\n","\n","Proporciona una interfaz sencilla de utilizar, con más de 50 corpus y recursos léxicos como WordNet, junto con un conjunto de bibliotecas de procesamiento de texto para clasificación, tokenización, lematización, etiquetado, o análisis semántico. \n","\n","El libro [Natural Language Processing with Python](https://www.nltk.org/book/) proporciona una introducción práctica a la programación para el procesamiento del lenguaje. Escrito por los creadores de NLTK, guía al lector a través de los fundamentos de la escritura de programas Python, el trabajo con corpus, la categorización del texto, el análisis de la estructura lingüística y más. "],"metadata":{"id":"jp7TrVrBhd6D"}},{"cell_type":"code","source":["# Importamos y descargamos algunos paquetes necesarios\n","import nltk\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","nltk.download('treebank')\n","\n","from nltk.tree import Tree\n","from IPython.display import display\n","\n","import os"],"metadata":{"id":"qGxsfmzjGyx2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tzR6HGejE6Ta"},"source":["## Dataset de Twitter\n","\n","Antes de empezar, vamos a probar un conjunto de datos de Twitter, que viene integrado en NLTK, para hacernos una idea de la necesidad de este pre-procesado. Además, NLTK tiene módulos para recopilar, manejar y procesar datos de Twitter. Es un conjunto de datos anotado manualmente (etiquetas e opinión positiva y negativa) y es útil para medir de manera rápida el rendimiento base de algoritmos de clasificación de texto, que veremos más adelante.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pUs5EI_3E6Ta"},"outputs":[],"source":["from nltk.corpus import twitter_samples    # sample Twitter dataset from NLTK\n","import matplotlib.pyplot as plt            # library for visualization\n","import random                              # pseudo-random number generator\n","\n","nltk.download('twitter_samples')"]},{"cell_type":"markdown","metadata":{"id":"wRUf_Zd2E6Td"},"source":["Podemos cargar los textos de los tweets positivos y negativos usando el método `strings()` del módulo de esta manera:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"91_RqhAcE6Te"},"outputs":[],"source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"]},{"cell_type":"markdown","metadata":{"id":"Wxnk9CRAE6Tf"},"source":["A continuación, imprimiremos el número de tweets positivos y negativos. Así como la estructura de datos del dataset. \n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SwgpP6tIE6Tg"},"outputs":[],"source":["print('Number of positive tweets: ', len(all_positive_tweets))\n","print('Number of negative tweets: ', len(all_negative_tweets))\n","\n","print('\\nThe type of all_positive_tweets is: ', type(all_positive_tweets))\n","print('The type of a tweet entry is: ', type(all_negative_tweets[0]))"]},{"cell_type":"markdown","metadata":{"id":"NyMur0EhmDwu"},"source":["## Texto de los tweets sin preprocesar\n","\n","Antes que nada, podemos imprimir un par de tweets del conjunto de datos para ver cómo son. El conocimiento de los datos es responsable de una gran parte del éxito o fracaso de los proyectos de ciencia de datos. Podemos observar aspectos que debemos tener en cuenta al preprocesar nuestros datos.\n","\n","A continuación, se imprimirá un tweet positivo al azar y otro negativo al azar. Hemos agregado una marca de color al comienzo de la cadena para distinguirlos aún más. (Advertencia: esta base de datos está tomada de un conjunto de datos públicos de tweets reales y una porción muy pequeña tiene contenido explícito). Puedes ejecutar varias veces la celda para ver varios ejemplos."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l9siKKNdmDwv"},"outputs":[],"source":["# print positive in greeen\n","print('\\033[92m' + all_positive_tweets[random.randint(0,5000)])\n","\n","# print negative in red\n","print('\\033[91m' + all_negative_tweets[random.randint(0,5000)])"]},{"cell_type":"markdown","metadata":{"id":"U2fmW0-gE6Tk"},"source":["Se puede observar la presencia de [emoticonos](https://en.wikipedia.org/wiki/Emoticon) y URLs en muchos de los tweets. Conocer esta información será útil en los siguientes pasos."]},{"cell_type":"markdown","source":["Utilizaremos el siguiente tweet para ejemplificar los posibles pasos del pre-procesado de texto"],"metadata":{"id":"MSlh-S1m67kR"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"hYJ70BRo66UC"},"outputs":[],"source":["# Our selected sample. Complex enough to exemplify each step\n","tweet = all_positive_tweets[2277]\n","print(tweet)"]},{"cell_type":"markdown","metadata":{"id":"aCEjsG0oE6Tn"},"source":["## Eliminar hipervínculos, marcas y estilos de Twitter\n","\n","Dado que tenemos un conjunto de datos de Twitter, nos gustaría eliminar algunas subcadenas que se usan comúnmente en la plataforma, como el hashtag, las marcas de retweet y los hipervínculos. Usaremos la biblioteca de expresiones regulares [re](https://docs.python.org/3/library/re.html) para realizar operaciones basadas en expresiones regulares en nuestro tweet.\n","\n","Definiremos nuestro patrón de búsqueda y usaremos el método `sub()` para eliminar las coincidencias sustituyéndolas con un carácter vacío (es decir, `''`)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q8fL2KmUE6Tn"},"outputs":[],"source":["import re                                  # library for regular expression operations\n","\n","print('\\033[92m' + tweet)\n","print('\\033[94m')\n","\n","# remove old style retweet text \"RT\"\n","tweet2 = re.sub(r'^RT[\\s]+', '', tweet)\n","\n","# remove hyperlinks\n","tweet2 = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet2)\n","\n","# remove hashtags\n","# only removing the hash # sign from the word\n","tweet2 = re.sub(r'#', '', tweet2)\n","\n","print(tweet2)"]},{"cell_type":"markdown","source":["## Tokenización"],"metadata":{"id":"ncTfYM_IaVqB"}},{"cell_type":"markdown","source":["La segmentación o \"tokenización\" de texto es una de las herramientas fundamentales para preprocesar textos de lenguaje natural. En este notebook vamos a ver algunas de las aproximaciones mas comunes, utilizando en este caso el lenguaje inglés como ejemplo"],"metadata":{"id":"g_bdiWMCFi37"}},{"cell_type":"markdown","source":["Una de las maneras mas sencillas de realizar esta tarea es segmentar los tokens (palabras) separados por espacios en blanco"],"metadata":{"id":"McbHI1XPYaRe"}},{"cell_type":"code","source":["text = \"This is Andrew's text, isn't it?\"\n","tokenizer = nltk.tokenize.WhitespaceTokenizer()\n","tokenizer.tokenize(text)\n"],"metadata":{"id":"Om8iVs02G0PO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Uno de los problemas de este método es que puede obtener tokens diferentes para palabras con el mismo significado. Por ejemplo el \"it?\" del final de la frase, comparado con un \"it\" en otro lugar de una frase"],"metadata":{"id":"11Q7ivqBYrWA"}},{"cell_type":"markdown","source":["Otra de las posibilidades es tokenizar mediante la separación establecida por los signos de puntuación. El problema de este método es que puede sobresegmentar la oración en tokens sin significado completo"],"metadata":{"id":"BVjve_kaaAAO"}},{"cell_type":"code","source":["tokenizer = nltk.tokenize.WordPunctTokenizer()\n","tokenizer.tokenize(text)"],"metadata":{"id":"WHauRIutG36F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Existen métodos más avanzados que tienen en cuenta relaciones más complejas, obtenidas ya sea mediante reglas o mediante aprendizaje. En este caso utilizamos un método basado en reglas del lenguaje incluido en la librería de NLTK"],"metadata":{"id":"W5lcF5i_Y3Va"}},{"cell_type":"code","source":["tokenizer = nltk.tokenize.TreebankWordTokenizer()\n","print(tokenizer.tokenize(text))\n","# el mismo método se puede invocar con la funcion word_tokenize()\n","print(word_tokenize(text))"],"metadata":{"id":"Rwc7OcQpG2Qd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dependiendo del idioma, este proceso puede no ser evidente, ni codificable según reglas fijas. Por ejemplo en el caso de idiomas con abundancia de palabras compuestas, como el alemán. \n","\n","Rechtsschutzversicherungsgesellschaften: compañía de seguros que proporciona protección legal \n","\n","Fijémonos que la segmentación no es correcta, incluso itentando utilizar reglas aplicadas al idioma aleman"],"metadata":{"id":"VPXxhWT9JSCw"}},{"cell_type":"code","source":["text = \"Rechtsschutzversicherungsgesellschaften\"\n","print(\"\\nOriginal string:\")\n","print(text)\n","from nltk.tokenize import word_tokenize\n","token_text = word_tokenize(text, language='german')\n","print(\"\\nWord-tokenized copy in a list:\")\n","print(token_text)"],"metadata":{"id":"n0UHjIKxgDQ8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aGODgkUiE6To"},"source":["NLTK nos proprciona un método TweetTokenizer, que funciona de forma muy similar al método mas genérico [word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#module-nltk.tokenize.casual). TweetTokenizer mantiene intactos los hashtags mientras que word_tokenize no lo hace.\n","\n","Fijémonos en cuales son los argumentos opcionales de TweetTokenizer, y como los usamos en el ejemplo para convertir el texto a minúsculas y eliminar posibles repeticiones de caracteres.\n","\n","- preserve_case (bool) – Flag indicating whether to preserve the casing (capitalisation) of text used in the tokenize method. Defaults to True.\n","\n","- reduce_len (bool) – Flag indicating whether to replace repeated character sequences of length 3 or greater with sequences of length 3. Defaults to False.\n","\n","- strip_handles (bool) – Flag indicating whether to remove Twitter handles of text used in the tokenize method. Defaults to False."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"l92lbqIOE6To"},"outputs":[],"source":["from nltk.tokenize import TweetTokenizer\n","\n","print('\\033[92m' + tweet)\n","print()\n","print('\\033[92m' + tweet2)\n","print('\\033[94m')\n","\n","# instantiate tokenizer class\n","tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,\n","                               reduce_len=True)\n","\n","# tokenize tweets\n","tweet_tokens = tokenizer.tokenize(tweet2)\n","\n","print()\n","print('Tokenized string:')\n","print(tweet_tokens)"]},{"cell_type":"markdown","metadata":{"id":"LnSGUL2AE6Tp"},"source":["## Eliminar palabras vacías y signos de puntuación\n","\n","En muchos casos, es útil eliminar  las palabras vacías y la puntuación. Las palabras vacías (stopwords en inglés) son palabras que no agregan un significado relevante al texto. Verá la lista proporcionada por NLTK cuando ejecutes la celdas a continuación."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uiDLYPuuE6Tm"},"outputs":[],"source":["# download the stopwords from NLTK\n","nltk.download('stopwords')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h_yC-WnHE6Tp"},"outputs":[],"source":["from nltk.corpus import stopwords\n","import string\n","\n","#Import the english stop words list from NLTK\n","stopwords_english = stopwords.words('english') \n","\n","print('Stop words\\n')\n","print(stopwords_english)\n","\n","print('\\nPunctuation\\n')\n","print(string.punctuation)"]},{"cell_type":"markdown","metadata":{"id":"rd64sLGJE6Tp"},"source":["Podemos ver que la lista de palabras vacías anterior contiene algunas palabras que podrían ser importantes en algunos contextos.\n","Estas podrían ser palabras como _i, not, between, because, won, against_. Es posible que debas personalizar la lista de palabras vacías para algunas aplicaciones. Para nuestro ejercicio, usaremos la lista completa.\n","\n","Para la puntuación, vimos anteriormente que ciertas agrupaciones como ':)' y '...' deben conservarse cuando se trata de tweets porque se usan para expresar emociones. En otros contextos, estos también deben eliminarse.\n","\n","¡Es hora de limpiar nuestro tweet tokenizado!"]},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Completar el bucle, añadiendo a tweets_clean aquellos tokens que no estén incluidos en la lista de stpowords o puntuación"],"metadata":{"id":"xXv--VeqJrKX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"LLWYKBx0E6Tp"},"outputs":[],"source":["print('\\033[92m' + tweet)\n","print()\n","print('\\033[92m')\n","print(tweet_tokens)\n","print('\\033[94m')\n","\n","tweets_clean = []\n","\n","for word in tweet_tokens: # Go through every word in your tokens list\n","    # tweets_clean = ???\n","\n","print('removed stop words and punctuation:')\n","print(tweets_clean)"]},{"cell_type":"markdown","metadata":{"id":"xKeMfSwDE6Tq"},"source":["Fíjate que las palabras **happy** y **sunny** en esta lista están escritas correctamente."]},{"cell_type":"markdown","source":["## Normalización\n","\n","En ciertos casos puede ser interesante normalizar los tokens (palabras) para obtener un único token a partir de diferentes versiones de una palabra, p.ej wolf, wolves --> wolf, o talk, talks --> talk. \n","\n","Esto reduce el vocabulario que es necesario considerar en el caso de una aplicación de análisis de opinión por ejemplo."],"metadata":{"id":"5rirVtHfaasH"}},{"cell_type":"markdown","source":["### Radicalización\n","\n","Radicalización o derivación (stemming): consiste en la eliminación de sufijos, convirtiéndola en su forma más general. Uno de los modelos mas comunes de radicalización es el modelo de Porter, basado en reglas morfoloógicas.\n","\n","\n","Considerando las palabras: \n"," * **learn**\n"," * **learn**ing\n"," * **learn**ed\n"," * **learn**t\n"," \n","Todas estas palabras se derivan de su raíz común **learn**. Sin embargo, en algunos casos, el proceso de lematización produce palabras que no son ortografías correctas de la raíz de la palabra. Por ejemplo, **happi** y **sunni**. Eso es porque elige la raíz más común para las palabras relacionadas. Por ejemplo, podemos fijarnos en el conjunto de palabras que componen las distintas formas de happy:\n","\n"," * **happ**y\n"," * **happi**ness\n"," * **happi**er\n"," \n","Podemos ver que el prefijo **happi** se usa más comúnmente. No podemos elegir **happ** porque es la raíz de palabras no relacionadas como **happen**\n"," \n","\n","\n","\n"],"metadata":{"id":"iaoiWYHjFmpR"}},{"cell_type":"code","source":["text = \"feet wolves cats talked\"\n","tokenizer = nltk.tokenize.TreebankWordTokenizer()\n","tokens = tokenizer.tokenize(text)\n","stemmer = nltk.stem.PorterStemmer()\n","\" \".join(stemmer.stem(token) for token in tokens)"],"metadata":{"id":"OWIBnTsnG_IC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vemos como es común generar errores en formas irregulares, como foot --> feet"],"metadata":{"id":"InfyZr1TdKMM"}},{"cell_type":"markdown","source":["### Lematización\n"," \n","La lematización consiste en una tarea similar pero con la utilización de vocabulario y análisis morfológico. Trata recupurar la forma básica de una palabra (o versión del diccionario). En el ejemplo se utiliza un lematizador basado en la base de datos WordNet (que codifica relaciones entre palabras)\n","\n"],"metadata":{"id":"Ql3KQyZoF3Ll"}},{"cell_type":"code","source":["stemmer = nltk.stem.WordNetLemmatizer()\n","\" \".join(stemmer.lemmatize(token) for token in tokens)\n"],"metadata":{"id":"RAg2EKvQF9Jl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["La lematización que hemos utilizado puede obtener resultados no deseados en el caso de verbos, ya que asume por defecto que todas las palabras son sustantivos.\n","\n","Mira lo que ocurre si indicamos a la función que las palabras son verbos\n","\n","\n","\n"," \n"],"metadata":{"id":"6PmEpQCLdze4"}},{"cell_type":"code","source":["\" \".join(stemmer.lemmatize(token,\"v\") for token in tokens)\n"],"metadata":{"id":"xA2WMe9I2BKB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En este caso, la lematización del verbo es correcta, pero no así los sustantivos. Por lo tanto, sería interesante usar etiquetas morfológicas para hacer esta lematización de forma más inteligente. \n","\n","Veremos herramientas para hacer esto, de forma automática, más adelante "],"metadata":{"id":"lxJeTDHC2Fio"}},{"cell_type":"markdown","source":["## Radicalización de tuits\n","\n","NLTK tiene diferentes módulos para radicalización. En el caso de nuestros datos de Twitter, utilizaremos la radicalización de Porter [PorterStemmer](https://www.nltk.org/api/nltk.stem.html#module-nltk.stem.porter) que usa el [Algoritmo de radicalización de Porter](https://tartarus.org/martin/PorterStemmer/)."],"metadata":{"id":"k0mPQ2sfHiLJ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"B-3RblamE6Tq"},"outputs":[],"source":["from nltk.stem import PorterStemmer        # module for stemming\n","\n","print('\\033[92m' + tweet)\n","print()\n","print('\\033[92m')\n","print(tweets_clean)\n","print('\\033[94m')\n","\n","# Instantiate stemming class\n","stemmer = PorterStemmer() \n","\n","# Create an empty list to store the stems\n","tweets_stem = [] \n","\n","for word in tweets_clean:\n","    stem_word = stemmer.stem(word)  # stemming word\n","    tweets_stem.append(stem_word)  # append to the list\n","\n","print('stemmed words:')\n","print(tweets_stem)"]},{"cell_type":"markdown","source":["## Función para el pre-procesado del dataset completo\n","\n","Una vez que hemos visto las tareas típicas de preprocesado, vamos a juntarlo todo en una única función que podamos utilizar para preprocesar nuestro dataset de tweets completo."],"metadata":{"id":"Qk0GGAM1C3ze"}},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Completa la funcion `process_tweet(tweet)`, utilizando los pasos que hemos definido más arriba\n","\n","Ejecutando el código de la siguiente celda puedes comprobar que el resultado es el mismo que haciendo el pre-procesado paso a paso\n"],"metadata":{"id":"e9J1epOUDurr"}},{"cell_type":"code","source":["import re\n","import numpy as np\n","\n","from nltk.stem import PorterStemmer\n","\n","\n","def process_tweet(tweet):\n","    \"\"\"Process tweet function.\n","    Input:\n","        tweet: a string containing a tweet\n","    Output:\n","        tweets_clean: a list of words containing the processed tweet\n","\n","    \"\"\"\n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","    # remove stock market tickers like $GE\n","    tweet = re.sub(r'\\$\\w*', '', tweet)\n","    \n","    # incluir la eliminacion de hyperlinks, etc que hemos visto antes\n","    # tweet = ???\n","    \n","    # tokenización\n","    # tweet_tokens = ???\n","\n","    # eliminación de stopwords, puntuación y radicalización\n","    # realizar todo a la vez en el mismo bucle\n","    tweets_clean = []\n","\n","    # tweets_clean = ???\n","\n","    return tweets_clean"],"metadata":{"id":"ktN25LFYFKOi"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1A5PFzLKE6Tr"},"outputs":[],"source":["# choose the same tweet\n","tweet = all_positive_tweets[2277]\n","\n","print()\n","print('\\033[92m')\n","print(tweet)\n","print('\\033[94m')\n","\n","# call the imported function\n","tweets_stem = process_tweet(tweet); # Preprocess a given tweet\n","\n","print('preprocessed tweet:')\n","print(tweets_stem) # Print the result"]},{"cell_type":"markdown","source":["### Ejercicio\n","\n","Puedes ejecutar la próxima celda múltiples veces para comprobar cual es el resultado de pre-procesar diferentes tweets de nuestro dataset"],"metadata":{"id":"dPxuX5KiEOS3"}},{"cell_type":"code","source":["# choose the same tweet\n","tweet_pos = all_positive_tweets[random.randint(0,5000)]\n","tweet_neg = all_negative_tweets[random.randint(0,5000)]\n","\n","# call the preprocessing function\n","tweet_pos_stem = process_tweet(tweet_pos); # Preprocess\n","tweet_neg_stem = process_tweet(tweet_neg); # Preprocess\n","\n","print()\n","print('\\033[92m')\n","print(tweet_pos)\n","print('\\033[94m')\n","\n","print('preprocessed tweet:')\n","print(tweet_pos_stem) # Print the result\n","\n","print()\n","print('\\033[91m')\n","print(tweet_neg)\n","print('\\033[94m')\n","\n","print('preprocessed tweet:')\n","print(tweet_neg_stem) # Print the result"],"metadata":{"id":"qsbSM6SREW53"},"execution_count":null,"outputs":[]}]}